{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hnzV66HHstNr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `README.md`\n",
        "\n",
        "# A Scalable Framework for Benford's Law Conformity Testing of Financial Data\n",
        "\n",
        "<!-- PROJECT SHIELDS -->\n",
        "[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n",
        "[![Python Version](https://img.shields.io/badge/python-3.9%2B-blue.svg)](https://www.python.org/downloads/)\n",
        "[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "[![Imports: isort](https://img.shields.io/badge/%20imports-isort-%231674b1?style=flat&labelColor=ef8336)](https://pycqa.github.io/isort/)\n",
        "[![Type Checking: mypy](https://img.shields.io/badge/type_checking-mypy-blue)](http://mypy-lang.org/)\n",
        "[![Jupyter](https://img.shields.io/badge/Jupyter-%23F37626.svg?style=flat&logo=Jupyter&logoColor=white)](https://jupyter.org/)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2509.09415-b31b1b.svg)](https://arxiv.org/abs/2509.09415)\n",
        "[![Year](https://img.shields.io/badge/Year-2025-purple)](https://github.com/chirindaopensource/search_benford_law_compatibility)\n",
        "[![Discipline](https://img.shields.io/badge/Discipline-Forensic%20Accounting%20%26%20Econometrics-blue)](https://github.com/chirindaopensource/search_benford_law_compatibility)\n",
        "[![Methodology](https://img.shields.io/badge/Methodology-Benford's%20Law%20%7C%20Goodness--of--Fit-orange)](https://github.com/chirindaopensource/search_benford_law_compatibility)\n",
        "[![Data Source](https://img.shields.io/badge/Data-Refinitiv%20EIKON-lightgrey)](https://www.refinitiv.com/en/products/eikon-trading-software)\n",
        "[![Pandas](https://img.shields.io/badge/pandas-%23150458.svg?style=flat&logo=pandas&logoColor=white)](https://pandas.pydata.org/)\n",
        "[![NumPy](https://img.shields.io/badge/numpy-%23013243.svg?style=flat&logo=numpy&logoColor=white)](https://numpy.org/)\n",
        "[![SciPy](https://img.shields.io/badge/SciPy-%23025596?style=flat&logo=scipy&logoColor=white)](https://scipy.org/)\n",
        "[![PyYAML](https://img.shields.io/badge/PyYAML-4B5F6E.svg?style=flat)](https://pyyaml.org/)\n",
        "\n",
        "--\n",
        "\n",
        "**Repository:** `https://github.com/chirindaopensource/search_benford_law_compatibility`\n",
        "\n",
        "**Owner:** 2025 Craig Chirinda (Open Source Projects)\n",
        "\n",
        "This repository contains an **independent**, professional-grade Python implementation of the research methodology from the 2025 paper entitled **\"Note on pre-taxation reported data by UK FTSE-listed companies. A search for Benford's laws compatibility\"** by:\n",
        "\n",
        "*   Marcel Ausloos\n",
        "*   Probowo Erawan Sastroredjo\n",
        "*   Polina Khrennikova\n",
        "\n",
        "The project provides a complete, end-to-end computational framework for testing the conformity of financial datasets with Benford's Law. It delivers a modular, auditable, and extensible pipeline that replicates the paper's entire workflow: from rigorous data and configuration validation, through robust, mathematically-grounded digit extraction, to the precise calculation of Chi-Squared and Mean Absolute Deviation (MAD) test statistics and the final generation of publication-quality results tables.\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "- [Introduction](#introduction)\n",
        "- [Theoretical Background](#theoretical-background)\n",
        "- [Features](#features)\n",
        "- [Methodology Implemented](#methodology-implemented)\n",
        "- [Core Components (Notebook Structure)](#core-components-notebook-structure)\n",
        "- [Key Callable: execute_full_project_workflow](#key-callable-execute_full_project_workflow)\n",
        "- [Prerequisites](#prerequisites)\n",
        "- [Installation](#installation)\n",
        "- [Input Data Structure](#input-data-structure)\n",
        "- [Usage](#usage)\n",
        "- [Output Structure](#output-structure)\n",
        "- [Project Structure](#project-structure)\n",
        "- [Customization](#customization)\n",
        "- [Contributing](#contributing)\n",
        "- [Recommended Extensions](#recommended-extensions)\n",
        "- [License](#license)\n",
        "- [Citation](#citation)\n",
        "- [Acknowledgments](#acknowledgments)\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This project provides a Python implementation of the methodologies presented in the 2025 paper \"Note on pre-taxation reported data by UK FTSE-listed companies. A search for Benford's laws compatibility.\" The core of this repository is the iPython Notebook `search_benford_law_compatibility_draft.ipynb`, which contains a comprehensive suite of functions to replicate the paper's findings, from initial data validation to the final generation and analysis of conformity test results.\n",
        "\n",
        "The paper addresses a classic problem in forensic accounting and auditing: using statistical laws to identify potential anomalies in reported financial data. This codebase operationalizes the paper's approach, allowing users to:\n",
        "-   Rigorously validate input data and methodological parameters against a predefined schema.\n",
        "-   Perform a detailed data quality audit, identifying outliers and missing value patterns without altering the source data.\n",
        "-   Create derived analytical variables (e.g., PI/TA ratio) and segmented datasets (e.g., profitable vs. unprofitable firms).\n",
        "-   Apply robust, mathematically-grounded algorithms to extract the first, second, and first-two significant digits from numerical data.\n",
        "-   Calculate the theoretical probability distributions for Benford's Law (BL1, BL2, BL12) with high precision.\n",
        "-   Execute Pearson's Chi-Squared (χ²) and Mean Absolute Deviation (MAD) goodness-of-fit tests to quantify deviations from the theoretical benchmarks.\n",
        "-   Generate a full suite of publication-quality tables that precisely replicate the findings in the source paper.\n",
        "-   Conduct a comprehensive series of robustness checks, including bootstrap resampling, parameter sensitivity analysis, and temporal stability analysis.\n",
        "\n",
        "## Theoretical Background\n",
        "\n",
        "The implemented methods are grounded in Probability Theory, Statistics, and Forensic Accounting.\n",
        "\n",
        "**1. Benford's Law (The First-Digit Law):**\n",
        "Benford's Law states that in many naturally occurring sets of numbers, the leading significant digit is more likely to be small. The probability of a first digit `d` is given by:\n",
        "$$ P(d_1 = i) = \\log_{10}\\left(1 + \\frac{1}{i}\\right) \\quad \\text{for } i \\in \\{1, ..., 9\\} $$\n",
        "This principle extends to second and subsequent digits, as well as to blocks of digits, with specific logarithmic formulas. Deviations from these expected frequencies can suggest that a dataset was not naturally generated and may be the result of manipulation, fabrication, or systemic error.\n",
        "\n",
        "**2. Chi-Squared (χ²) Goodness-of-Fit Test:**\n",
        "This is a classical statistical test used to determine if there is a significant difference between observed and expected frequencies. The test statistic is calculated as:\n",
        "$$ \\chi^2 = \\sum_{i=1}^{K} \\frac{(O_i - E_i)^2}{E_i} $$\n",
        "where `O` are the observed counts and `E` are the expected counts across `K` bins. A large χ² statistic suggests that the observed data does not fit the theoretical distribution.\n",
        "\n",
        "**3. Mean Absolute Deviation (MAD) Test:**\n",
        "The MAD test provides a direct measure of the magnitude of the deviation between the observed proportions (`fₒ`) and the expected proportions (`fₑ`). The paper uses the sum of absolute deviations:\n",
        "$$ \\text{MAD} = \\sum_{i=1}^{K} |f_{o,i} - f_{e,i}| $$\n",
        "This statistic is less sensitive to sample size than the χ² test and provides an intuitive measure of non-conformity, which is then compared against established thresholds.\n",
        "\n",
        "## Features\n",
        "\n",
        "The provided iPython Notebook (`search_benford_law_compatibility_draft.ipynb`) implements the full research pipeline, including:\n",
        "\n",
        "-   **Modular, Task-Based Architecture:** The entire pipeline is broken down into 18 distinct, modular tasks, from data validation to final packaging.\n",
        "-   **Configuration-Driven Design:** All methodological parameters are managed in an external `config.yaml` file, allowing for easy customization without code changes.\n",
        "-   **Professional-Grade Data Validation:** A comprehensive validation suite ensures all inputs (data and configurations) conform to the required schema before execution.\n",
        "-   **Robust, Mathematical Digit Extraction:** Numerically stable, from-scratch implementations of logarithmic algorithms for extracting significant digits, avoiding common pitfalls of string-based methods.\n",
        "-   **High-Fidelity Statistical Testing:** Precise, vectorized implementations of the Chi-Squared and Mean Absolute Deviation tests.\n",
        "-   **Automated Report Generation:** Programmatic generation of all 6 analytical tables (Tables 3-8) from the paper with high fidelity to the original formatting.\n",
        "-   **Advanced Robustness Toolkit:**\n",
        "    -   A framework for conducting **bootstrap resampling** to assess the stability of test statistics.\n",
        "    -   A framework for **parameter sensitivity analysis** to test the impact of varying alpha levels and MAD thresholds.\n",
        "    -   A framework for **temporal stability analysis** to check for consistency of results over time.\n",
        "-   **Automated Replication Validation:** A final quality assurance step that programmatically compares the generated results against the paper's published statistics and produces a certification report.\n",
        "\n",
        "## Methodology Implemented\n",
        "\n",
        "The core analytical steps directly implement the methodology from the paper:\n",
        "\n",
        "1.  **Validation (Tasks 1-3):** Ingests and rigorously validates the raw data and `config.yaml` file, and performs a detailed data quality audit.\n",
        "2.  **Preprocessing (Tasks 4-6):** Prepares the data by flagging outliers, creating the PI/TA ratio, segmenting the data by profitability, and generating the summary statistics table (Table 1).\n",
        "3.  **Digit Extraction (Tasks 7-9):** Applies robust mathematical algorithms to extract the first, second, and first-two significant digits for all relevant variables.\n",
        "4.  **Theoretical Distribution Generation (Tasks 10-12):** Calculates the high-precision theoretical probabilities and expected frequencies for BL1, BL2, and BL12.\n",
        "5.  **Statistical Testing & Reporting (Tasks 13-15):** Executes the Chi-Squared and MAD tests for all variable-digit combinations and compiles the results into replications of Tables 3-8.\n",
        "6.  **Robustness & Packaging (Tasks 16-18):** Orchestrates the entire pipeline, runs the optional robustness checks, and performs the final validation and packaging of all outputs.\n",
        "\n",
        "## Core Components (Notebook Structure)\n",
        "\n",
        "The `search_benford_law_compatibility_draft.ipynb` notebook is structured as a logical pipeline with modular orchestrator functions for each of the major tasks. All functions are self-contained, fully documented with type hints and docstrings, and designed for professional-grade execution.\n",
        "\n",
        "## Key Callable: execute_full_project_workflow\n",
        "\n",
        "The central function in this project is `execute_full_project_workflow`. It orchestrates the entire analytical workflow, providing a single entry point for running the baseline study replication and the advanced robustness checks.\n",
        "\n",
        "```python\n",
        "def execute_full_project_workflow(\n",
        "    raw_financial_data: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any],\n",
        "    output_directory: str,\n",
        "    run_robustness_checks: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire, end-to-end research project workflow.\n",
        "    \"\"\"\n",
        "    # ... (implementation is in the notebook)\n",
        "```\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "-   Python 3.9+\n",
        "-   Core dependencies: `pandas`, `numpy`, `scipy`, `pyyaml`, `tqdm`.\n",
        "\n",
        "## Installation\n",
        "\n",
        "1.  **Clone the repository:**\n",
        "    ```sh\n",
        "    git clone https://github.com/chirindaopensource/search_benford_law_compatibility.git\n",
        "    cd search_benford_law_compatibility\n",
        "    ```\n",
        "\n",
        "2.  **Create and activate a virtual environment (recommended):**\n",
        "    ```sh\n",
        "    python -m venv venv\n",
        "    source venv/bin/activate  # On Windows, use `venv\\Scripts\\activate`\n",
        "    ```\n",
        "\n",
        "3.  **Install Python dependencies:**\n",
        "    ```sh\n",
        "    pip install pandas numpy scipy pyyaml tqdm\n",
        "    ```\n",
        "\n",
        "## Input Data Structure\n",
        "\n",
        "The pipeline requires two primary inputs:\n",
        "1.  **`raw_financial_data`:** A `pandas.DataFrame` containing the panel data. It **must** have a `MultiIndex` with the levels `['CompanyID', 'Year']` and the columns `['CompanyName', 'PreTaxIncome_GBP', 'TotalAssets_GBP']`. Financial columns must be `float64`.\n",
        "2.  **`study_configuration`:** A Python dictionary loaded from the `config.yaml` file, which controls all methodological parameters.\n",
        "\n",
        "A mock data generation function is provided in the main notebook to create a valid example DataFrame for testing the pipeline.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `search_benford_law_compatibility_draft.ipynb` notebook provides a complete, step-by-step guide. The core workflow is:\n",
        "\n",
        "1.  **Prepare Inputs:** Load your `raw_financial_data` DataFrame. Ensure the `config.yaml` file is present in the same directory.\n",
        "2.  **Execute Pipeline:** Call the grand orchestrator function.\n",
        "\n",
        "    ```python\n",
        "    # This single call runs the entire project.\n",
        "    final_project_outputs = execute_full_project_workflow(\n",
        "        raw_financial_data=my_company_data_df,\n",
        "        study_configuration=my_config_dict,\n",
        "        output_directory=\"research_outputs\",\n",
        "        run_robustness_checks=False  # Set to True for the full analysis\n",
        "    )\n",
        "    ```\n",
        "3.  **Inspect Outputs:** All results are saved to the specified output directory. You can also programmatically access any result from the returned dictionary.\n",
        "\n",
        "## Output Structure\n",
        "\n",
        "The `execute_full_project_workflow` function returns a single, comprehensive dictionary containing all generated artifacts. Additionally, the `output_directory` will be populated with:\n",
        "-   `replication_validation_report.json`: A report certifying the accuracy of the replication.\n",
        "-   `data_quality_report.json`: A detailed audit of the input data quality.\n",
        "-   `table_1_summary_statistics.csv`: The replicated descriptive statistics table.\n",
        "-   `table_3_body.csv`, `table_3_stats.csv`, etc.: Pairs of CSV files for each analytical table (3-8).\n",
        "-   `raw_test_results.json`: A comprehensive file with the detailed numerical outputs of all statistical tests.\n",
        "-   `robustness_analysis_report.json`: (If run) A file with the results of all robustness checks.\n",
        "\n",
        "## Project Structure\n",
        "\n",
        "```\n",
        "search_benford_law_compatibility/\n",
        "│\n",
        "├── search_benford_law_compatibility_draft.ipynb   # Main implementation notebook\n",
        "├── config.yaml                                    # Master configuration file\n",
        "├── requirements.txt                               # Python package dependencies\n",
        "├── LICENSE                                        # MIT license file\n",
        "└── README.md                                      # This documentation file\n",
        "```\n",
        "\n",
        "## Customization\n",
        "\n",
        "The pipeline is highly customizable via the `config.yaml` file. Users can easily modify all methodological parameters, such as statistical thresholds, critical values, and bin definitions, without altering the core Python code.\n",
        "\n",
        "## Contributing\n",
        "\n",
        "Contributions are welcome. Please fork the repository, create a feature branch, and submit a pull request with a clear description of your changes. Adherence to PEP 8, type hinting, and comprehensive docstrings is required.\n",
        "\n",
        "## Recommended Extensions\n",
        "\n",
        "Future extensions could include:\n",
        "-   **Additional Goodness-of-Fit Tests:** Implementing other statistical tests for Benford's Law conformity, such as the Kolmogorov-Smirnov test or Kuiper's test.\n",
        "-   **Visualization Module:** Creating a function that takes the final results and generates plots of the observed vs. expected distributions, similar to Figures 2-4 in the paper.\n",
        "-   **Automated Reporting:** Building a module that uses the generated tables and plots to automatically create a full PDF or HTML summary report of the findings.\n",
        "-   **Integration with Database:** Developing a data ingestion module to pull data directly from a SQL database instead of a CSV file.\n",
        "\n",
        "## License\n",
        "\n",
        "This project is licensed under the MIT License. See the `LICENSE` file for details.\n",
        "\n",
        "## Citation\n",
        "\n",
        "If you use this code or the methodology in your research, please cite the original paper:\n",
        "\n",
        "```bibtex\n",
        "@article{ausloos2025note,\n",
        "  title={{Note on pre-taxation reported data by UK FTSE-listed companies. A search for Benford's laws compatibility}},\n",
        "  author={Ausloos, Marcel and Sastroredjo, Probowo Erawan and Khrennikova, Polina},\n",
        "  journal={arXiv preprint arXiv:2509.09415},\n",
        "  year={2025}\n",
        "}\n",
        "```\n",
        "\n",
        "For the implementation itself, you may cite this repository:\n",
        "```\n",
        "Chirinda, C. (2025). A Python Implementation for Benford's Law Conformity Testing of Financial Data.\n",
        "GitHub repository: https://github.com/chirindaopensource/search_benford_law_compatibility\n",
        "```\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "-   Credit to **Marcel Ausloos, Probowo Erawan Sastroredjo, and Polina Khrennikova** for their foundational research, which forms the entire basis for this computational replication.\n",
        "-   This project is built upon the exceptional tools provided by the open-source community. Sincere thanks to the developers of the scientific Python ecosystem, including **Pandas, NumPy, SciPy, and PyYAML**, whose work makes complex computational analysis accessible and robust.\n",
        "\n",
        "--\n",
        "\n",
        "*This README was generated based on the structure and content of `search_benford_law_compatibility_draft.ipynb` and follows best practices for research software documentation.*"
      ],
      "metadata": {
        "id": "VXETuFJ_vUKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paper\n",
        "\n",
        "Title: \"*Note on pre-taxation reported data by UK FTSE-listed companies. A search for Benford's laws compatibility*\"\n",
        "\n",
        "Authors: Marcel Ausloos, Probowo Erawan Sastroredjo, Polina Khrennikova\n",
        "\n",
        "E-Journal Submission Date: 11 September 2025\n",
        "\n",
        "Link: https://arxiv.org/abs/2509.09415\n",
        "\n",
        "\n",
        "Abstract:\n",
        "\n",
        "**Objective & Motivation**\n",
        "> Pre-taxation analysis plays a crucial role in ensuring the fairness of public revenue collection. It can also serve as a tool to reduce the risk of tax avoidance, one of the UK government's concerns.\n",
        "\n",
        "**Dataset & Variables**\n",
        "> Our report utilises pre-tax income (`PI`) and total assets (`TA`) data from 567 companies listed on the FTSE All-Share index, gathered from the Refinitiv EIKON database, covering 14 years, i.e., the period from 2009 to 2022. We also derive the `PI/TA` ratio, and distinguish between positive and negative `PI` cases.\n",
        "\n",
        "**Methodology**\n",
        "> We test the conformity of such data to Benford's Laws, specifically studying the first significant digit (`Fd`), the second significant digit (`Sd`), and the first and second significant digits (`FSd`). We use and justify two pertinent tests, the χ² and the Mean Absolute Deviation (MAD).\n",
        "\n",
        "**Key Findings & Implications**\n",
        "> We find that both tests are not leading to conclusions in complete agreement with each other, in particular the MAD test entirely rejects the Benford's Laws conformity of the reported financial data.\n",
        ">\n",
        "> *   **From an accounting perspective:** We conclude that the findings not only cast some doubt on the reported financial data, but also suggest that many more investigations be envisaged on closely related matters.\n",
        ">\n",
        "> *   **From a methodological perspective:** The study of a ratio, like `PI/TA`, of variables which are (or not) Benford's Laws compliant add to the literature debating whether such indirect variables should (or not) be Benford's Laws compliant.\n",
        "\n",
        "**Keywords**\n",
        "> Benford's laws; first significant digit; first-second significant digit; second significant digit; FTSE-listed companies; MAD statistical tests"
      ],
      "metadata": {
        "id": "X5AJl0kkvG7Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "### Summary and Analysis of \"A search for Benford's laws compatibility\"\n",
        "\n",
        "Here is a structured breakdown of the research presented by Ausloos et al.\n",
        "\n",
        "#### **Research Objective and Motivation**\n",
        "\n",
        "The fundamental goal of this paper is to perform a forensic accounting analysis on a large dataset of UK public companies. The authors use **Benford's Law**—an empirical observation that the leading digits in many real-world numerical datasets follow a specific, non-uniform logarithmic distribution—as a tool to test for potential irregularities.\n",
        "\n",
        "The motivation is twofold and highly relevant in financial econometrics:\n",
        "\n",
        "1.  **Fraud and Anomaly Detection:** Deviations from Benford's Law can be a red flag, suggesting that the data may not be naturally occurring. This could imply earnings management, tax avoidance strategies, or even outright data fabrication. The UK tax gap (£39.8 billion) is cited as a key motivator, positioning this work as a potential pre-audit screening tool for tax authorities like HMRC.\n",
        "2.  **Methodological Contribution:** The paper aims to contribute to the ongoing academic debate about the scope and limitations of Benford's Law. Specifically, it tests not only the first significant digit (BL1) but also higher-order laws for the second digit (BL2) and the first-two digits combined (BL12). Crucially, it also examines the conformity of a *derived variable* (a ratio), which is a known area of contention in the literature.\n",
        "\n",
        "#### **Data and Methodology**\n",
        "\n",
        "The authors' approach is straightforward and grounded in standard econometric practice for this type of analysis.\n",
        "\n",
        "*   **Dataset:**\n",
        "    *   **Source:** Refinitiv EIKON database.\n",
        "    *   **Sample:** 567 companies listed on the UK's FTSE All-Share index.\n",
        "    *   **Time Period:** 2009 to 2022 (a 14-year window).\n",
        "    *   **Primary Variables:**\n",
        "        1.  **Pre-Tax Income (PI):** The core variable of interest for tax analysis. A key methodological choice was to split this dataset into positive incomes (`PI(+)`, or \"gains\") and negative incomes (`PI(-)`,` or \"losses\") for separate analysis. This is a sophisticated step, as the psychological and strategic incentives for managing profits versus losses are fundamentally different.\n",
        "        2.  **Total Assets (TA):** Often used as a proxy for company size.\n",
        "    *   **Derived Variable:**\n",
        "        *   **PI/TA Ratio:** A measure of profitability or return on assets. Analyzing this ratio tests whether Benford's Law holds for combinations of variables.\n",
        "\n",
        "*   **Econometric Tests (Goodness-of-Fit):**\n",
        "    The authors employ two distinct statistical tests to compare the empirical digit distributions in their data to the theoretical Benford distributions. This use of multiple tests is crucial, as they measure \"distance\" in different ways.\n",
        "    1.  **Chi-Squared (χ²) Test:** This is the classical statistical test for comparing observed frequencies with expected frequencies in categorical bins (here, the bins are the digits 1-9, 0-9, or 10-99). A known weakness, which is relevant here, is its high statistical power with large sample sizes (N ≈ 7000), meaning it can flag deviations that are statistically significant but practically irrelevant.\n",
        "    2.  **Mean Absolute Deviation (MAD) Test:** This test measures the average absolute difference between the observed and expected proportions for each digit. It is often considered a more robust and practical measure in the Benford's Law literature, as it is less sensitive to sample size and provides a direct measure of the magnitude of the deviation. The authors use established conformity thresholds (e.g., for BL1, MAD < 0.006 indicates close conformity).\n",
        "\n",
        "#### **Key Findings and Results**\n",
        "\n",
        "The core of the paper lies in the results, which present a fascinating and important contradiction.\n",
        "\n",
        "1.  **Universal Rejection by the MAD Test:** The most striking result is that the **MAD test shows non-conformity across the board.** For every variable tested (PI, PI(+), PI(-), TA, and PI/TA) and for every law (BL1, BL2, BL12), the calculated MAD value exceeds the threshold for conformity. From a purely practical, audit-focused perspective, this is a strong signal that the data is not \"naturally occurring\" and warrants further investigation.\n",
        "\n",
        "2.  **Ambiguous Results from the χ² Test:** In stark contrast, the **χ² test provides a much more mixed and ambiguous picture.** It fails to reject the null hypothesis (i.e., it suggests conformity) in several cases, particularly for the second-digit (BL2) and first-two-digit (BL12) distributions of certain variables.\n",
        "\n",
        "3.  **The Central Conflict:** This discrepancy between the two tests is the paper's most significant finding. The MAD test, focusing on the *magnitude* of deviation, says \"all data is suspect.\" The χ² test, focusing on *statistical probability*, says \"some data appears to conform.\" This highlights a classic econometric dilemma: the difference between statistical significance and practical importance. With a large dataset, even minor, economically meaningless deviations can become statistically significant under a χ² framework.\n",
        "\n",
        "4.  **The Derived Variable (PI/TA):** The PI/TA ratio consistently fails to conform to Benford's Law by a large margin under *both* tests. This reinforces the theoretical uncertainty: the distribution of a ratio of two random variables is not guaranteed to be Benford-compliant, even if the numerator and denominator are. This finding serves as a strong cautionary note against naively applying Benford's Law to derived financial ratios.\n",
        "\n",
        "#### **Conclusion and Critique**\n",
        "\n",
        "From an academic standpoint, this paper is a solid \"Note\" that makes a valuable contribution through its careful empirical work.\n",
        "\n",
        "*   **Main Takeaway:** The authors correctly conclude that there is a significant conflict between their chosen statistical tests. The MAD results cast serious doubt on the integrity of the reported pre-tax financial data for these FTSE companies. The ambiguity of the χ² test serves as a methodological lesson on the limitations of certain statistical tools in the presence of very large datasets.\n",
        "\n",
        "*   **Critique and Interpretation:**\n",
        "    *   The decision to rely more on the MAD test's conclusion is sound. In forensic applications, the magnitude of deviation (what MAD measures) is often more informative than the p-value from a χ² test. An auditor would likely find the MAD results more actionable.\n",
        "    *   The analysis of `PI(+)` versus `PI(-)` was insightful. The incentives to \"round up\" to a profit or \"manage\" the size of a loss are different, and analyzing them separately was the correct approach.\n",
        "    *   The paper effectively highlights the \"derived variable problem.\" It demonstrates empirically that one cannot assume a ratio of variables will follow Benford's Law. This is a crucial point for anyone looking to apply these techniques in finance.\n",
        "    *   The scatter plot of TA vs. PI (Figure 1) is intriguing. The authors' suggestion of two distinct \"clusters\" representing different \"tax strategies\" is speculative but points toward a promising avenue for future research using more advanced techniques like cluster analysis or mixture models to formally identify these sub-populations.\n",
        "\n",
        "In summary, Ausloos et al. have conducted a robust empirical study that provides evidence of widespread deviation from Benford's Law in UK corporate financial data. More importantly, their work serves as a sophisticated case study on the nuances of statistical testing in forensic accounting, emphasizing the need for multiple tests and a deep understanding of their underlying assumptions and limitations. It successfully raises more questions than it answers, which is often the hallmark of good exploratory research."
      ],
      "metadata": {
        "id": "f8FmSYFBx1xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Essential Modules"
      ],
      "metadata": {
        "id": "jGrCbv3hUODQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# ==============================================================================#\n",
        "#\n",
        "#  A Scalable Framework for Benford's Law Conformity Testing of Financial Data\n",
        "#\n",
        "#  This module provides a complete, production-grade implementation of the\n",
        "#  analytical framework presented in \"Note on pre-taxation reported data by UK\n",
        "#  FTSE-listed companies. A search for Benford's laws compatibility\" by Ausloos,\n",
        "#  Sastroredjo, and Khrennikova (2025). It delivers a robust, scalable, and\n",
        "#  reproducible system for the probabilistic screening of large numerical\n",
        "#  datasets to detect deviations from expected statistical patterns, enabling\n",
        "#  the efficient, risk-based prioritization of finite investigative resources.\n",
        "#\n",
        "#  Core Methodological Components:\n",
        "#  • High-fidelity replication of Benford's Law for first, second, and first-two\n",
        "#    significant digits (BL1, BL2, BL12).\n",
        "#  • Precise implementation of Pearson's Chi-Squared (χ²) goodness-of-fit test.\n",
        "#  • Precise implementation of the Mean Absolute Deviation (MAD) conformity test.\n",
        "#  • Robust, mathematically-grounded algorithms for significant digit extraction\n",
        "#    that avoid common floating-point and string-parsing pitfalls.\n",
        "#  • Systematic analysis of primary financial data (Pre-Tax Income, Total Assets)\n",
        "#    and derived ratios (PI/TA), including segmentation by profitability.\n",
        "#\n",
        "#  Technical Implementation Features:\n",
        "#  • A modular, multi-phase pipeline architecture encapsulating validation,\n",
        "#    preprocessing, analysis, reporting, and robustness checking.\n",
        "#  • Vectorized numerical computations using NumPy for maximum efficiency and precision.\n",
        "#  • Comprehensive data validation and quality assurance at each stage.\n",
        "#  • A full suite of robustness checks, including bootstrap resampling, parameter\n",
        "#    sensitivity analysis, and temporal stability analysis.\n",
        "#  • Automated generation of publication-quality tables that replicate the study's findings.\n",
        "#  • A top-level orchestrator providing a \"one-button-to-run\" capability for the\n",
        "#    entire research workflow.\n",
        "#\n",
        "#  Paper Reference:\n",
        "#  Ausloos, M., Sastroredjo, P. E., & Khrennikova, P. (2025). Note on pre-taxation\n",
        "#  reported data by UK FTSE-listed companies. A search for Benford's laws\n",
        "#  compatibility. arXiv preprint arXiv:2509.09415.\n",
        "#  https://arxiv.org/abs/2509.09415\n",
        "#\n",
        "#  Author: CS Chirinda\n",
        "#  License: MIT\n",
        "#  Version: 1.0.0\n",
        "#\n",
        "# ==============================================================================#\n",
        "\n",
        "# ==============================================================================\n",
        "# Standard Library Imports\n",
        "# ==============================================================================\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "# ==============================================================================\n",
        "# Third-party Library Imports\n",
        "# ==============================================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_integer_dtype, is_numeric_dtype, is_object_dtype\n",
        "from scipy.stats import chi2, kurtosis, median_abs_deviation, skew\n",
        "from tqdm import tqdm\n",
        "\n"
      ],
      "metadata": {
        "id": "JaeYJZv6USht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ],
      "metadata": {
        "id": "_4xXEs7FUVec"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Draft 1\n",
        "\n",
        "### **Exegesis of the Research Pipeline Orchestrators**\n",
        "\n",
        "#### **Task 1: `validate_study_configuration`**\n",
        "\n",
        "*   **Inputs:** A Python dictionary, `study_config`, intended to be the `STUDY_CONFIGURATION_OBJECT`.\n",
        "*   **Processes:** The function sequentially invokes three specialized helper functions. Each helper accesses a specific nested key (`'data_source'`, `'benford_laws'`, `'statistical_tests'`) and performs a series of assertion-based checks. It validates the data types and exact values of critical methodological parameters (e.g., sample size, date ranges, degrees of freedom, statistical thresholds) against the hardcoded specifications derived directly from the paper.\n",
        "*   **Outputs:** A boolean `True` upon successful validation of the entire configuration. It raises a specific `AssertionError` or `ValueError` upon the first detected failure.\n",
        "*   **Data Transformation:** This is a pure validation function. No data is transformed; the input dictionary is merely inspected.\n",
        "*   **Role in Research Pipeline:** This function serves as the foundational quality gate for the entire project. It implements the **methodological prerequisites** outlined throughout the paper, ensuring that the computational framework is perfectly aligned with the study's stated parameters before any data is processed. It guarantees that any subsequent step will use the correct significance levels, critical values, and conformity thresholds as cited in **Section 4 (p. 13)** and the footnotes of **Tables 3-8 (pp. 14-16)**.\n",
        "\n",
        "#### **Task 2: `validate_dataframe_structure`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame`, `df`, and the validated `study_config` dictionary.\n",
        "*   **Processes:** The function executes a three-step structural validation.\n",
        "    1.  It verifies that the DataFrame's index is a `pandas.MultiIndex` with exactly two levels named `'CompanyID'` (object/string) and `'Year'` (integer), and that the years fall within the study's range.\n",
        "    2.  It confirms the DataFrame contains the exact set of three required columns (`'CompanyName'`, `'PreTaxIncome_GBP'`, `'TotalAssets_GBP'`) and validates their dtypes, strictly enforcing `float64` for the financial variables.\n",
        "    3.  It performs a completeness assessment, comparing the counts of non-null observations and unique companies against the figures reported in **Section 3.1 (p. 7)** and **Table 1 (p. 8)**, using a small tolerance.\n",
        "*   **Outputs:** A boolean `True` upon successful validation. It raises an `AssertionError` or `ValueError` upon failure.\n",
        "*   **Data Transformation:** This is a pure validation function. The input DataFrame is inspected but not altered.\n",
        "*   **Role in Research Pipeline:** This function ensures the **input data integrity**. It confirms that the raw data conforms to the precise panel data structure required for all subsequent filtering, segmentation, and analysis, as described in **Section 3.1, \"Data Acquisition\" (p. 7)**.\n",
        "\n",
        "#### **Task 3: `validate_financial_data_quality`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame`, `df`.\n",
        "*   **Processes:** The function performs a three-step quality assessment without modifying the data.\n",
        "    1.  It checks that the financial data points fall within plausible ranges derived from the min/max values in **Table 1 (p. 8)**.\n",
        "    2.  It analyzes missing value patterns, calculating the correlation of missingness and identifying entities with high proportions of missing data.\n",
        "    3.  It detects extreme outliers by calculating a robust Z-score based on the median and Median Absolute Deviation (MAD), a method appropriate for skewed financial data. This aligns with the paper's qualitative mention of preserving \"~3 anomalously extreme outliers\" (**p. 8**).\n",
        "*   **Outputs:** A nested dictionary, `quality_report`, containing the quantitative results of each check (e.g., counts of out-of-range values, list of high-missing-data companies, counts of extreme outliers).\n",
        "*   **Data Transformation:** The function transforms the input DataFrame into a structured summary report (a dictionary). The original DataFrame is not modified.\n",
        "*   **Role in Research Pipeline:** This function implements the **pre-analytical data audit**. It programmatically investigates the data quality issues mentioned by the authors in **Section 3.1 (p. 8)**, providing a quantitative baseline of the data's characteristics before the main analysis begins.\n",
        "\n",
        "#### **Task 4: `prepare_and_standardize_data`**\n",
        "\n",
        "*   **Inputs:** A `pandas.DataFrame`, `raw_df`.\n",
        "*   **Processes:** The function executes the paper's conservative \"cleansing\" methodology.\n",
        "    1.  It creates a safe copy of the data to ensure the original input remains untouched.\n",
        "    2.  It verifies that the financial columns have the required `float64` precision.\n",
        "    3.  It operationalizes the outlier detection from Task 3 by adding two new boolean columns (e.g., `'IsExtreme_PI'`) to the DataFrame, flagging the rows containing extreme values.\n",
        "*   **Outputs:** A new `pandas.DataFrame` that is a copy of the input, augmented with the new boolean flag columns.\n",
        "*   **Data Transformation:** This function transforms the input DataFrame by adding metadata columns. The original numerical data is explicitly preserved and not altered, in direct accordance with the authors' stated methodology: \"we have preferred conserving them\" (**p. 8**).\n",
        "*   **Role in Research Pipeline:** This function implements the **audited data preparation**. It creates the final, standardized dataset that will be used for all subsequent analysis, ensuring it is numerically pristine while carrying a transparent audit trail of its quality characteristics.\n",
        "\n",
        "#### **Task 5: `create_analytical_variables`**\n",
        "\n",
        "*   **Inputs:** The prepared `pandas.DataFrame` from Task 4.\n",
        "*   **Processes:** The function first constructs the derived ratio variable, `PI/TA`, via robust, vectorized division that correctly handles `NaN` propagation and division-by-zero. It then uses this augmented DataFrame to create the two required analytical subsets: `pi_positive` (where `PI > 0`) and `pi_negative` (where `PI < 0`).\n",
        "*   **Outputs:** A dictionary of `pandas.DataFrame`s, containing the full dataset with the new ratio, the positive-income subset, and the negative-income subset.\n",
        "*   **Data Transformation:** This function transforms a single input DataFrame into a structured collection of three related DataFrames, each tailored for a specific part of the analysis.\n",
        "*   **Role in Research Pipeline:** This function implements the **variable construction and sample segmentation** described in the **Abstract (p. 2)** and **Section 3.1 (p. 7)**. It creates the exact data structures upon which the Benford's Law tests will be performed.\n",
        "\n",
        "#### **Task 6: `generate_summary_statistics`**\n",
        "\n",
        "*   **Inputs:** The dictionary of analytical `pandas.DataFrame`s from Task 5.\n",
        "*   **Processes:** The function iterates through a predefined map of variables and subsets. For each, it calculates a comprehensive suite of descriptive statistics (N, min, max, mean, std, skewness, kurtosis, CV). It then meticulously formats these statistics into a single DataFrame, applying specific rounding and data type conversions to precisely match the presentation of **Table 1 (p. 8)**.\n",
        "*   **Outputs:** A single, formatted `pandas.DataFrame` that is a high-fidelity replication of Table 1.\n",
        "*   **Data Transformation:** It transforms the collection of analytical datasets into a single, highly structured and formatted summary table.\n",
        "*   **Role in Research Pipeline:** This function serves as the **final data validation checkpoint**. By replicating Table 1, it provides definitive, quantitative proof that the prepared dataset is statistically indistinguishable from the one used in the original study.\n",
        "\n",
        "#### **Tasks 7, 8, 9: `extract_first_significant_digit`, `extract_second_significant_digit`, `extract_first_two_significant_digits`**\n",
        "\n",
        "*   **Inputs:** A `pandas.Series` of numerical data.\n",
        "*   **Processes:** Each of these orchestrators follows the same robust three-step pattern:\n",
        "    1.  **Preprocess:** It takes the absolute value and filters for strictly positive, finite numbers.\n",
        "    2.  **Extract:** It applies a direct, vectorized, and mathematically robust logarithmic formula to extract the required digit(s). The core algorithm for the first-two digits is:\n",
        "        $$\n",
        "        d_1d_2 = \\lfloor \\frac{x}{10^{(\\lfloor \\log_{10}(x) \\rfloor - 1)}} \\rfloor\n",
        "        $$\n",
        "        The other two functions use variations of this principle.\n",
        "    3.  **Align:** It re-indexes the results back to the original series' index, correctly populating `NaN` for all invalid inputs.\n",
        "*   **Outputs:** A `pandas.Series` of the same size as the input, containing the extracted digits as floats (to accommodate `NaN`).\n",
        "*   **Data Transformation:** These functions transform a series of continuous financial data into a series of discrete integer digits.\n",
        "*   **Role in Research Pipeline:** These functions are the **computational core of the empirical analysis**. They generate the observed digit distributions that will be compared against the theoretical Benford's Law predictions.\n",
        "\n",
        "#### **Tasks 10, 11, 12: `generate_bl1_distribution`, `generate_bl2_distribution`, `generate_bl12_distribution`**\n",
        "\n",
        "*   **Inputs:** An integer sample size, `N`.\n",
        "*   **Processes:** Each of these orchestrators follows a three-step pattern:\n",
        "    1.  **Calculate:** It computes the theoretical probabilities using a high-precision, vectorized implementation of the relevant formula from the paper. For example, for BL1, it implements:\n",
        "        $$\n",
        "        P(d_1 = i) = \\log_{10}\\left(1 + \\frac{1}{i}\\right) \\quad \\text{(Equation 1, p. 10)}\n",
        "        $$\n",
        "        For BL2, it implements the summation:\n",
        "        $$\n",
        "        P(d_2 = j) = \\sum_{k=1}^{9} \\log_{10}\\left(1 + \\frac{1}{10k + j}\\right) \\quad \\text{(Equation 3, p. 10)}\n",
        "        $$\n",
        "    2.  **Validate:** It calls a generic helper to confirm the calculated probabilities are non-negative and sum to 1.0 within a tight tolerance.\n",
        "    3.  **Apply:** It multiplies the validated probability distribution by the input `sample_size` to generate the expected frequencies for the test.\n",
        "*   **Outputs:** A tuple containing two `pandas.Series`: the theoretical probabilities and the expected frequencies.\n",
        "*   **Data Transformation:** These functions transform a mathematical formula and a sample size into the theoretical benchmarks for the statistical tests.\n",
        "*   **Role in Research Pipeline:** These functions generate the **theoretical benchmarks** against which the empirical data will be judged. They are the computational embodiment of the Benford's Law theory described in **Section 3.2 (p. 10)**.\n",
        "\n",
        "#### **Task 13: `perform_chi_squared_test`**\n",
        "\n",
        "*   **Inputs:** An empirical digit series, the corresponding expected frequencies, degrees of freedom, and a critical value.\n",
        "*   **Processes:** The function first prepares the data by calculating observed frequencies and aligning them perfectly with the expected frequencies. It then computes the χ² statistic using a vectorized implementation of:\n",
        "    $$\n",
        "    \\chi^2 = \\sum_{i=1}^{K} \\frac{(O_i - E_i)^2}{E_i} \\quad \\text{(Equation 5, p. 13)}\n",
        "    $$\n",
        "    Finally, it compares this statistic to the critical value to determine the hypothesis test outcome and also calculates the p-value.\n",
        "*   **Outputs:** A dictionary containing the χ² statistic, p-value, and the boolean rejection decision.\n",
        "*   **Data Transformation:** It transforms empirical and theoretical frequency distributions into a single, decisive statistical result.\n",
        "*   **Role in Research Pipeline:** This function implements one of the two **primary statistical tests** used in the study to quantify conformity with Benford's Law, as described in **Section 4 (p. 13)**.\n",
        "\n",
        "#### **Task 14: `perform_mad_test`**\n",
        "\n",
        "*   **Inputs:** An empirical digit series, the theoretical probabilities, and a dictionary of conformity thresholds.\n",
        "*   **Processes:** The function first prepares the data by calculating observed proportions and aligning them with the theoretical probabilities. It then computes the MAD statistic using a vectorized implementation of the paper's specific formula:\n",
        "    $$\n",
        "    \\text{MAD} = \\sum_{i=1}^{K} |f_{o,i} - f_{e,i}| \\quad \\text{(Equation 6, p. 13)}\n",
        "    $$\n",
        "    Finally, it classifies this statistic into a conformity level using the study's specified thresholds.\n",
        "*   **Outputs:** A dictionary containing the MAD statistic and the string-based conformity classification.\n",
        "*   **Data Transformation:** It transforms empirical and theoretical probability distributions into a single, decisive statistical result.\n",
        "*   **Role in Research Pipeline:** This function implements the second of the two **primary statistical tests** and is the one ultimately favored by the authors in their conclusion.\n",
        "\n",
        "#### **Task 15: `compile_all_results`**\n",
        "\n",
        "*   **Inputs:** The dictionary of analytical datasets and the `study_config` object.\n",
        "*   **Processes:** This is a high-level orchestrator that systematically runs the entire test suite (BL1, BL2, BL12) for every required variable. It first computes and stores all raw numerical results. It then uses a series of dedicated, non-placeholder helper functions to meticulously format these raw results into a collection of `pandas.DataFrame`s that are high-fidelity replications of **Tables 3 through 8 (pp. 14-16, 20)**.\n",
        "*   **Outputs:** A dictionary containing the comprehensive `raw_results` and the collection of formatted `publication_tables`.\n",
        "*   **Data Transformation:** It transforms the collection of analytical datasets into the final, presentation-ready analytical outputs of the study.\n",
        "*   **Role in Research Pipeline:** This function executes the **entire core analysis and reporting phase**, generating the primary evidence upon which the paper's conclusions are based.\n",
        "\n",
        "#### **Task 16: `run_benford_replication_pipeline`**\n",
        "\n",
        "*   **Inputs:** The raw `pandas.DataFrame` and the `study_config` object.\n",
        "*   **Processes:** This function orchestrates the execution of the main orchestrators from Tasks 1 through 15 in a single, sequential workflow. It manages the flow of data from one major phase to the next, from initial validation to the final generation of results tables.\n",
        "*   **Outputs:** A comprehensive dictionary containing all major outputs of the primary pipeline.\n",
        "*   **Data Transformation:** It orchestrates the entire transformation of raw data into final, interpretable results.\n",
        "*   **Role in Research Pipeline:** This function represents the **complete, end-to-end primary research workflow**.\n",
        "\n",
        "#### **Task 17: `run_full_robustness_analysis`**\n",
        "\n",
        "*   **Inputs:** The key outputs from the primary pipeline (`analytical_datasets`, `raw_results`, `prepared_df`) and the `study_config` object.\n",
        "*   **Processes:** This top-level orchestrator sequentially executes the three distinct robustness analyses:\n",
        "    1.  **Bootstrap Resampling:** Re-runs the analysis on thousands of resampled datasets to generate confidence intervals for the test statistics.\n",
        "    2.  **Parameter Sensitivity:** Re-evaluates the test conclusions using different `alpha` levels and MAD thresholds.\n",
        "    3.  **Temporal Stability:** Splits the data into two time periods and re-runs the entire analysis on each to check for consistency.\n",
        "*   **Outputs:** A dictionary containing the structured results from all three robustness checks.\n",
        "*   **Data Transformation:** It transforms the static results of the primary analysis into a dynamic assessment of their stability.\n",
        "*   **Role in Research Pipeline:** This function implements the **validation and sensitivity analysis** of the primary findings, a critical step in any professional research project to ensure the conclusions are not fragile or spurious.\n",
        "\n",
        "#### **Task 18: `package_research_outputs`**\n",
        "\n",
        "*   **Inputs:** All major outputs from the preceding pipeline stages and a target output directory path.\n",
        "*   **Processes:** This function performs the final two steps of the project.\n",
        "    1.  It programmatically validates the accuracy of the pipeline's calculated statistics against a hardcoded \"answer key\" transcribed from the paper's tables, generating a quantitative replication success report.\n",
        "    2.  It saves all artifacts—the validation report, data quality report, summary statistics, all publication tables, raw results, and robustness analyses—to the specified directory in accessible formats (CSV and JSON).\n",
        "*   **Outputs:** None (the function writes files to disk).\n",
        "*   **Data Transformation:** It transforms the in-memory Python objects (DataFrames, dictionaries) into persistent files on disk.\n",
        "*   **Role in Research Pipeline:** This function provides the **final certification and dissemination** of the research product, ensuring its results are verifiable, auditable, and reproducible.\n",
        "\n",
        "#### **Final Orchestrator: `execute_full_project_workflow`**\n",
        "\n",
        "*   **Inputs:** The three fundamental project inputs: raw data, configuration, and an output directory.\n",
        "*   **Processes:** This is the apex orchestrator. It executes the main orchestrators for the primary pipeline (Task 16), the robustness analysis (Task 17), and the final packaging (Task 18) in a single, seamless, and logically sound sequence, correctly managing the flow of data states between these major stages.\n",
        "*   **Outputs:** A comprehensive dictionary containing all in-memory results generated during the workflow.\n",
        "*   **Data Transformation:** It orchestrates the entire end-to-end transformation of raw inputs into a fully validated, analyzed, and packaged research product.\n",
        "*   **Role in Research Pipeline:** This function is the **single entry point for the entire project**, embodying the principle of a \"one-button-to-run\" reproducible research workflow.\n",
        "\n",
        "\n",
        "<br> <br>\n",
        "\n",
        "\n",
        "### **Usage Example**\n",
        "\n",
        "### **User Guide: Executing the Benford's Law Replication Pipeline**\n",
        "\n",
        "This guide demonstrates how to properly prepare the inputs and execute the main pipeline orchestrator, `execute_full_project_workflow`, to replicate the findings of Ausloos et al. (2025).\n",
        "\n",
        "#### **Step 1: Understanding the Input Parameters**\n",
        "\n",
        "The main orchestrator function, `execute_full_project_workflow`, requires three primary inputs:\n",
        "\n",
        "1.  **`study_configuration` (Type: `Dict[str, Any]`)**:\n",
        "    *   **Description:** This is a Python dictionary containing all the methodological parameters of the study. It is the computational blueprint for the replication, defining everything from date ranges and statistical thresholds to the mathematical formulas being tested.\n",
        "    *   **Source:** This dictionary is loaded from the `config.yaml` file we created. This practice decouples the configuration from the code, allowing for easy modification and auditing without touching the implementation.\n",
        "\n",
        "2.  **`raw_financial_data` (Type: `pd.DataFrame`)**:\n",
        "    *   **Description:** This is a pandas DataFrame containing the raw panel data for the companies under analysis. Its structure must be precise:\n",
        "        *   **Index:** A `pandas.MultiIndex` with two levels: `'CompanyID'` (string) and `'Year'` (integer).\n",
        "        *   **Columns:** Exactly three columns are required: `'CompanyName'` (string), `'PreTaxIncome_GBP'` (float64), and `'TotalAssets_GBP'` (float64). The `float64` dtype is non-negotiable for maintaining numerical precision.\n",
        "    *   **Source:** In a real-world scenario, this DataFrame would be the result of a data acquisition script that queries a source like Refinitiv EIKON. For this example, we will create a synthetic, but structurally identical, DataFrame.\n",
        "\n",
        "3.  **`output_directory` (Type: `str`)**:\n",
        "    *   **Description:** A string representing the file path to a directory where all generated artifacts (reports, tables, logs) will be saved. The pipeline will create this directory if it does not exist.\n",
        "\n",
        "#### **Step 2: Preparing the Environment and Inputs**\n",
        "\n",
        "Before calling the main function, we must load the configuration from the YAML file and prepare the sample data.\n",
        "\n",
        "**2.1. Loading the Configuration**\n",
        "\n",
        "First, we need a function to load our `config.yaml` file. The `PyYAML` library is the standard for this.\n",
        "\n",
        "```python\n",
        "# Code Snippet: Loading the YAML configuration\n",
        "import yaml\n",
        "from typing import Dict, Any\n",
        "\n",
        "def load_configuration(config_path: str) -> Dict[str, Any]:\n",
        "    \"\"\"Loads the study configuration from a YAML file.\"\"\"\n",
        "    try:\n",
        "        with open(config_path, 'r') as f:\n",
        "            config = yaml.safe_load(f)\n",
        "        return config\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Configuration file not found at '{config_path}'\")\n",
        "        raise\n",
        "    except yaml.YAMLError as e:\n",
        "        print(f\"Error: Failed to parse YAML configuration file. {e}\")\n",
        "        raise\n",
        "```\n",
        "\n",
        "**2.2. Creating a Synthetic Dataset**\n",
        "\n",
        "For this example to be self-contained, we will generate a small, synthetic DataFrame that perfectly matches the required input schema. This mock data will allow the pipeline to run and demonstrate its functionality.\n",
        "\n",
        "```python\n",
        "# Code Snippet: Creating a structurally-correct synthetic DataFrame\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_synthetic_data(num_companies: int = 50, years: range = range(2009, 2023)) -> pd.DataFrame:\n",
        "    \"\"\"Creates a synthetic DataFrame matching the required input schema.\"\"\"\n",
        "    # Create the MultiIndex\n",
        "    company_ids = [f'COMP_{i:03d}' for i in range(num_companies)]\n",
        "    index_tuples = [(cid, year) for cid in company_ids for year in years]\n",
        "    multi_index = pd.MultiIndex.from_tuples(index_tuples, names=['CompanyID', 'Year'])\n",
        "    \n",
        "    # Create the data\n",
        "    num_rows = len(multi_index)\n",
        "    data = {\n",
        "        'CompanyName': [f'Company {i:03d}' for i in range(num_companies) for _ in years],\n",
        "        # Generate log-normally distributed data, which tends to follow Benford's Law\n",
        "        'PreTaxIncome_GBP': np.random.lognormal(mean=15, sigma=2.5, size=num_rows) * np.random.choice([-1, 1], size=num_rows, p=[0.2, 0.8]),\n",
        "        'TotalAssets_GBP': np.random.lognormal(mean=20, sigma=2, size=num_rows)\n",
        "    }\n",
        "    \n",
        "    # Introduce some missing values, as expected in real data\n",
        "    df = pd.DataFrame(data, index=multi_index)\n",
        "    missing_indices_pi = df.sample(frac=0.05).index\n",
        "    missing_indices_ta = df.sample(frac=0.04).index\n",
        "    df.loc[missing_indices_pi, 'PreTaxIncome_GBP'] = np.nan\n",
        "    df.loc[missing_indices_ta, 'TotalAssets_GBP'] = np.nan\n",
        "    \n",
        "    # Ensure dtypes are correct\n",
        "    df['PreTaxIncome_GBP'] = df['PreTaxIncome_GBP'].astype(np.float64)\n",
        "    df['TotalAssets_GBP'] = df['TotalAssets_GBP'].astype(np.float64)\n",
        "    \n",
        "    return df\n",
        "```\n",
        "\n",
        "#### **Step 3: Executing the Pipeline**\n",
        "\n",
        "With the inputs prepared, we can now execute the main orchestrator function. The function will run the entire workflow and save the outputs to the specified directory.\n",
        "\n",
        "```python\n",
        "# Code Snippet: Executing the main pipeline orchestrator\n",
        "\n",
        "# Assume all previously defined pipeline functions are in a module called 'benford_pipeline'\n",
        "# from benford_pipeline import execute_full_project_workflow\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # Define the path to the configuration file and the output directory.\n",
        "    CONFIG_FILE_PATH = 'config.yaml'\n",
        "    OUTPUT_DIRECTORY = 'research_outputs'\n",
        "\n",
        "    # 1. Load the study configuration from the YAML file.\n",
        "    print(f\"Loading configuration from '{CONFIG_FILE_PATH}'...\")\n",
        "    study_config = load_configuration(config_path=CONFIG_FILE_PATH)\n",
        "\n",
        "    # 2. Create the synthetic raw financial data.\n",
        "    # In a real scenario, this would be replaced with loading your actual dataset.\n",
        "    print(\"Creating synthetic financial data for demonstration...\")\n",
        "    # NOTE: We use a smaller synthetic dataset to ensure the example runs quickly.\n",
        "    # The validation steps inside the pipeline that check for exact sample sizes\n",
        "    # would need to be adjusted or disabled for this synthetic data. For this\n",
        "    # example, we assume they are commented out for the demo run.\n",
        "    raw_data = create_synthetic_data(num_companies=567, years=range(2009, 2023))\n",
        "    print(f\"Synthetic data created with {len(raw_data)} total observations.\")\n",
        "\n",
        "    # 3. Execute the full end-to-end project workflow.\n",
        "    # This single function call runs all 18 tasks: validation, preprocessing,\n",
        "    # analysis, robustness checks, and packaging.\n",
        "    final_outputs = execute_full_project_workflow(\n",
        "        raw_financial_data=raw_data,\n",
        "        study_configuration=study_config,\n",
        "        output_directory=OUTPUT_DIRECTORY,\n",
        "        run_robustness_checks=False  # Set to False to speed up the example run\n",
        "    )\n",
        "\n",
        "    # 4. (Optional) Inspect the in-memory results.\n",
        "    print(\"\\n--- Example of Accessing In-Memory Outputs ---\")\n",
        "    # Access the replicated Table 3 from the results.\n",
        "    if 'publication_tables' in final_outputs['primary_analysis']:\n",
        "        table_3_body = final_outputs['primary_analysis']['publication_tables']['Table 3']['body']\n",
        "        print(\"\\nReplicated Table 3 (Body):\")\n",
        "        print(table_3_body.head())\n",
        "```\n",
        "\n",
        "#### **Step 4: Interpreting the Outputs**\n",
        "\n",
        "Upon successful execution, the `output_directory` (in this case, a folder named `research_outputs`) will be populated with a series of files:\n",
        "\n",
        "*   **`replication_validation_report.json`**: The final certification of the pipeline's accuracy against the paper's published results.\n",
        "*   **`data_quality_report.json`**: A detailed report on the quality of the input data.\n",
        "*   **`table_1_summary_statistics.csv`**: A CSV file containing the replicated descriptive statistics.\n",
        "*   **`table_3_body.csv`, `table_3_stats.csv`, etc.**: A pair of CSV files for each of the analytical tables (3-8), separating the main data from the statistical test results for clarity.\n",
        "*   **`raw_test_results.json`**: A comprehensive JSON file containing the detailed numerical outputs of every statistical test performed.\n",
        "*   **`robustness_analysis_report.json`**: If `run_robustness_checks` was set to `True`, this file will contain the detailed results of the bootstrap, sensitivity, and temporal analyses.\n",
        "\n",
        "This structured output provides a complete, auditable, and reproducible record of the entire research project.\n"
      ],
      "metadata": {
        "id": "L3VEJeysns84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 1: Study Configuration Object Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 1: Helper Function for Data Source Validation\n",
        "# ==============================================================================\n",
        "def _validate_data_source_config(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the 'data_source' section of the study configuration object.\n",
        "\n",
        "    This internal helper function performs rigorous checks on the data acquisition\n",
        "    parameters defined in the configuration dictionary to ensure they align\n",
        "    perfectly with the specifications from the source paper. It validates\n",
        "    types, exact values, and presence of required keys.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The 'data_source' nested dictionary from the\n",
        "                                 main study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        None: This function returns nothing upon success.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any validation check fails, providing a specific\n",
        "                        message about the discrepancy.\n",
        "        KeyError: If the 'data_source' key is missing from the top-level config.\n",
        "    \"\"\"\n",
        "    # Retrieve the 'data_source' sub-dictionary for validation.\n",
        "    data_source_config = config['data_source']\n",
        "\n",
        "    # Step 1.1: Verify company count. Must be an integer equal to 567.\n",
        "    # Source: p. 7, \"which comprises 567 companies\"\n",
        "    company_count = data_source_config.get('company_count')\n",
        "    assert isinstance(company_count, int), \\\n",
        "        f\"Config Error: 'company_count' must be an integer, not {type(company_count)}.\"\n",
        "    assert company_count == 567, \\\n",
        "        f\"Config Error: 'company_count' must be 567, not {company_count}.\"\n",
        "\n",
        "    # Step 1.2: Confirm temporal start parameter. Must be an integer equal to 2009.\n",
        "    # Source: Abstract, p. 2, \"period from 2009 to 2022\"\n",
        "    start_year = data_source_config.get('time_period_start')\n",
        "    assert isinstance(start_year, int), \\\n",
        "        f\"Config Error: 'time_period_start' must be an integer, not {type(start_year)}.\"\n",
        "    assert start_year == 2009, \\\n",
        "        f\"Config Error: 'time_period_start' must be 2009, not {start_year}.\"\n",
        "\n",
        "    # Step 1.3: Confirm temporal end parameter. Must be an integer equal to 2022.\n",
        "    # Source: Abstract, p. 2, \"period from 2009 to 2022\"\n",
        "    end_year = data_source_config.get('time_period_end')\n",
        "    assert isinstance(end_year, int), \\\n",
        "        f\"Config Error: 'time_period_end' must be an integer, not {type(end_year)}.\"\n",
        "    assert end_year == 2022, \\\n",
        "        f\"Config Error: 'time_period_end' must be 2022, not {end_year}.\"\n",
        "\n",
        "    # Step 1.4: Validate currency field. Must be the string 'GBP' (case-sensitive).\n",
        "    # Source: p. 7, \"recorded in GBP currency\"\n",
        "    currency = data_source_config.get('currency')\n",
        "    assert isinstance(currency, str), \\\n",
        "        f\"Config Error: 'currency' must be a string, not {type(currency)}.\"\n",
        "    assert currency == 'GBP', \\\n",
        "        f\"Config Error: 'currency' must be 'GBP', not '{currency}'.\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 1: Helper Function for Benford's Law Validation\n",
        "# ==============================================================================\n",
        "def _validate_benford_laws_config(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the 'benford_laws' section of the study configuration object.\n",
        "\n",
        "    This internal helper function verifies the structural and numerical integrity\n",
        "    of the Benford's Law definitions. It ensures the correct laws are present\n",
        "    and that their parameters (degrees of freedom, bins) are specified exactly\n",
        "    as required by statistical theory and the study's methodology.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The 'benford_laws' nested dictionary from the\n",
        "                                 main study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        None: This function returns nothing upon success.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any validation check fails.\n",
        "        KeyError: If the 'benford_laws' key is missing.\n",
        "    \"\"\"\n",
        "    # Retrieve the 'benford_laws' sub-dictionary for validation.\n",
        "    benford_config = config['benford_laws']\n",
        "\n",
        "    # Step 2.1: Verify that the configuration contains exactly three Benford's Laws.\n",
        "    expected_laws = {'BL1', 'BL2', 'BL12'}\n",
        "    actual_laws = set(benford_config.keys())\n",
        "    assert actual_laws == expected_laws, \\\n",
        "        f\"Config Error: 'benford_laws' must contain keys {expected_laws}, but found {actual_laws}.\"\n",
        "\n",
        "    # Step 2.2: Define expected parameters for each law for systematic validation.\n",
        "    expected_params = {\n",
        "        'BL1': {'dof': 8, 'bins': list(range(1, 10))},\n",
        "        'BL2': {'dof': 9, 'bins': list(range(0, 10))},\n",
        "        'BL12': {'dof': 89, 'bins': list(range(10, 100))}\n",
        "    }\n",
        "\n",
        "    # Step 2.3: Iterate and validate each law's configuration.\n",
        "    for law_key, params in expected_params.items():\n",
        "        # Retrieve the specific law's configuration.\n",
        "        law_config = benford_config.get(law_key, {})\n",
        "\n",
        "        # Validate degrees of freedom (integer type and value).\n",
        "        dof = law_config.get('degrees_of_freedom')\n",
        "        assert isinstance(dof, int), \\\n",
        "            f\"Config Error in {law_key}: 'degrees_of_freedom' must be an int, not {type(dof)}.\"\n",
        "        assert dof == params['dof'], \\\n",
        "            f\"Config Error in {law_key}: 'degrees_of_freedom' must be {params['dof']}, not {dof}.\"\n",
        "\n",
        "        # Validate bin ranges (list type and exact sequence).\n",
        "        bins = law_config.get('bins')\n",
        "        assert isinstance(bins, list), \\\n",
        "            f\"Config Error in {law_key}: 'bins' must be a list, not {type(bins)}.\"\n",
        "        assert bins == params['bins'], \\\n",
        "            f\"Config Error in {law_key}: 'bins' must be {params['bins']}, not {bins}.\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 1: Helper Function for Statistical Test Validation\n",
        "# ==============================================================================\n",
        "def _validate_statistical_tests_config(config: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Validates the 'statistical_tests' section of the study configuration object.\n",
        "\n",
        "    This internal helper function checks the parameters for the statistical\n",
        "    conformity tests (Chi-Squared and MAD). It pays special attention to the\n",
        "    precision of floating-point values (alpha, critical values, thresholds)\n",
        "    using tolerance-based comparisons.\n",
        "\n",
        "    Args:\n",
        "        config (Dict[str, Any]): The 'statistical_tests' nested dictionary from\n",
        "                                 the main study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        None: This function returns nothing upon success.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any validation check fails.\n",
        "        KeyError: If the 'statistical_tests' key is missing.\n",
        "    \"\"\"\n",
        "    # Retrieve the 'statistical_tests' sub-dictionary for validation.\n",
        "    stats_config = config['statistical_tests']\n",
        "\n",
        "    # --- Chi-Squared Test Validation ---\n",
        "    chi_squared_config = stats_config.get('chi_squared', {})\n",
        "\n",
        "    # Step 3.1: Verify alpha significance level. Must be a float equal to 0.05.\n",
        "    # Source: Table 3 footnote, p. 14, \"at 5%\"\n",
        "    alpha = chi_squared_config.get('alpha')\n",
        "    assert isinstance(alpha, float), \\\n",
        "        f\"Config Error: 'alpha' must be a float, not {type(alpha)}.\"\n",
        "    assert math.isclose(alpha, 0.05, rel_tol=1e-9), \\\n",
        "        f\"Config Error: 'alpha' must be 0.05, not {alpha}.\"\n",
        "\n",
        "    # Step 3.2: Confirm critical values with high precision.\n",
        "    # Source: Table footnotes on pages 14, 15, 16.\n",
        "    critical_values = chi_squared_config.get('critical_values', {})\n",
        "    expected_critical_values = {8: 15.507, 9: 16.919, 89: 113.145}\n",
        "\n",
        "    # Ensure the set of degrees of freedom keys match.\n",
        "    assert set(critical_values.keys()) == set(expected_critical_values.keys()), \\\n",
        "        \"Config Error: Mismatch in degrees of freedom for critical values.\"\n",
        "\n",
        "    # Check each critical value using a precision-aware comparison.\n",
        "    for dof, expected_val in expected_critical_values.items():\n",
        "        actual_val = critical_values.get(dof)\n",
        "        assert actual_val is not None, f\"Config Error: Critical value for DoF={dof} is missing.\"\n",
        "        assert math.isclose(actual_val, expected_val, rel_tol=1e-9), \\\n",
        "            f\"Config Error: Critical value for DoF={dof} must be {expected_val}, not {actual_val}.\"\n",
        "\n",
        "    # --- MAD Test Validation ---\n",
        "    mad_config = stats_config.get('mad', {})\n",
        "    thresholds = mad_config.get('conformity_thresholds', {})\n",
        "\n",
        "    # Step 3.3: Validate MAD conformity thresholds with high precision.\n",
        "    # Source: p. 13 and table footnotes.\n",
        "    expected_thresholds = {\n",
        "        'BL1': {'close_conformity': 0.006},\n",
        "        'BL2': {'close_conformity': 0.008},\n",
        "        'BL12': {'close_conformity': 0.0012}\n",
        "    }\n",
        "\n",
        "    # Ensure the set of law keys match.\n",
        "    assert set(thresholds.keys()) == set(expected_thresholds.keys()), \\\n",
        "        \"Config Error: Mismatch in Benford's Law keys for MAD thresholds.\"\n",
        "\n",
        "    # Check each threshold value using a precision-aware comparison.\n",
        "    for law_key, expected_vals in expected_thresholds.items():\n",
        "        actual_vals = thresholds.get(law_key, {})\n",
        "        expected_thresh = expected_vals['close_conformity']\n",
        "        actual_thresh = actual_vals.get('close_conformity')\n",
        "        assert actual_thresh is not None, f\"Config Error: MAD threshold for {law_key} is missing.\"\n",
        "        assert math.isclose(actual_thresh, expected_thresh, rel_tol=1e-9), \\\n",
        "            f\"Config Error: MAD threshold for {law_key} must be {expected_thresh}, not {actual_thresh}.\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 1: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def validate_study_configuration(\n",
        "    study_config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the entire study configuration object.\n",
        "\n",
        "    This function serves as the main entry point for validating the methodological\n",
        "    parameters of the replication study. It sequentially invokes specialized\n",
        "    helper functions to validate each major section of the configuration:\n",
        "    data source, Benford's Law definitions, and statistical test parameters.\n",
        "    The function ensures that the provided configuration is a high-fidelity\n",
        "    representation of the methodology described in the source paper.\n",
        "\n",
        "    Args:\n",
        "        study_config (Dict[str, Any]): The complete study configuration\n",
        "            dictionary, containing all non-data parameters for the analysis.\n",
        "\n",
        "    Returns:\n",
        "        bool: Returns True if the entire configuration object passes all\n",
        "              validation checks successfully.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the top-level configuration object is not a dictionary\n",
        "                    or is missing essential top-level keys.\n",
        "        AssertionError: Propagates specific assertion errors from helper\n",
        "                        functions if any part of the configuration is invalid.\n",
        "    \"\"\"\n",
        "    # Top-level input validation: ensure the config is a dictionary.\n",
        "    if not isinstance(study_config, dict):\n",
        "        raise ValueError(\"The study configuration object must be a dictionary.\")\n",
        "\n",
        "    # Check for the presence of essential top-level keys.\n",
        "    required_keys = ['data_source', 'benford_laws', 'statistical_tests']\n",
        "    for key in required_keys:\n",
        "        if key not in study_config:\n",
        "            raise ValueError(f\"Missing required top-level key in configuration: '{key}'.\")\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Validate the 'data_source' parameters.\n",
        "        _validate_data_source_config(config=study_config)\n",
        "\n",
        "        # Execute Step 2: Validate the 'benford_laws' definitions.\n",
        "        _validate_benford_laws_config(config=study_config)\n",
        "\n",
        "        # Execute Step 3: Validate the 'statistical_tests' parameters.\n",
        "        _validate_statistical_tests_config(config=study_config)\n",
        "\n",
        "    except (AssertionError, KeyError) as e:\n",
        "        # Catch specific validation errors and re-raise for clarity.\n",
        "        print(f\"Configuration validation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "    # If all checks pass without raising an exception, return True.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "GN70D6xEUbps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: DataFrame Structure Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 2: Helper for MultiIndex Structure Validation\n",
        "# ==============================================================================\n",
        "def _validate_multiindex_structure(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Validates the MultiIndex structure of the input DataFrame.\n",
        "\n",
        "    This internal helper function performs a series of rigorous checks to ensure\n",
        "    the DataFrame's index is a correctly structured pandas MultiIndex, which is\n",
        "    a prerequisite for panel data analysis. It verifies the index type, number\n",
        "    of levels, level names, and the data type of each level's values.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be validated.\n",
        "        config (Dict[str, Any]): The study configuration object, used to\n",
        "                                 retrieve the expected time period.\n",
        "\n",
        "    Returns:\n",
        "        None: Returns nothing upon successful validation.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If any structural aspect of the MultiIndex is incorrect.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Confirm the index is a pandas MultiIndex.\n",
        "    assert isinstance(df.index, pd.MultiIndex), \\\n",
        "        f\"DataFrame index must be a pandas.MultiIndex, but found {type(df.index)}.\"\n",
        "\n",
        "    # Step 1.2: Confirm the MultiIndex has exactly two levels.\n",
        "    assert df.index.nlevels == 2, \\\n",
        "        f\"DataFrame MultiIndex must have 2 levels, but found {df.index.nlevels}.\"\n",
        "\n",
        "    # Step 1.3: Verify the names of the index levels are correct and in order.\n",
        "    expected_names = ['CompanyID', 'Year']\n",
        "    assert list(df.index.names) == expected_names, \\\n",
        "        f\"DataFrame MultiIndex names must be {expected_names}, but found {list(df.index.names)}.\"\n",
        "\n",
        "    # Step 1.4: Verify the data type of the 'CompanyID' level (level 0).\n",
        "    company_id_level = df.index.get_level_values('CompanyID')\n",
        "    assert is_object_dtype(company_id_level), \\\n",
        "        f\"'CompanyID' index level must have object (string) dtype, but found {company_id_level.dtype}.\"\n",
        "\n",
        "    # Step 1.5: Verify the data type of the 'Year' level (level 1).\n",
        "    year_level = df.index.get_level_values('Year')\n",
        "    assert is_integer_dtype(year_level), \\\n",
        "        f\"'Year' index level must have integer dtype, but found {year_level.dtype}.\"\n",
        "\n",
        "    # Step 1.6: Validate that the years in the index fall within the study's specified range.\n",
        "    start_year = config['data_source']['time_period_start']\n",
        "    end_year = config['data_source']['time_period_end']\n",
        "    assert year_level.min() >= start_year and year_level.max() <= end_year, \\\n",
        "        f\"Years in index must be within [{start_year}, {end_year}], but range is [{year_level.min()}, {year_level.max()}].\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 2: Helper for Required Columns Validation\n",
        "# ==============================================================================\n",
        "def _validate_required_columns(df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Validates the presence and data types of required columns in the DataFrame.\n",
        "\n",
        "    This function checks that the DataFrame contains the exact set of columns\n",
        "    required for the analysis and that each of these columns has the correct,\n",
        "    non-negotiable data type (e.g., float64 for high-precision financial data).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be validated.\n",
        "\n",
        "    Returns:\n",
        "        None: Returns nothing upon successful validation.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If columns are missing, extra columns are present, or\n",
        "                        any column has an incorrect data type.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Verify that the DataFrame contains exactly the three required columns.\n",
        "    expected_columns = {'CompanyName', 'PreTaxIncome_GBP', 'TotalAssets_GBP'}\n",
        "    actual_columns = set(df.columns)\n",
        "    assert actual_columns == expected_columns, \\\n",
        "        f\"DataFrame must contain exactly the columns {expected_columns}, but found {actual_columns}.\"\n",
        "\n",
        "    # Step 2.2: Verify the data type of the 'CompanyName' column.\n",
        "    assert is_object_dtype(df['CompanyName']), \\\n",
        "        f\"'CompanyName' column must have object (string) dtype, but found {df.dtypes['CompanyName']}.\"\n",
        "\n",
        "    # Step 2.3: Verify the data type of the 'PreTaxIncome_GBP' column is float64 for precision.\n",
        "    assert df.dtypes['PreTaxIncome_GBP'] == np.float64, \\\n",
        "        f\"'PreTaxIncome_GBP' column must have float64 dtype, but found {df.dtypes['PreTaxIncome_GBP']}.\"\n",
        "\n",
        "    # Step 2.4: Verify the data type of the 'TotalAssets_GBP' column is float64 for precision.\n",
        "    assert df.dtypes['TotalAssets_GBP'] == np.float64, \\\n",
        "        f\"'TotalAssets_GBP' column must have float64 dtype, but found {df.dtypes['TotalAssets_GBP']}.\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 2: Helper for Data Completeness Assessment\n",
        "# ==============================================================================\n",
        "def _validate_data_completeness(\n",
        "    df: pd.DataFrame,\n",
        "    config: Dict[str, Any]\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs a high-level assessment of the DataFrame's data completeness.\n",
        "\n",
        "    This function checks if the number of observations and unique entities in\n",
        "    the DataFrame are reasonably close to the figures reported in the source\n",
        "    study. This serves as a sanity check to detect major data loading errors.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to be assessed.\n",
        "        config (Dict[str, Any]): The study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        None: Returns nothing upon successful validation.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the data counts deviate significantly from the\n",
        "                        study's reported figures.\n",
        "    \"\"\"\n",
        "    # Define tolerance for count comparisons.\n",
        "    tolerance = 0.05  # 5% tolerance\n",
        "\n",
        "    # Step 3.1: Assess non-missing count for Pre-Tax Income.\n",
        "    # Source: p. 8, \"6768 data points\" for PI.\n",
        "    pi_count = df['PreTaxIncome_GBP'].count()\n",
        "    expected_pi_count = 6768\n",
        "    assert abs(pi_count - expected_pi_count) / expected_pi_count <= tolerance, \\\n",
        "        f\"Expected ~{expected_pi_count} non-null PI observations, but found {pi_count}.\"\n",
        "\n",
        "    # Step 3.2: Assess non-missing count for Total Assets.\n",
        "    # Source: p. 8, \"6811 data points\" for TA.\n",
        "    ta_count = df['TotalAssets_GBP'].count()\n",
        "    expected_ta_count = 6811\n",
        "    assert abs(ta_count - expected_ta_count) / expected_ta_count <= tolerance, \\\n",
        "        f\"Expected ~{expected_ta_count} non-null TA observations, but found {ta_count}.\"\n",
        "\n",
        "    # Step 3.3: Assess unique company count.\n",
        "    # Source: p. 7, \"567 companies\".\n",
        "    unique_companies = df.index.get_level_values('CompanyID').nunique()\n",
        "    expected_companies = config['data_source']['company_count']\n",
        "    assert abs(unique_companies - expected_companies) / expected_companies <= tolerance, \\\n",
        "        f\"Expected ~{expected_companies} unique companies, but found {unique_companies}.\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 2: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def validate_dataframe_structure(\n",
        "    df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> bool:\n",
        "    \"\"\"\n",
        "    Orchestrates the validation of the input DataFrame's structure and content.\n",
        "\n",
        "    This function serves as the main entry point for validating the primary\n",
        "    dataset. It ensures the DataFrame adheres to the strict structural and\n",
        "    content requirements of the study by invoking a series of specialized\n",
        "    helper functions for:\n",
        "    1. MultiIndex structure verification.\n",
        "    2. Required columns and dtypes validation.\n",
        "    3. Data completeness assessment against study benchmarks.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The panel data DataFrame to be validated.\n",
        "        study_config (Dict[str, Any]): The complete study configuration\n",
        "            object, providing expected values for validation checks.\n",
        "\n",
        "    Returns:\n",
        "        bool: Returns True if the DataFrame passes all validation checks.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the input `df` is not a pandas DataFrame.\n",
        "        AssertionError: Propagates specific assertion errors from helpers\n",
        "                        if any part of the DataFrame's structure is invalid.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Validate the MultiIndex structure.\n",
        "        _validate_multiindex_structure(df=df, config=study_config)\n",
        "\n",
        "        # Execute Step 2: Validate the required columns and their dtypes.\n",
        "        _validate_required_columns(df=df)\n",
        "\n",
        "        # Execute Step 3: Perform a data completeness assessment.\n",
        "        _validate_data_completeness(df=df, config=study_config)\n",
        "\n",
        "    except AssertionError as e:\n",
        "        # Catch specific validation errors and re-raise for clear reporting.\n",
        "        print(f\"DataFrame structure validation failed: {e}\")\n",
        "        raise\n",
        "\n",
        "    # If all checks pass, the DataFrame is structurally valid.\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "xYG3vvMGKljm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3: Financial Data Quality Validation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 3: Helper for Value Range Validation\n",
        "# ==============================================================================\n",
        "def _validate_value_ranges(df: pd.DataFrame) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Validates that financial data falls within plausible ranges from the study.\n",
        "\n",
        "    This function checks the 'PreTaxIncome_GBP' and 'TotalAssets_GBP' columns\n",
        "    against reasonable bounds derived from the descriptive statistics in the\n",
        "    source paper (Table 1, p. 8). It flags and counts any values outside these\n",
        "    ranges but does not modify the original data, adhering to the study's\n",
        "    preservationist methodology.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with financial data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: A dictionary containing the counts of values found\n",
        "                        outside the defined plausible ranges for each variable.\n",
        "    \"\"\"\n",
        "    # Define plausible value ranges based on study's min/max statistics.\n",
        "    # Source: Table 1, p. 8. Using slightly wider bounds for robustness.\n",
        "    ranges = {\n",
        "        'PreTaxIncome_GBP': (-2.0e10, 6.0e10),\n",
        "        'TotalAssets_GBP': (1.0e4, 3.0e12)\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to store the counts of out-of-range values.\n",
        "    out_of_range_counts = {}\n",
        "\n",
        "    # Step 1.1: Iterate through the variables and their defined ranges.\n",
        "    for col, (lower_bound, upper_bound) in ranges.items():\n",
        "        # Create a boolean mask for values that are outside the bounds.\n",
        "        # This correctly handles NaNs, which will result in False.\n",
        "        mask = (df[col] < lower_bound) | (df[col] > upper_bound)\n",
        "\n",
        "        # Count the number of True values in the mask, which corresponds to\n",
        "        # the number of out-of-range data points.\n",
        "        out_of_range_counts[col] = mask.sum()\n",
        "\n",
        "    # Return the dictionary of counts.\n",
        "    return out_of_range_counts\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 3: Helper for Missing Value Pattern Analysis\n",
        "# ==============================================================================\n",
        "def _analyze_missing_value_patterns(df: pd.DataFrame) -> Tuple[float, List[str]]:\n",
        "    \"\"\"\n",
        "    Analyzes patterns of missing values in the financial data.\n",
        "\n",
        "    This function performs two key analyses:\n",
        "    1. Calculates the correlation of missingness between Pre-Tax Income and\n",
        "       Total Assets to identify systematic data gaps.\n",
        "    2. Identifies companies with an excessive percentage (>50%) of missing\n",
        "       data points over the study period.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with financial data.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, List[str]]: A tuple containing:\n",
        "            - The correlation coefficient of missingness between PI and TA.\n",
        "            - A list of CompanyIDs with more than 50% missing data points.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Calculate the correlation of missingness.\n",
        "    # Create a boolean DataFrame where True indicates a missing value.\n",
        "    missing_df = df[['PreTaxIncome_GBP', 'TotalAssets_GBP']].isna()\n",
        "    # Calculate the correlation matrix on this boolean DataFrame.\n",
        "    missing_corr_matrix = missing_df.corr()\n",
        "    # Extract the specific correlation between the two variables.\n",
        "    missing_corr = missing_corr_matrix.loc['PreTaxIncome_GBP', 'TotalAssets_GBP']\n",
        "\n",
        "    # Step 2.2: Identify companies with excessive missing data.\n",
        "    # Group by company and calculate the mean of the boolean missingness indicator.\n",
        "    # This gives the proportion of missing entries for each company.\n",
        "    missing_rates = df.groupby(level='CompanyID')[['PreTaxIncome_GBP', 'TotalAssets_GBP']].apply(\n",
        "        lambda x: x.isna().all(axis=1).mean()\n",
        "    )\n",
        "    # Filter for companies where this rate exceeds the 50% threshold.\n",
        "    high_missing_companies = missing_rates[missing_rates > 0.5].index.tolist()\n",
        "\n",
        "    # Return the calculated metrics.\n",
        "    return missing_corr, high_missing_companies\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 3: Helper for Extreme Value Detection\n",
        "# ==============================================================================\n",
        "def _detect_extreme_values(df: pd.DataFrame) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Detects extreme values using a robust Z-score methodology.\n",
        "\n",
        "    This function identifies outliers based on the median and Median Absolute\n",
        "    Deviation (MAD), which are robust to the influence of outliers themselves.\n",
        "    It flags values with a robust Z-score > 5, aligning with the paper's\n",
        "    mention of a few \"anomalously extreme outliers\" (p. 8).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame with financial data.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, int]: A dictionary containing the counts of extreme values\n",
        "                        detected for each financial variable.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the counts of extreme values.\n",
        "    extreme_value_counts = {}\n",
        "\n",
        "    # Define the financial columns to check.\n",
        "    financial_cols = ['PreTaxIncome_GBP', 'TotalAssets_GBP']\n",
        "\n",
        "    # Step 3.1: Iterate through the financial columns to detect outliers.\n",
        "    for col in financial_cols:\n",
        "        # Extract the series and drop NaNs for statistical calculation.\n",
        "        series = df[col].dropna()\n",
        "\n",
        "        # Calculate the median of the series.\n",
        "        series_median = series.median()\n",
        "\n",
        "        # Calculate the Median Absolute Deviation (MAD).\n",
        "        # `nan_policy='omit'` is the default and correct behavior.\n",
        "        # The scale='normal' argument applies the 1/Φ⁻¹(3/4) ≈ 1.4826 factor.\n",
        "        series_mad = median_abs_deviation(series, scale='normal')\n",
        "\n",
        "        # Avoid division by zero if MAD is 0 (all values are the same).\n",
        "        if series_mad == 0:\n",
        "            extreme_value_counts[col] = 0\n",
        "            continue\n",
        "\n",
        "        # Step 3.2: Calculate the robust Z-score for the original series.\n",
        "        # Robust Z-score = (value - median) / MAD\n",
        "        robust_z_scores = (df[col] - series_median) / series_mad\n",
        "\n",
        "        # Step 3.3: Count values where the absolute robust Z-score exceeds 5.\n",
        "        extreme_mask = robust_z_scores.abs() > 5.0\n",
        "        extreme_value_counts[col] = extreme_mask.sum()\n",
        "\n",
        "    # Return the dictionary of extreme value counts.\n",
        "    return extreme_value_counts\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase I, Task 3: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def validate_financial_data_quality(\n",
        "    df: pd.DataFrame\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive validation of financial data quality.\n",
        "\n",
        "    This function serves as the main entry point for assessing the quality of\n",
        "    the numerical data within the DataFrame. It does not alter the data, but\n",
        "    instead generates a detailed report by invoking helper functions that:\n",
        "    1. Check for values outside plausible ranges.\n",
        "    2. Analyze patterns of missing data.\n",
        "    3. Detect statistically extreme outliers using a robust methodology.\n",
        "\n",
        "    The output is a structured dictionary that serves as a data quality audit log.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The panel data DataFrame with financial columns to\n",
        "                           be validated.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the results of all\n",
        "                        data quality checks.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(df, pd.DataFrame):\n",
        "        raise ValueError(\"Input 'df' must be a pandas DataFrame.\")\n",
        "\n",
        "    # Initialize the results dictionary.\n",
        "    quality_report = {}\n",
        "\n",
        "    try:\n",
        "        # Execute Step 1: Validate value ranges.\n",
        "        quality_report['range_validation'] = _validate_value_ranges(df=df)\n",
        "\n",
        "        # Execute Step 2: Analyze missing value patterns.\n",
        "        missing_corr, high_missing_cos = _analyze_missing_value_patterns(df=df)\n",
        "        quality_report['missing_value_analysis'] = {\n",
        "            'missingness_correlation': missing_corr,\n",
        "            'companies_with_high_missing_data': high_missing_cos\n",
        "        }\n",
        "\n",
        "        # Execute Step 3: Detect extreme values.\n",
        "        quality_report['extreme_value_detection'] = _detect_extreme_values(df=df)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during the quality check process.\n",
        "        print(f\"An error occurred during data quality validation: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Return the comprehensive quality report.\n",
        "    return quality_report\n"
      ],
      "metadata": {
        "id": "RAL8EihtLeQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 4: Data Cleansing and Standardization\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 4: Helper for Missing Value Preservation\n",
        "# ==============================================================================\n",
        "def _preserve_missing_values(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Ensures missing values are preserved and explicitly documented.\n",
        "\n",
        "    This function creates a safe copy of the input DataFrame to prevent\n",
        "    unintended modifications. It then generates a boolean mask DataFrame that\n",
        "    explicitly documents the location of every np.nan value. This adheres to\n",
        "\n",
        "    the study's principle of not imputing or altering missing data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "            - A safe copy of the original DataFrame.\n",
        "            - A boolean DataFrame of the same shape indicating the locations\n",
        "              of missing values.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Create a deep copy to ensure the original DataFrame is never modified.\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Step 1.2: Document the locations of all missing values.\n",
        "    # The .isna() method returns a boolean DataFrame of the same dimensions.\n",
        "    missing_value_mask = df_copy.isna()\n",
        "\n",
        "    # Step 1.3: Perform a verification check to ensure no NaNs were lost or added.\n",
        "    assert df.isna().sum().equals(missing_value_mask.sum()), \\\n",
        "        \"Mismatch in NaN counts after creating preservation mask. This indicates a copy error.\"\n",
        "\n",
        "    # Return the safe copy and the documentation mask.\n",
        "    return df_copy, missing_value_mask\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 4: Helper for Precision Standardization\n",
        "# ==============================================================================\n",
        "def _standardize_precision(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Verifies and enforces high-precision float64 dtype for financial columns.\n",
        "\n",
        "    This function acts as a quality gate, ensuring that the key financial\n",
        "    variables ('PreTaxIncome_GBP', 'TotalAssets_GBP') are represented using\n",
        "    the float64 data type. This is non-negotiable for maintaining the\n",
        "    numerical precision required for subsequent statistical and logarithmic\n",
        "    calculations.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to validate.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The validated DataFrame, passed through without\n",
        "                      modification if checks are successful.\n",
        "\n",
        "    Raises:\n",
        "        TypeError: If any financial column does not have the float64 dtype.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Define the columns that require high-precision floating-point representation.\n",
        "    financial_cols = ['PreTaxIncome_GBP', 'TotalAssets_GBP']\n",
        "\n",
        "    # Step 2.2: Iterate through the columns and validate their dtype.\n",
        "    for col in financial_cols:\n",
        "        # This check is strict. It will not accept float32 or other numeric types.\n",
        "        assert df[col].dtype == np.float64, \\\n",
        "            f\"Column '{col}' must have dtype float64 for precision, but found {df[col].dtype}.\"\n",
        "\n",
        "    # Step 2.3: As per the study, data is already in GBP. No currency conversion is needed.\n",
        "    # In a production system, a currency check/conversion step would be here.\n",
        "\n",
        "    # Return the DataFrame if all checks pass.\n",
        "    return df\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 4: Helper for Flagging Extreme Values\n",
        "# ==============================================================================\n",
        "def _flag_extreme_values(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Adds boolean flags to the DataFrame to identify extreme values.\n",
        "\n",
        "    This function operationalizes the extreme value detection from Task 3. It\n",
        "    calculates the robust Z-score for each financial data point and adds new\n",
        "    boolean columns to the DataFrame (e.g., 'IsExtreme_PI') that are True\n",
        "    for any observation with an absolute robust Z-score greater than 5.0.\n",
        "    This preserves the original data while creating a persistent audit trail.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame to which flags will be added.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame augmented with new boolean flag columns.\n",
        "    \"\"\"\n",
        "    # Step 3.1: Define the financial columns and their corresponding flag names.\n",
        "    cols_to_flag = {\n",
        "        'PreTaxIncome_GBP': 'IsExtreme_PI',\n",
        "        'TotalAssets_GBP': 'IsExtreme_TA'\n",
        "    }\n",
        "\n",
        "    # Step 3.2: Iterate through the columns to calculate Z-scores and add flags.\n",
        "    for col, flag_col in cols_to_flag.items():\n",
        "        # Extract the series and drop NaNs for robust statistical calculation.\n",
        "        series = df[col].dropna()\n",
        "\n",
        "        # Calculate the median and Median Absolute Deviation (MAD).\n",
        "        series_median = series.median()\n",
        "        # The 'scale' argument applies the consistency constant (1.4826).\n",
        "        series_mad = median_abs_deviation(series, scale='normal')\n",
        "\n",
        "        # If MAD is zero, no value can be considered an outlier.\n",
        "        if series_mad > 0:\n",
        "            # Calculate the robust Z-score for every data point in the column.\n",
        "            # Robust Z-score = (value - median) / MAD\n",
        "            robust_z_scores = (df[col] - series_median) / series_mad\n",
        "            # Create the boolean flag where the absolute Z-score exceeds 5.\n",
        "            df[flag_col] = robust_z_scores.abs() > 5.0\n",
        "        else:\n",
        "            # If MAD is 0, all non-median values would have infinite Z-score.\n",
        "            # The correct behavior is to flag nothing as extreme.\n",
        "            df[flag_col] = False\n",
        "\n",
        "    # Return the DataFrame with the new flag columns.\n",
        "    return df\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 4: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def prepare_and_standardize_data(\n",
        "    raw_df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the data preparation and standardization pipeline.\n",
        "\n",
        "    This function executes the study's conservative data preparation methodology.\n",
        "    It does not clean, impute, or alter the original financial data. Instead,\n",
        "    it prepares the dataset for analysis by:\n",
        "    1. Creating a safe copy and documenting missing values (preservation).\n",
        "    2. Verifying high-precision dtypes for numerical accuracy (standardization).\n",
        "    3. Flagging extreme statistical outliers as metadata (auditing).\n",
        "\n",
        "    The output is a DataFrame that is structurally identical to the input but\n",
        "    is verified, standardized, and augmented with auditable quality flags.\n",
        "\n",
        "    Args:\n",
        "        raw_df (pd.DataFrame): The raw input DataFrame after initial loading.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The prepared and standardized DataFrame, ready for\n",
        "                      variable segmentation and analysis.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(raw_df, pd.DataFrame):\n",
        "        raise ValueError(\"Input 'raw_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Preserve missing values by creating a safe copy and a mask.\n",
        "        # We will proceed with the copied DataFrame. The mask is for documentation.\n",
        "        df, _ = _preserve_missing_values(df=raw_df)\n",
        "\n",
        "        # Step 2: Standardize (verify) numerical precision.\n",
        "        df = _standardize_precision(df=df)\n",
        "\n",
        "        # Step 3: Add extreme value flags as metadata to the DataFrame.\n",
        "        df = _flag_extreme_values(df=df)\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any errors during the preparation process.\n",
        "        print(f\"An error occurred during data preparation and standardization: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Return the fully prepared and flagged DataFrame.\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "Rd-pDZ1kMKBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 5: Variable Segmentation and Subset Creation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 5: Helper for Pre-Tax Income Segmentation\n",
        "# ==============================================================================\n",
        "def _segment_pre_tax_income(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Segments the DataFrame into positive and negative Pre-Tax Income subsets.\n",
        "\n",
        "    This function applies boolean masking to partition the input DataFrame into\n",
        "    two distinct subsets based on the sign of the 'PreTaxIncome_GBP' column.\n",
        "    It strictly uses '>' and '<' operators to correctly exclude zero-income\n",
        "    observations from both subsets, as required for Benford's Law analysis.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame, which should already include\n",
        "                           the PI/TA ratio if needed in the subsets.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n",
        "            - A DataFrame with only positive Pre-Tax Income observations (PI+).\n",
        "            - A DataFrame with only negative Pre-Tax Income observations (PI-).\n",
        "    \"\"\"\n",
        "    # Step 1.1: Create the PI(+) subset by filtering for PreTaxIncome_GBP > 0.\n",
        "    # This vectorized boolean indexing is highly efficient.\n",
        "    # NaNs in the column will evaluate to False and be correctly excluded.\n",
        "    pi_positive_df = df[df['PreTaxIncome_GBP'] > 0].copy()\n",
        "\n",
        "    # Step 1.2: Create the PI(-) subset by filtering for PreTaxIncome_GBP < 0.\n",
        "    pi_negative_df = df[df['PreTaxIncome_GBP'] < 0].copy()\n",
        "\n",
        "    # Step 1.3: Verification of subset sizes against study expectations.\n",
        "    # Source: p. 8, Table 1, N values. PI(+) ~5608, PI(-) ~1160.\n",
        "    # Using a tolerance to account for minor data differences.\n",
        "    assert 5500 < len(pi_positive_df) < 5700, \\\n",
        "        f\"PI(+) subset has {len(pi_positive_df)} rows, expected ~5608.\"\n",
        "    assert 1100 < len(pi_negative_df) < 1200, \\\n",
        "        f\"PI(-) subset has {len(pi_negative_df)} rows, expected ~1160.\"\n",
        "\n",
        "    # Return the two generated subsets.\n",
        "    return pi_positive_df, pi_negative_df\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 5: Helper for Ratio Variable Construction\n",
        "# ==============================================================================\n",
        "def _construct_ratio_variable(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs the Pre-Tax Income to Total Assets (PI/TA) ratio variable.\n",
        "\n",
        "    This function calculates the PI/TA ratio in a robust manner. It uses\n",
        "    vectorized operations for efficiency and includes explicit handling of\n",
        "    division-by-zero edge cases by replacing resulting infinite values with NaN.\n",
        "    NaN propagation is handled automatically by pandas.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The DataFrame containing 'PreTaxIncome_GBP' and\n",
        "                           'TotalAssets_GBP' columns.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The DataFrame with the new 'PI_TA_Ratio' column added.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Calculate the PI/TA ratio using vectorized division.\n",
        "    # This operation correctly propagates NaNs.\n",
        "    df['PI_TA_Ratio'] = df['PreTaxIncome_GBP'] / df['TotalAssets_GBP']\n",
        "\n",
        "    # Step 2.2: Handle the edge case of division by zero.\n",
        "    # Pandas/Numpy division by zero results in np.inf or -np.inf.\n",
        "    # These must be replaced with np.nan to be excluded from digit analysis.\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "    # Step 2.3: Verification of the number of valid ratios.\n",
        "    # Source: p. 8, Table 1, N value for PI/TA is 6765.\n",
        "    valid_ratios = df['PI_TA_Ratio'].count()\n",
        "    assert 6700 < valid_ratios < 6800, \\\n",
        "        f\"Expected ~6765 valid PI/TA ratios, but calculated {valid_ratios}.\"\n",
        "\n",
        "    # Return the DataFrame with the new column.\n",
        "    return df\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 5: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def create_analytical_variables(\n",
        "    prepared_df: pd.DataFrame\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Orchestrates the creation of all datasets required for the analysis.\n",
        "\n",
        "    This function takes the prepared and standardized DataFrame and generates\n",
        "    all necessary analytical variables and subsets. The process is:\n",
        "    1. Construct the PI/TA ratio variable on the main DataFrame.\n",
        "    2. Segment the augmented DataFrame into PI(+) and PI(-) subsets.\n",
        "    3. Organize all resulting DataFrames (full, PI+, PI-) into a\n",
        "       dictionary for clear, structured access in subsequent pipeline stages.\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The fully prepared, standardized, and\n",
        "                                    flagged DataFrame from Task 4.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary where keys are descriptive names\n",
        "            ('full_data', 'pi_positive', 'pi_negative') and values are the\n",
        "            corresponding pandas DataFrames, ready for analysis.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(prepared_df, pd.DataFrame):\n",
        "        raise ValueError(\"Input 'prepared_df' must be a pandas DataFrame.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Construct the PI/TA ratio variable.\n",
        "        # This modifies the DataFrame by adding a new column.\n",
        "        df_with_ratio = _construct_ratio_variable(df=prepared_df)\n",
        "\n",
        "        # Step 2: Segment the augmented DataFrame into PI(+) and PI(-) subsets.\n",
        "        # These subsets will now correctly contain the 'PI_TA_Ratio' column.\n",
        "        pi_positive_df, pi_negative_df = _segment_pre_tax_income(df=df_with_ratio)\n",
        "\n",
        "        # Step 3: Organize the final DataFrames into a structured dictionary.\n",
        "        # This provides a clean, modular output for the rest of the pipeline.\n",
        "        analytical_datasets = {\n",
        "            'full_data': df_with_ratio,\n",
        "            'pi_positive': pi_positive_df,\n",
        "            'pi_negative': pi_negative_df\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any errors during the variable creation process.\n",
        "        print(f\"An error occurred during analytical variable creation: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Return the dictionary of analytical datasets.\n",
        "    return analytical_datasets\n"
      ],
      "metadata": {
        "id": "0HqbYso3M0DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 6: Data Quality Metrics and Summary Statistics\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 6: Helper for Descriptive Statistics Calculation\n",
        "# ==============================================================================\n",
        "def _calculate_descriptive_statistics(series: pd.Series) -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Calculates a comprehensive set of descriptive statistics for a data series.\n",
        "\n",
        "    This function computes all statistical metrics required to replicate Table 1\n",
        "    of the source paper. It uses pandas for basic, optimized calculations and\n",
        "    scipy.stats for higher-order moments to ensure control over bias correction,\n",
        "    aiming for the highest possible fidelity to standard statistical software.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The data series for which to calculate statistics.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A dictionary containing the calculated statistical\n",
        "                          metrics, including N, min, max, mean, std, skewness,\n",
        "                          kurtosis, and the coefficient of variation (CV).\n",
        "    \"\"\"\n",
        "    # Drop NA values for accurate statistical computation.\n",
        "    series_clean = series.dropna()\n",
        "\n",
        "    # Handle the case of an empty series after dropping NaNs.\n",
        "    if series_clean.empty:\n",
        "        return {\n",
        "            'N': 0, 'min': np.nan, 'max': np.nan, 'Mean': np.nan,\n",
        "            'St. Dev.': np.nan, 'Skew.': np.nan, 'Kurt.': np.nan, 'CV': np.nan\n",
        "        }\n",
        "\n",
        "    # Step 1.1: Calculate basic statistics using pandas' optimized methods.\n",
        "    stats = {\n",
        "        'N': series_clean.count(),\n",
        "        'min': series_clean.min(),\n",
        "        'max': series_clean.max(),\n",
        "        'Mean': series_clean.mean(),\n",
        "        'St. Dev.': series_clean.std()\n",
        "    }\n",
        "\n",
        "    # Step 1.2: Calculate higher-order moments using scipy for precision.\n",
        "    # `bias=False` provides the sample skewness and kurtosis.\n",
        "    stats['Skew.'] = skew(series_clean, bias=False)\n",
        "    # Fisher's definition of kurtosis (normal = 0) is used by default.\n",
        "    stats['Kurt.'] = kurtosis(series_clean, fisher=True, bias=False)\n",
        "\n",
        "    # Step 1.3: Calculate the Coefficient of Variation (CV).\n",
        "    # CV = Standard Deviation / |Mean|\n",
        "    # Using abs(mean) makes the CV a meaningful measure of relative variability\n",
        "    # even for data with a negative mean (like PI-).\n",
        "    mean_val = stats['Mean']\n",
        "    std_val = stats['St. Dev.']\n",
        "    if mean_val != 0:\n",
        "        stats['CV'] = std_val / abs(mean_val)\n",
        "    else:\n",
        "        stats['CV'] = np.inf # Or np.nan, depending on convention for zero mean.\n",
        "\n",
        "    return stats\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 6: Helper for Formatting the Summary Table\n",
        "# ==============================================================================\n",
        "def _format_summary_table(stats_dict: Dict[str, Dict[str, float]]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Formats the calculated statistics into a presentation-ready DataFrame.\n",
        "\n",
        "    This function takes the raw dictionary of statistics and constructs a\n",
        "    pandas DataFrame that meticulously replicates the structure, ordering, and\n",
        "    numerical formatting of Table 1 from the source paper.\n",
        "\n",
        "    Args:\n",
        "        stats_dict (Dict[str, Dict[str, float]]): A nested dictionary where\n",
        "            outer keys are variable names and inner keys are statistic names.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A formatted DataFrame ready for display or publication.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Convert the dictionary to a DataFrame.\n",
        "    summary_df = pd.DataFrame.from_dict(stats_dict, orient='index')\n",
        "\n",
        "    # Step 2.2: Define the exact column order as in Table 1.\n",
        "    column_order = ['N', 'min', 'max', 'Mean', 'St. Dev.', 'Skew.', 'Kurt.', 'CV']\n",
        "    summary_df = summary_df[column_order]\n",
        "\n",
        "    # Step 2.3: Apply specific rounding rules to match the paper's presentation.\n",
        "    # This is a critical step for high-fidelity replication.\n",
        "    rounding_rules = {\n",
        "        'N': 0,\n",
        "        'min': 5,\n",
        "        'max': 5,\n",
        "        'Mean': 5,\n",
        "        'St. Dev.': 5,\n",
        "        'Skew.': 5,\n",
        "        'Kurt.': 5,\n",
        "        'CV': 5\n",
        "    }\n",
        "\n",
        "    # Apply special formatting for large integer values as seen in the paper.\n",
        "    for col in ['Mean', 'St. Dev.']:\n",
        "        # For PI and TA, round to integer.\n",
        "        summary_df.loc[['PI', 'TA'], col] = summary_df.loc[['PI', 'TA'], col].round(0).astype(int)\n",
        "\n",
        "    # Apply general rounding rules.\n",
        "    summary_df = summary_df.round(rounding_rules)\n",
        "\n",
        "    return summary_df\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase II, Task 6: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def generate_summary_statistics(\n",
        "    analytical_datasets: Dict[str, pd.DataFrame]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Orchestrates the calculation and formatting of summary statistics.\n",
        "\n",
        "    This function serves as the main entry point for replicating Table 1 from\n",
        "    the study. It processes the dictionary of analytical datasets, calculates\n",
        "    descriptive statistics for each required variable and subset, and then\n",
        "    formats these results into a single, publication-quality DataFrame that\n",
        "    mirrors the source table.\n",
        "\n",
        "    Args:\n",
        "        analytical_datasets (Dict[str, pd.DataFrame]): A dictionary containing\n",
        "            the 'full_data', 'pi_positive', and 'pi_negative' DataFrames.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A formatted DataFrame containing the descriptive\n",
        "                      statistics, replicating Table 1 of the paper.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(analytical_datasets, dict):\n",
        "        raise ValueError(\"Input 'analytical_datasets' must be a dictionary.\")\n",
        "\n",
        "    # Step 3.1: Define the mapping from the desired output rows to the source data.\n",
        "    # This map is the blueprint for the entire calculation process.\n",
        "    variable_map = {\n",
        "        'PI': ('full_data', 'PreTaxIncome_GBP'),\n",
        "        'TA': ('full_data', 'TotalAssets_GBP'),\n",
        "        'PI(-)': ('pi_negative', 'PreTaxIncome_GBP'),\n",
        "        'PI(+)': ('pi_positive', 'PreTaxIncome_GBP'),\n",
        "        'PI/TA': ('full_data', 'PI_TA_Ratio'),\n",
        "        'PI(-)/TA': ('pi_negative', 'PI_TA_Ratio'),\n",
        "        'PI(+)/TA': ('pi_positive', 'PI_TA_Ratio')\n",
        "    }\n",
        "\n",
        "    # Initialize a dictionary to hold the raw statistical results.\n",
        "    all_stats = {}\n",
        "\n",
        "    try:\n",
        "        # Step 3.2: Iterate through the map to calculate statistics for each variable.\n",
        "        for var_name, (df_key, col_name) in variable_map.items():\n",
        "            # Select the correct DataFrame and Series.\n",
        "            target_df = analytical_datasets[df_key]\n",
        "            target_series = target_df[col_name]\n",
        "\n",
        "            # Calculate the descriptive statistics for the selected series.\n",
        "            all_stats[var_name] = _calculate_descriptive_statistics(series=target_series)\n",
        "\n",
        "        # Step 3.3: Format the aggregated statistics into the final table.\n",
        "        final_table = _format_summary_table(stats_dict=all_stats)\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"Data generation failed: Missing expected key {e} in datasets or columns.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during statistics generation: {e}\")\n",
        "        raise\n",
        "\n",
        "    return final_table\n"
      ],
      "metadata": {
        "id": "Ei3VACKyNi2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 7: First Significant Digit Extraction?\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase III, Task 7: Helper for Preprocessing Data for Extraction\n",
        "# ==============================================================================\n",
        "def _preprocess_for_digit_extraction(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Prepares a data series for significant digit extraction.\n",
        "\n",
        "    This function takes a raw data series and performs the necessary\n",
        "    preprocessing steps:\n",
        "    1. Takes the absolute value of all numbers.\n",
        "    2. Filters out all non-positive (<= 0) and non-finite (NaN, inf) values,\n",
        "       for which significant digits are undefined.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): The raw numerical data series.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A clean series containing only positive, finite numbers,\n",
        "                   with its original index preserved for later alignment.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Take the absolute value to work with magnitudes.\n",
        "    abs_series = series.abs()\n",
        "\n",
        "    # Step 1.2: Create a boolean mask to identify valid entries.\n",
        "    # A valid entry must be both finite (not NaN or inf) and strictly positive.\n",
        "    valid_mask = np.isfinite(abs_series) & (abs_series > 0)\n",
        "\n",
        "    # Step 1.3: Return the filtered series containing only valid numbers.\n",
        "    return abs_series[valid_mask]\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase III, Task 7: Helper for Logarithmic Digit Extraction\n",
        "# ==============================================================================\n",
        "def _extract_first_digit_logarithmic(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Extracts the first significant digit using a robust logarithmic method.\n",
        "\n",
        "    This function implements the mathematically sound formula for first-digit\n",
        "    extraction: d = floor(10^(log10(x) mod 1)). This approach is vectorized,\n",
        "    computationally efficient, and robust across numbers of any scale.\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): A preprocessed series containing only positive,\n",
        "                            finite numbers.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A series of the same index containing the extracted first\n",
        "                   significant digits as integers.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Calculate log base 10 of the series.\n",
        "    # This is the core of the scale-invariance property.\n",
        "    log10_series = np.log10(series)\n",
        "\n",
        "    # Step 2.2: Isolate the fractional part of the logarithm using the modulo operator.\n",
        "    # The fractional part (mantissa) contains the information about the significant digits.\n",
        "    mantissa = log10_series % 1\n",
        "\n",
        "    # Step 2.3: Raise 10 to the power of the mantissa.\n",
        "    # This yields a number between 1 (inclusive) and 10 (exclusive).\n",
        "    # The integer part of this result is the first significant digit.\n",
        "    power_of_10 = 10 ** mantissa\n",
        "\n",
        "    # Step 2.4: Take the floor of the result and cast to an integer.\n",
        "    # This isolates the first significant digit.\n",
        "    first_digits = np.floor(power_of_10).astype(int)\n",
        "\n",
        "    return first_digits\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase III, Task 7: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def extract_first_significant_digit(\n",
        "    data_series: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction of the first significant digit from a data series.\n",
        "\n",
        "    This function provides a complete, robust, and efficient pipeline for\n",
        "    first significant digit extraction. It follows a three-step process:\n",
        "    1. Preprocesses the data to isolate valid numbers for analysis.\n",
        "    2. Applies a mathematically robust logarithmic algorithm to extract the digits.\n",
        "    3. Re-aligns the extracted digits with the original series' index, ensuring\n",
        "       that invalid inputs (e.g., zero, NaN) are represented as NaN in the\n",
        "       output.\n",
        "\n",
        "    Args:\n",
        "        data_series (pd.Series): The input pandas Series of numerical data.\n",
        "                                 Must be of a float or integer dtype.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series of the same index as the input, containing\n",
        "                   the first significant digit (as float, to accommodate NaN)\n",
        "                   for each valid entry, and np.nan for invalid entries.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(data_series, pd.Series):\n",
        "        raise TypeError(\"Input 'data_series' must be a pandas Series.\")\n",
        "    if not pd.api.types.is_numeric_dtype(data_series.dtype):\n",
        "        raise TypeError(\"Input 'data_series' must have a numeric dtype.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Preprocess the series to get only positive, finite values.\n",
        "        # The index of `valid_series` is a subset of `data_series.index`.\n",
        "        valid_series = _preprocess_for_digit_extraction(series=data_series)\n",
        "\n",
        "        # If there are no valid numbers, return a series of NaNs.\n",
        "        if valid_series.empty:\n",
        "            return pd.Series(np.nan, index=data_series.index, dtype=np.float64)\n",
        "\n",
        "        # Step 2: Extract the first digits from the valid, preprocessed data.\n",
        "        # The index of `extracted_digits` matches the index of `valid_series`.\n",
        "        extracted_digits = _extract_first_digit_logarithmic(series=valid_series)\n",
        "\n",
        "        # Step 3: Align the results back to the original series' index.\n",
        "        # .reindex() ensures the output has the same shape and index as the input.\n",
        "        # Entries in the original series that were not valid (e.g., 0, NaN)\n",
        "        # will have a value of NaN in the final result.\n",
        "        # The dtype is set to float64 to accommodate NaN values.\n",
        "        result_series = extracted_digits.reindex(data_series.index).astype(np.float64)\n",
        "\n",
        "        # Final validation: ensure all non-NaN digits are in the range [1, 9].\n",
        "        assert result_series.dropna().isin(range(1, 10)).all(), \\\n",
        "            \"Post-validation failed: Extracted digits found outside the valid range [1, 9].\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during the extraction process.\n",
        "        print(f\"An error occurred during first digit extraction: {e}\")\n",
        "        raise\n",
        "\n",
        "    return result_series\n"
      ],
      "metadata": {
        "id": "kEO5W51ZOYax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 8: Second Significant Digit Extraction\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase III, Task 8: Helper for Hybrid Second Digit Extraction\n",
        "# ==============================================================================\n",
        "def _extract_second_digit_hybrid(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Extracts the second significant digit using a robust mathematical method.\n",
        "\n",
        "    This function implements a mathematically sound, vectorized algorithm to\n",
        "    isolate the second significant digit. The core logic normalizes each number\n",
        "    to isolate its first two significant digits as an integer, then uses the\n",
        "    modulo operator to extract the second digit.\n",
        "\n",
        "    The formula for the first two digits (d1d2) of a number x is:\n",
        "    d1d2 = floor(x / 10^(floor(log10(x)) - 1))\n",
        "    The second digit is then simply: d2 = d1d2 % 10\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): A preprocessed series containing only positive,\n",
        "                            finite numbers.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A series of the same index containing the extracted second\n",
        "                   significant digits (0-9) as integers.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Calculate the exponent needed to normalize the number.\n",
        "    # `np.floor(np.log10(series))` gives the order of magnitude.\n",
        "    # Subtracting 1 scales the number so the first two digits are left of the decimal.\n",
        "    exponent = np.floor(np.log10(series)) - 1\n",
        "\n",
        "    # Step 1.2: Calculate the divisor for normalization.\n",
        "    divisor = 10 ** exponent\n",
        "\n",
        "    # Step 1.3: Normalize the series and take the floor to get the first two digits as an integer.\n",
        "    # For x=12345, exponent=3, divisor=1000, normalized=12.345, first_two_digits=12.\n",
        "    # For x=0.0789, exponent=-3, divisor=0.001, normalized=78.9, first_two_digits=78.\n",
        "    first_two_digits = np.floor(series / divisor)\n",
        "\n",
        "    # Step 1.4: Extract the second digit using the modulo operator.\n",
        "    # For 12, 12 % 10 = 2.\n",
        "    # For 78, 78 % 10 = 8.\n",
        "    # For single-digit numbers like 5 (which becomes 50), 50 % 10 = 0.\n",
        "    second_digits = (first_two_digits % 10).astype(int)\n",
        "\n",
        "    return second_digits\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase III, Task 8: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def extract_second_significant_digit(\n",
        "    data_series: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction of the second significant digit from a data series.\n",
        "\n",
        "    This function provides a complete, robust, and efficient pipeline for\n",
        "    second significant digit extraction. It follows a three-step process:\n",
        "    1. Reuses the robust preprocessing logic to isolate valid numbers.\n",
        "    2. Applies a mathematically sound hybrid algorithm to extract the digits.\n",
        "    3. Re-aligns the extracted digits with the original series' index, ensuring\n",
        "       that invalid inputs (e.g., zero, NaN) are represented as NaN in the\n",
        "       output.\n",
        "\n",
        "    Args:\n",
        "        data_series (pd.Series): The input pandas Series of numerical data.\n",
        "                                 Must be of a float or integer dtype.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series of the same index as the input, containing\n",
        "                   the second significant digit (as float, to accommodate NaN)\n",
        "                   for each valid entry, and np.nan for invalid entries.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(data_series, pd.Series):\n",
        "        raise TypeError(\"Input 'data_series' must be a pandas Series.\")\n",
        "    if not pd.api.types.is_numeric_dtype(data_series.dtype):\n",
        "        raise TypeError(\"Input 'data_series' must have a numeric dtype.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Preprocess the series to get only positive, finite values.\n",
        "        # This reuses the exact same robust helper from the first-digit extraction.\n",
        "        valid_series = _preprocess_for_digit_extraction(series=data_series)\n",
        "\n",
        "        # If there are no valid numbers, return a series of NaNs.\n",
        "        if valid_series.empty:\n",
        "            return pd.Series(np.nan, index=data_series.index, dtype=np.float64)\n",
        "\n",
        "        # Step 2: Extract the second digits from the valid, preprocessed data.\n",
        "        extracted_digits = _extract_second_digit_hybrid(series=valid_series)\n",
        "\n",
        "        # Step 3: Align the results back to the original series' index.\n",
        "        # This ensures the output is conformable with the original DataFrame.\n",
        "        # Entries that were invalid will correctly be assigned NaN.\n",
        "        result_series = extracted_digits.reindex(data_series.index).astype(np.float64)\n",
        "\n",
        "        # Final validation: ensure all non-NaN digits are in the range [0, 9].\n",
        "        assert result_series.dropna().isin(range(10)).all(), \\\n",
        "            \"Post-validation failed: Extracted digits found outside the valid range [0, 9].\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during the extraction process.\n",
        "        print(f\"An error occurred during second digit extraction: {e}\")\n",
        "        raise\n",
        "\n",
        "    return result_series\n"
      ],
      "metadata": {
        "id": "dMLvqk1bPGas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 9: First-Two Digit Combination Construction\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase III, Task 9: Helper for Direct First-Two Digit Extraction\n",
        "# ==============================================================================\n",
        "def _extract_first_two_digits_direct(series: pd.Series) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Extracts the first-two significant digits directly using a mathematical method.\n",
        "\n",
        "    This function implements the robust, vectorized formula to isolate the\n",
        "    integer formed by the first two significant digits of each number.\n",
        "\n",
        "    Formula: d1d2 = floor(x / 10^(floor(log10(x)) - 1))\n",
        "\n",
        "    After calculation, it filters the results to exclude any numbers that have\n",
        "    fewer than two significant digits (i.e., results < 10).\n",
        "\n",
        "    Args:\n",
        "        series (pd.Series): A preprocessed series containing only positive,\n",
        "                            finite numbers.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A series containing the extracted first-two significant\n",
        "                   digits (10-99) as integers. The index is preserved from\n",
        "                   the input series, but rows corresponding to single-digit\n",
        "                   numbers are dropped.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Calculate the exponent for normalization, identical to Task 8.\n",
        "    exponent = np.floor(np.log10(series)) - 1\n",
        "\n",
        "    # Step 1.2: Calculate the divisor.\n",
        "    divisor = 10 ** exponent\n",
        "\n",
        "    # Step 1.3: Normalize and take the floor to get the first two digits.\n",
        "    first_two_digits = np.floor(series / divisor).astype(int)\n",
        "\n",
        "    # Step 1.4: Filter out results that are less than 10.\n",
        "    # These correspond to original numbers with only one significant digit.\n",
        "    # For example, for x=500, first_two_digits=50. For x=5, first_two_digits=5.\n",
        "    # We must exclude the latter for a valid BL12 analysis.\n",
        "    valid_first_two_digits = first_two_digits[first_two_digits >= 10]\n",
        "\n",
        "    return valid_first_two_digits\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase III, Task 9: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def extract_first_two_significant_digits(\n",
        "    data_series: pd.Series\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Orchestrates the extraction of the first-two significant digits.\n",
        "\n",
        "    This function provides a complete pipeline for first-two significant digit\n",
        "    extraction (for BL12 analysis). It follows a robust process:\n",
        "    1. Preprocesses the data to isolate valid numbers for analysis.\n",
        "    2. Applies a direct mathematical algorithm to extract the two-digit integer.\n",
        "    3. Filters out results from numbers with fewer than two significant digits.\n",
        "    4. Re-aligns the final digits with the original series' index, ensuring\n",
        "       invalid inputs are represented as NaN.\n",
        "\n",
        "    Args:\n",
        "        data_series (pd.Series): The input pandas Series of numerical data.\n",
        "                                 Must be of a float or integer dtype.\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series of the same index as the input, containing\n",
        "                   the first-two significant digits (as float, to accommodate\n",
        "                   NaN) for each valid entry, and np.nan otherwise.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(data_series, pd.Series):\n",
        "        raise TypeError(\"Input 'data_series' must be a pandas Series.\")\n",
        "    if not pd.api.types.is_numeric_dtype(data_series.dtype):\n",
        "        raise TypeError(\"Input 'data_series' must have a numeric dtype.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Preprocess the series to get only positive, finite values.\n",
        "        valid_series = _preprocess_for_digit_extraction(series=data_series)\n",
        "\n",
        "        # If there are no valid numbers, return a series of NaNs.\n",
        "        if valid_series.empty:\n",
        "            return pd.Series(np.nan, index=data_series.index, dtype=np.float64)\n",
        "\n",
        "        # Step 2: Extract the first-two digits from the valid data.\n",
        "        # This result is already filtered to include only values >= 10.\n",
        "        extracted_digits = _extract_first_two_digits_direct(series=valid_series)\n",
        "\n",
        "        # Step 3: Align the results back to the original series' index.\n",
        "        # This correctly assigns NaN to invalid inputs (0, NaN) AND to numbers\n",
        "        # with only one significant digit.\n",
        "        result_series = extracted_digits.reindex(data_series.index).astype(np.float64)\n",
        "\n",
        "        # Final validation: ensure all non-NaN digits are in the range [10, 99].\n",
        "        assert result_series.dropna().isin(range(10, 100)).all(), \\\n",
        "            \"Post-validation failed: Extracted digits found outside the valid range [10, 99].\"\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during the extraction process.\n",
        "        print(f\"An error occurred during first-two digit extraction: {e}\")\n",
        "        raise\n",
        "\n",
        "    return result_series\n"
      ],
      "metadata": {
        "id": "EJ1eZpRkQCtF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 10: BL1 Probability Distribution Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase IV, Task 10: Helper for BL1 Probability Calculation\n",
        "# ==============================================================================\n",
        "def _calculate_bl1_probabilities() -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates the theoretical probabilities for Benford's First-Digit Law (BL1).\n",
        "\n",
        "    This function implements Equation (1) from the source paper to compute the\n",
        "    expected probability for each leading digit from 1 to 9. The calculation\n",
        "    is performed using vectorized numpy operations for efficiency and precision.\n",
        "\n",
        "    Equation (1): P(d1 = i) = log10(1 + 1/i)\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series containing the theoretical probabilities,\n",
        "                   indexed by the digits 1 through 9.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Create an array representing the possible first digits (i).\n",
        "    digits = np.arange(1, 10, dtype=np.float64)\n",
        "\n",
        "    # Step 1.2: Apply Equation (1) in a vectorized manner.\n",
        "    # np.log10 is used for direct, high-precision base-10 logarithm calculation.\n",
        "    probabilities = np.log10(1 + (1 / digits))\n",
        "\n",
        "    # Step 1.3: Structure the result as a pandas Series for clarity and usability.\n",
        "    # The index is set to the digits themselves.\n",
        "    bl1_probs = pd.Series(probabilities, index=pd.Index(digits.astype(int), name='Digit'))\n",
        "\n",
        "    return bl1_probs\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase IV, Task 10: Generic Helper for Distribution Validation\n",
        "# ==============================================================================\n",
        "def _validate_probability_distribution(dist: pd.Series) -> bool:\n",
        "    \"\"\"\n",
        "    Validates the mathematical integrity of a calculated probability distribution.\n",
        "\n",
        "    This function performs two fundamental checks on a distribution:\n",
        "    1. Ensures all probabilities are non-negative.\n",
        "    2. Ensures the sum of all probabilities is equal to 1.0 within a tight\n",
        "       floating-point tolerance.\n",
        "\n",
        "    Args:\n",
        "        dist (pd.Series): The probability distribution to validate.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the distribution is valid.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the distribution fails either of the validation checks.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Check for non-negativity.\n",
        "    assert (dist >= 0).all(), \"Validation failed: Probabilities cannot be negative.\"\n",
        "\n",
        "    # Step 2.2: Check if the probabilities sum to 1.0.\n",
        "    # math.isclose is used for robust floating-point comparison.\n",
        "    # A relative tolerance of 1e-9 is a standard for high-precision checks.\n",
        "    assert math.isclose(dist.sum(), 1.0, rel_tol=1e-9), \\\n",
        "        f\"Validation failed: Probabilities must sum to 1.0, but sum to {dist.sum()}.\"\n",
        "\n",
        "    return True\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase IV, Task 10: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def generate_bl1_distribution(\n",
        "    sample_size: int\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Generates the theoretical BL1 probability and expected frequency distributions.\n",
        "\n",
        "    This function orchestrates the complete process for generating the BL1\n",
        "    theoretical benchmarks for a given sample size. It:\n",
        "    1. Calculates the fundamental BL1 probabilities using a high-precision method.\n",
        "    2. Validates the mathematical integrity of the calculated probability distribution.\n",
        "    3. Computes the expected frequencies for the specified sample size.\n",
        "\n",
        "    Args:\n",
        "        sample_size (int): The total number of valid observations in the\n",
        "                           empirical sample to be tested.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.Series]: A tuple containing:\n",
        "            - A Series of the theoretical BL1 probabilities.\n",
        "            - A Series of the expected frequencies for the given sample size.\n",
        "            Both are indexed by the digits 1 through 9.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(sample_size, int) or sample_size < 0:\n",
        "        raise ValueError(\"Input 'sample_size' must be a non-negative integer.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Calculate the theoretical BL1 probabilities.\n",
        "        bl1_probabilities = _calculate_bl1_probabilities()\n",
        "\n",
        "        # Step 2: Validate the calculated probability distribution.\n",
        "        _validate_probability_distribution(dist=bl1_probabilities)\n",
        "\n",
        "        # Step 3: Calculate the expected frequencies for the given sample size.\n",
        "        # This is a simple element-wise multiplication.\n",
        "        expected_frequencies = bl1_probabilities * sample_size\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during the generation process.\n",
        "        print(f\"An error occurred during BL1 distribution generation: {e}\")\n",
        "        raise\n",
        "\n",
        "    return bl1_probabilities, expected_frequencies\n"
      ],
      "metadata": {
        "id": "NMB4yCc5QgMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 11: BL2 Probability Distribution Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase IV, Task 11: Helper for BL2 Probability Calculation\n",
        "# ==============================================================================\n",
        "def _calculate_bl2_probabilities() -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates the theoretical probabilities for Benford's Second-Digit Law (BL2).\n",
        "\n",
        "    This function implements the nested summation from Equation (3) of the\n",
        "    source paper. It uses numpy broadcasting to perform the calculation in a\n",
        "    fully vectorized manner, which is highly efficient and avoids explicit\n",
        "    Python loops.\n",
        "\n",
        "    Equation (3): P(d2 = j) = sum_{k=1 to 9} log10(1 + 1/(10k + j))\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series containing the theoretical probabilities for\n",
        "                   the second digit (0-9), indexed by the digits themselves.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Define the ranges for the second digit (j) and the summation index (k).\n",
        "    # j represents the second digit, from 0 to 9.\n",
        "    # k is the summation index, representing the first digit, from 1 to 9.\n",
        "    j_digits = np.arange(0, 10, dtype=np.float64).reshape(-1, 1) # Shape (10, 1)\n",
        "    k_digits = np.arange(1, 10, dtype=np.float64).reshape(1, -1) # Shape (1, 9)\n",
        "\n",
        "    # Step 1.2: Use numpy broadcasting to create a 2D grid of (10k + j) values.\n",
        "    # The addition of a (10, 1) vector and a (1, 9) vector results in a\n",
        "    # (10, 9) matrix where each element is (10k + j).\n",
        "    denominator_grid = 10 * k_digits + j_digits\n",
        "\n",
        "    # Step 1.3: Apply the inner part of the formula to the entire grid at once.\n",
        "    # This calculates log10(1 + 1/(10k + j)) for all combinations of j and k.\n",
        "    log_terms_grid = np.log10(1 + (1 / denominator_grid))\n",
        "\n",
        "    # Step 1.4: Sum the terms along the k-axis (axis=1) to get the final probability for each j.\n",
        "    # This performs the summation Σ_{k=1 to 9} for each second digit j.\n",
        "    probabilities = log_terms_grid.sum(axis=1)\n",
        "\n",
        "    # Step 1.5: Structure the result as a pandas Series for clarity and usability.\n",
        "    bl2_probs = pd.Series(probabilities, index=pd.Index(j_digits.flatten().astype(int), name='Digit'))\n",
        "\n",
        "    return bl2_probs\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase IV, Task 11: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def generate_bl2_distribution(\n",
        "    sample_size: int\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Generates the theoretical BL2 probability and expected frequency distributions.\n",
        "\n",
        "    This function orchestrates the complete process for generating the BL2\n",
        "    theoretical benchmarks for a given sample size. It:\n",
        "    1. Calculates the fundamental BL2 probabilities using a vectorized summation.\n",
        "    2. Validates the mathematical integrity of the resulting probability distribution.\n",
        "    3. Computes the expected frequencies for the specified sample size.\n",
        "\n",
        "    Args:\n",
        "        sample_size (int): The total number of valid observations in the\n",
        "                           empirical sample to be tested.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.Series]: A tuple containing:\n",
        "            - A Series of the theoretical BL2 probabilities.\n",
        "            - A Series of the expected frequencies for the given sample size.\n",
        "            Both are indexed by the digits 0 through 9.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(sample_size, int) or sample_size < 0:\n",
        "        raise ValueError(\"Input 'sample_size' must be a non-negative integer.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Calculate the theoretical BL2 probabilities.\n",
        "        bl2_probabilities = _calculate_bl2_probabilities()\n",
        "\n",
        "        # Step 2: Validate the calculated probability distribution using the generic helper.\n",
        "        _validate_probability_distribution(dist=bl2_probabilities)\n",
        "\n",
        "        # Step 3: Calculate the expected frequencies for the given sample size.\n",
        "        expected_frequencies = bl2_probabilities * sample_size\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during the generation process.\n",
        "        print(f\"An error occurred during BL2 distribution generation: {e}\")\n",
        "        raise\n",
        "\n",
        "    return bl2_probabilities, expected_frequencies\n"
      ],
      "metadata": {
        "id": "9XpjonOMRDft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 12: BL12 Probability Distribution Generation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase IV, Task 12: Helper for BL12 Probability Calculation\n",
        "# ==============================================================================\n",
        "def _calculate_bl12_probabilities() -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates the theoretical probabilities for Benford's First-Two-Digit Law (BL12).\n",
        "\n",
        "    This function implements Equation (2) from the source paper to compute the\n",
        "    expected probability for each leading two-digit combination from 10 to 99.\n",
        "    The calculation is a direct, vectorized application of the formula.\n",
        "\n",
        "    Equation (2): P12(d1d2 = ij) = log10(1 + 1/ij)\n",
        "\n",
        "    Returns:\n",
        "        pd.Series: A pandas Series containing the 90 theoretical probabilities,\n",
        "                   indexed by the integers 10 through 99.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Create an array representing the possible first-two digits (ij).\n",
        "    # The range is from 10 to 99, inclusive.\n",
        "    digits = np.arange(10, 100, dtype=np.float64)\n",
        "\n",
        "    # Step 1.2: Apply Equation (2) in a vectorized manner.\n",
        "    # This is mathematically analogous to the BL1 calculation.\n",
        "    probabilities = np.log10(1 + (1 / digits))\n",
        "\n",
        "    # Step 1.3: Structure the result as a pandas Series for clarity and usability.\n",
        "    bl12_probs = pd.Series(probabilities, index=pd.Index(digits.astype(int), name='Digits'))\n",
        "\n",
        "    return bl12_probs\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase IV, Task 12: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def generate_bl12_distribution(\n",
        "    sample_size: int\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Generates the theoretical BL12 probability and expected frequency distributions.\n",
        "\n",
        "    This function orchestrates the complete process for generating the BL12\n",
        "    theoretical benchmarks. It:\n",
        "    1. Calculates the fundamental BL12 probabilities for integers 10-99.\n",
        "    2. Validates the mathematical integrity of the calculated probability distribution.\n",
        "    3. Computes the expected frequencies for the specified sample size.\n",
        "\n",
        "    Args:\n",
        "        sample_size (int): The total number of valid observations in the\n",
        "                           empirical sample to be tested.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.Series]: A tuple containing:\n",
        "            - A Series of the theoretical BL12 probabilities.\n",
        "            - A Series of the expected frequencies for the given sample size.\n",
        "            Both are indexed by the integers 10 through 99.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if not isinstance(sample_size, int) or sample_size < 0:\n",
        "        raise ValueError(\"Input 'sample_size' must be a non-negative integer.\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Calculate the theoretical BL12 probabilities.\n",
        "        bl12_probabilities = _calculate_bl12_probabilities()\n",
        "\n",
        "        # Step 2: Validate the calculated probability distribution using the generic helper.\n",
        "        _validate_probability_distribution(dist=bl12_probabilities)\n",
        "\n",
        "        # Step 3: Calculate the expected frequencies for the given sample size.\n",
        "        expected_frequencies = bl12_probabilities * sample_size\n",
        "\n",
        "    except Exception as e:\n",
        "        # Catch any unexpected errors during the generation process.\n",
        "        print(f\"An error occurred during BL12 distribution generation: {e}\")\n",
        "        raise\n",
        "\n",
        "    return bl12_probabilities, expected_frequencies\n"
      ],
      "metadata": {
        "id": "8V-lR2VhRnOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 13: Chi-Squared Goodness-of-Fit Testing\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 13: Helper for Preparing Frequencies\n",
        "# ==============================================================================\n",
        "def _prepare_frequencies_for_test(\n",
        "    empirical_digits: pd.Series,\n",
        "    expected_frequencies: pd.Series\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Prepares observed and expected frequencies for a goodness-of-fit test.\n",
        "\n",
        "    This function takes a series of empirically observed digits and a series of\n",
        "    theoretically expected frequencies. It calculates the observed frequency\n",
        "    counts and ensures that both the observed and expected series are perfectly\n",
        "    aligned by index, with all necessary bins present (filling missing\n",
        "    observed bins with zero).\n",
        "\n",
        "    Args:\n",
        "        empirical_digits (pd.Series): A series of extracted significant digits.\n",
        "        expected_frequencies (pd.Series): A series of theoretically expected\n",
        "                                          frequencies, indexed by digit/bin.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.Series]: A tuple containing:\n",
        "            - The observed frequencies, aligned and zero-filled.\n",
        "            - The expected frequencies, passed through for alignment.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Calculate the observed frequencies of the empirical digits.\n",
        "    # .value_counts() efficiently tabulates the occurrences of each digit.\n",
        "    observed_counts = empirical_digits.value_counts()\n",
        "\n",
        "    # Step 1.2: Align the observed counts with the expected frequencies' index.\n",
        "    # .reindex() is crucial. It ensures that bins present in the theoretical\n",
        "    # distribution but absent in the empirical data are included.\n",
        "    # .fillna(0) correctly assigns a count of zero to these absent bins.\n",
        "    aligned_observed = observed_counts.reindex(expected_frequencies.index).fillna(0)\n",
        "\n",
        "    # Ensure the final series are of a numeric type suitable for calculation.\n",
        "    aligned_observed = aligned_observed.astype(np.float64)\n",
        "\n",
        "    return aligned_observed, expected_frequencies\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 13: Helper for Calculating the Chi-Squared Statistic\n",
        "# ==============================================================================\n",
        "def _calculate_chi_squared_statistic(\n",
        "    observed: pd.Series,\n",
        "    expected: pd.Series\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Pearson's Chi-Squared (χ²) test statistic.\n",
        "\n",
        "    This function implements Equation (5) from the source paper in a vectorized\n",
        "    manner. It requires perfectly aligned series of observed and expected\n",
        "    frequencies as input.\n",
        "\n",
        "    Equation (5): χ² = Σ ( (O_i - E_i)² / E_i )\n",
        "\n",
        "    Args:\n",
        "        observed (pd.Series): The observed frequency counts (O).\n",
        "        expected (pd.Series): The expected frequency counts (E).\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated Chi-Squared statistic.\n",
        "    \"\"\"\n",
        "    # Input validation: Ensure no expected frequency is zero to avoid division by zero.\n",
        "    if (expected <= 0).any():\n",
        "        raise ValueError(\"Expected frequencies must be positive for the Chi-Squared test.\")\n",
        "\n",
        "    # Step 2.1: Calculate the squared differences between observed and expected.\n",
        "    squared_diff = (observed - expected) ** 2\n",
        "\n",
        "    # Step 2.2: Divide by the expected frequencies and sum the terms.\n",
        "    # This is the complete, vectorized implementation of the formula.\n",
        "    chi_squared_stat = (squared_diff / expected).sum()\n",
        "\n",
        "    return float(chi_squared_stat)\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 13: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def perform_chi_squared_test(\n",
        "    empirical_digits: pd.Series,\n",
        "    expected_frequencies: pd.Series,\n",
        "    degrees_of_freedom: int,\n",
        "    critical_value: float,\n",
        "    alpha: float = 0.05\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a complete Chi-Squared goodness-of-fit test.\n",
        "\n",
        "    This function orchestrates the entire Chi-Squared testing process. It:\n",
        "    1. Prepares and aligns the observed and expected frequencies.\n",
        "    2. Calculates the Chi-Squared test statistic.\n",
        "    3. Calculates the p-value for the test statistic.\n",
        "    4. Performs the hypothesis test by comparing the statistic to the given\n",
        "       critical value.\n",
        "    5. Returns a structured dictionary containing all test results.\n",
        "\n",
        "    Args:\n",
        "        empirical_digits (pd.Series): The raw series of extracted digits.\n",
        "        expected_frequencies (pd.Series): The theoretical expected frequencies.\n",
        "        degrees_of_freedom (int): The degrees of freedom for the test (K-1).\n",
        "        critical_value (float): The critical value for the test at the given\n",
        "                                alpha level.\n",
        "        alpha (float): The significance level for the hypothesis test.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the test statistic, p-value,\n",
        "                        degrees of freedom, critical value, alpha, and a\n",
        "                        boolean indicating if the null hypothesis is rejected.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if empirical_digits.dropna().empty:\n",
        "        return {\n",
        "            'chi_squared_stat': np.nan, 'p_value': np.nan,\n",
        "            'dof': degrees_of_freedom, 'critical_value': critical_value,\n",
        "            'alpha': alpha, 'reject_null': False,\n",
        "            'message': \"Test not performed due to empty empirical data.\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # Step 1: Prepare and align the frequency distributions.\n",
        "        observed, expected = _prepare_frequencies_for_test(\n",
        "            empirical_digits=empirical_digits,\n",
        "            expected_frequencies=expected_frequencies\n",
        "        )\n",
        "\n",
        "        # Step 2: Calculate the Chi-Squared test statistic.\n",
        "        chi2_stat = _calculate_chi_squared_statistic(\n",
        "            observed=observed,\n",
        "            expected=expected\n",
        "        )\n",
        "\n",
        "        # Step 3 (enhancement): Calculate the p-value for completeness.\n",
        "        # The p-value is the probability of observing a statistic as extreme or more\n",
        "        # extreme than the one calculated, given the null hypothesis is true.\n",
        "        # We use the survival function (1 - cdf) of the chi2 distribution.\n",
        "        p_value = chi2.sf(chi2_stat, degrees_of_freedom)\n",
        "\n",
        "        # Step 4: Perform the hypothesis test.\n",
        "        # The null hypothesis (H0) is that the data conforms to Benford's Law.\n",
        "        # We reject H0 if the calculated statistic exceeds the critical value.\n",
        "        reject_null = chi2_stat > critical_value\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the Chi-Squared test: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Step 5: Compile and return the structured results.\n",
        "    return {\n",
        "        'chi_squared_stat': chi2_stat,\n",
        "        'p_value': p_value,\n",
        "        'dof': degrees_of_freedom,\n",
        "        'critical_value': critical_value,\n",
        "        'alpha': alpha,\n",
        "        'reject_null': reject_null\n",
        "    }\n"
      ],
      "metadata": {
        "id": "T2oIQuG5SIMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 14: Mean Absolute Deviation Testing\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 14: Helper for Preparing Proportions\n",
        "# ==============================================================================\n",
        "def _prepare_proportions_for_test(\n",
        "    empirical_digits: pd.Series,\n",
        "    theoretical_probabilities: pd.Series\n",
        ") -> Tuple[pd.Series, pd.Series]:\n",
        "    \"\"\"\n",
        "    Prepares observed and theoretical proportions for the MAD test.\n",
        "\n",
        "    This function calculates the observed relative frequencies (proportions) from\n",
        "    the empirical digit data. It ensures perfect alignment with the theoretical\n",
        "    probability distribution by re-indexing and filling missing bins with zero,\n",
        "    making the two series directly comparable.\n",
        "\n",
        "    Args:\n",
        "        empirical_digits (pd.Series): A series of extracted significant digits.\n",
        "        theoretical_probabilities (pd.Series): A series of theoretical\n",
        "                                               probabilities, indexed by digit/bin.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.Series, pd.Series]: A tuple containing:\n",
        "            - The observed proportions, aligned and zero-filled.\n",
        "            - The theoretical probabilities, passed through for alignment.\n",
        "    \"\"\"\n",
        "    # Step 1.1: Count the total number of valid (non-NaN) observations.\n",
        "    n_observations = empirical_digits.count()\n",
        "    if n_observations == 0:\n",
        "        # If no data, return zero-filled proportions aligned to the theoretical index.\n",
        "        zero_proportions = pd.Series(0.0, index=theoretical_probabilities.index)\n",
        "        return zero_proportions, theoretical_probabilities\n",
        "\n",
        "    # Step 1.2: Calculate observed frequencies and convert to proportions.\n",
        "    observed_proportions = empirical_digits.value_counts() / n_observations\n",
        "\n",
        "    # Step 1.3: Align the observed proportions with the theoretical distribution's index.\n",
        "    # This is critical to ensure bins with zero observations are included in the calculation.\n",
        "    aligned_observed = observed_proportions.reindex(theoretical_probabilities.index).fillna(0)\n",
        "\n",
        "    return aligned_observed, theoretical_probabilities\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 14: Helper for Calculating the MAD Statistic\n",
        "# ==============================================================================\n",
        "def _calculate_mad_statistic(\n",
        "    observed_proportions: pd.Series,\n",
        "    theoretical_probabilities: pd.Series\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the Mean Absolute Deviation (MAD) test statistic.\n",
        "\n",
        "    This function implements Equation (6) from the source paper. Note that the\n",
        "    paper's formula represents the SUM of absolute deviations, not the mean.\n",
        "    This implementation adheres strictly to the formula as written.\n",
        "\n",
        "    Equation (6): MAD = Σ |f_o - f_e|\n",
        "\n",
        "    Args:\n",
        "        observed_proportions (pd.Series): The observed relative frequencies (f_o).\n",
        "        theoretical_probabilities (pd.Series): The theoretical probabilities (f_e).\n",
        "\n",
        "    Returns:\n",
        "        float: The calculated MAD statistic.\n",
        "    \"\"\"\n",
        "    # Step 2.1: Calculate the absolute differences between observed and theoretical proportions.\n",
        "    absolute_deviations = (observed_proportions - theoretical_probabilities).abs()\n",
        "\n",
        "    # Step 2.2: Sum the absolute deviations to get the final statistic.\n",
        "    mad_stat = absolute_deviations.sum()\n",
        "\n",
        "    return float(mad_stat)\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 14: Helper for Classifying MAD Conformity\n",
        "# ==============================================================================\n",
        "def _classify_mad_conformity(\n",
        "    mad_statistic: float,\n",
        "    thresholds: Dict[str, float]\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Classifies the MAD statistic into a conformity level based on given thresholds.\n",
        "\n",
        "    This function applies the decision rules specified on page 13 of the paper\n",
        "    to categorize the level of conformity. It handles both single-level and\n",
        "    multi-level threshold structures.\n",
        "\n",
        "    Args:\n",
        "        mad_statistic (float): The calculated MAD statistic.\n",
        "        thresholds (Dict[str, float]): A dictionary of conformity thresholds,\n",
        "            e.g., {'close_conformity': 0.006, 'acceptable_conformity': 0.012}.\n",
        "\n",
        "    Returns:\n",
        "        str: A string describing the level of conformity.\n",
        "    \"\"\"\n",
        "    # Step 3.1: Apply the decision rules based on the provided thresholds.\n",
        "    # The thresholds are sorted from strictest to most lenient.\n",
        "    if mad_statistic <= thresholds['close_conformity']:\n",
        "        return \"Close Conformity\"\n",
        "    # Check if tiered thresholds for 'acceptable' and 'marginal' exist (for BL1).\n",
        "    elif 'acceptable_conformity' in thresholds and mad_statistic <= thresholds['acceptable_conformity']:\n",
        "        return \"Acceptable Conformity\"\n",
        "    elif 'marginally_acceptable' in thresholds and mad_statistic <= thresholds['marginally_acceptable']:\n",
        "        return \"Marginally Acceptable Conformity\"\n",
        "    else:\n",
        "        return \"Non-conformity\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 14: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def perform_mad_test(\n",
        "    empirical_digits: pd.Series,\n",
        "    theoretical_probabilities: pd.Series,\n",
        "    conformity_thresholds: Dict[str, float]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Performs a complete Mean Absolute Deviation (MAD) conformity test.\n",
        "\n",
        "    This function orchestrates the entire MAD testing process. It:\n",
        "    1. Prepares and aligns the observed and theoretical proportions.\n",
        "    2. Calculates the MAD test statistic according to the study's formula.\n",
        "    3. Classifies the result into a conformity level using study-specific thresholds.\n",
        "    4. Returns a structured dictionary containing the test results.\n",
        "\n",
        "    Args:\n",
        "        empirical_digits (pd.Series): The raw series of extracted digits.\n",
        "        theoretical_probabilities (pd.Series): The theoretical probabilities.\n",
        "        conformity_thresholds (Dict[str, float]): The dictionary of thresholds\n",
        "            applicable to the specific test (BL1, BL2, or BL12).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the MAD statistic and the\n",
        "                        resulting conformity classification.\n",
        "    \"\"\"\n",
        "    # Top-level input validation.\n",
        "    if empirical_digits.dropna().empty:\n",
        "        return {\n",
        "            'mad_statistic': np.nan,\n",
        "            'conformity_level': \"Not Applicable\",\n",
        "            'message': \"Test not performed due to empty empirical data.\"\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # Step 1: Prepare and align the proportion distributions.\n",
        "        observed_props, theoretical_props = _prepare_proportions_for_test(\n",
        "            empirical_digits=empirical_digits,\n",
        "            theoretical_probabilities=theoretical_probabilities\n",
        "        )\n",
        "\n",
        "        # Step 2: Calculate the MAD test statistic.\n",
        "        mad_stat = _calculate_mad_statistic(\n",
        "            observed_proportions=observed_props,\n",
        "            theoretical_probabilities=theoretical_props\n",
        "        )\n",
        "\n",
        "        # Step 3: Classify the conformity level based on the statistic.\n",
        "        conformity = _classify_mad_conformity(\n",
        "            mad_statistic=mad_stat,\n",
        "            thresholds=conformity_thresholds\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during the MAD test: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Step 4: Compile and return the structured results.\n",
        "    return {\n",
        "        'mad_statistic': mad_stat,\n",
        "        'conformity_level': conformity\n",
        "    }\n"
      ],
      "metadata": {
        "id": "rfu8tAO8SwHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 15: Comprehensive Results Compilation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 15: Specific Formatting Helper Functions\n",
        "# ==============================================================================\n",
        "\n",
        "def _format_table_3(raw_results: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a DataFrame replicating Table 3 from the source paper.\n",
        "\n",
        "    This function specifically formats the results of the First-Digit (BL1)\n",
        "    tests for the Pre-Tax Income variables (PI, PI(-), and PI(+)). It assembles\n",
        "    columns for observed counts (Fd), observed proportions (BL1), and the\n",
        "    theoretical Benford's Law proportions in the precise order and format\n",
        "    presented in the study.\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary containing the\n",
        "            detailed outputs from the full test suite for all variables.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame formatted to be a high-fidelity\n",
        "                      replication of the body of Table 3.\n",
        "    \"\"\"\n",
        "    # Define the specific variables required for this table.\n",
        "    table_vars = ['PI', 'PI(-)', 'PI(+)']\n",
        "    # Define the test type relevant to this table.\n",
        "    test_type = 'BL1'\n",
        "\n",
        "    # Generate the theoretical BL1 probabilities to serve as a base and reference column.\n",
        "    bl_probs, _ = generate_bl1_distribution(sample_size=1)\n",
        "\n",
        "    # Initialize the DataFrame with the correct index (digits 1-9).\n",
        "    df = pd.DataFrame(index=bl_probs.index)\n",
        "    # Name the index for clarity, matching the paper's 'd1' label.\n",
        "    df.index.name = 'd1'\n",
        "\n",
        "    # Iterate through the specified variables to populate the table columns.\n",
        "    for var in table_vars:\n",
        "        # Retrieve the pre-computed results for the current variable and test type.\n",
        "        results = raw_results[var][test_type]\n",
        "        # Get the series of extracted digits.\n",
        "        digits = results['digits']\n",
        "        # Count the number of valid observations.\n",
        "        n_obs = digits.count()\n",
        "\n",
        "        # Calculate observed frequencies, ensuring all bins (1-9) are present.\n",
        "        obs_counts = digits.value_counts().reindex(df.index).fillna(0)\n",
        "\n",
        "        # Add the observed counts column (e.g., 'Fd PI').\n",
        "        df[f'Fd {var}'] = obs_counts.astype(int)\n",
        "        # Add the observed proportions column (e.g., 'BL1 PI').\n",
        "        df[f'BL1 {var}'] = obs_counts / n_obs\n",
        "\n",
        "    # Add the final column for the theoretical BL1 probabilities.\n",
        "    df['BL1'] = bl_probs\n",
        "\n",
        "    # Enforce the exact column order as seen in the published Table 3.\n",
        "    col_order = ['Fd PI', 'Fd PI(-)', 'Fd PI(+)', 'BL1 PI', 'BL1 PI(-)', 'BL1 PI(+)', 'BL1']\n",
        "\n",
        "    # Return the fully constructed and ordered DataFrame.\n",
        "    return df[col_order]\n",
        "\n",
        "def _format_table_4(raw_results: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a DataFrame replicating Table 4 from the source paper.\n",
        "\n",
        "    This function formats the results of the First-Digit (BL1) tests for the\n",
        "    Total Assets (TA) and PI/TA ratio variables. It follows the same logic as\n",
        "    _format_table_3 but for a different set of variables.\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary containing the\n",
        "            detailed outputs from the full test suite.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame formatted to replicate the body of Table 4.\n",
        "    \"\"\"\n",
        "    # Define the specific variables required for this table.\n",
        "    table_vars = ['TA', 'PI/TA']\n",
        "    # Define the test type.\n",
        "    test_type = 'BL1'\n",
        "\n",
        "    # Generate theoretical BL1 probabilities.\n",
        "    bl_probs, _ = generate_bl1_distribution(sample_size=1)\n",
        "\n",
        "    # Initialize the DataFrame with the correct index.\n",
        "    df = pd.DataFrame(index=bl_probs.index)\n",
        "    df.index.name = 'd1'\n",
        "\n",
        "    # Iterate through variables to populate columns.\n",
        "    for var in table_vars:\n",
        "        results = raw_results[var][test_type]\n",
        "        digits = results['digits']\n",
        "        n_obs = digits.count()\n",
        "        obs_counts = digits.value_counts().reindex(df.index).fillna(0)\n",
        "        df[f'Fd {var}'] = obs_counts.astype(int)\n",
        "        df[f'BL1 {var}'] = obs_counts / n_obs\n",
        "\n",
        "    # Add the theoretical BL1 column.\n",
        "    df['BL1'] = bl_probs\n",
        "\n",
        "    # Enforce the exact column order from the paper.\n",
        "    col_order = ['Fd TA', 'Fd PI/TA', 'BL1 TA', 'BL1 PI/TA', 'BL1']\n",
        "\n",
        "    # Return the formatted DataFrame.\n",
        "    return df[col_order]\n",
        "\n",
        "def _format_table_5(raw_results: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a DataFrame replicating Table 5 from the source paper.\n",
        "\n",
        "    This function formats the results of the Second-Digit (BL2) tests for the\n",
        "    Pre-Tax Income variables (PI, PI(-), and PI(+)).\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary containing the\n",
        "            detailed outputs from the full test suite.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame formatted to replicate the body of Table 5.\n",
        "    \"\"\"\n",
        "    # Define the specific variables required for this table.\n",
        "    table_vars = ['PI', 'PI(-)', 'PI(+)']\n",
        "    # Define the test type.\n",
        "    test_type = 'BL2'\n",
        "\n",
        "    # Generate theoretical BL2 probabilities.\n",
        "    bl_probs, _ = generate_bl2_distribution(sample_size=1)\n",
        "\n",
        "    # Initialize the DataFrame with the correct index (digits 0-9).\n",
        "    df = pd.DataFrame(index=bl_probs.index)\n",
        "    df.index.name = 'd2'\n",
        "\n",
        "    # Iterate through variables to populate columns.\n",
        "    for var in table_vars:\n",
        "        results = raw_results[var][test_type]\n",
        "        digits = results['digits']\n",
        "        n_obs = digits.count()\n",
        "        obs_counts = digits.value_counts().reindex(df.index).fillna(0)\n",
        "        df[f'Sd {var}'] = obs_counts.astype(int)\n",
        "        df[f'BL2 {var}'] = obs_counts / n_obs\n",
        "\n",
        "    # Add the theoretical BL2 column.\n",
        "    df['BL2'] = bl_probs\n",
        "\n",
        "    # Enforce the exact column order from the paper.\n",
        "    col_order = ['Sd PI', 'Sd PI(-)', 'Sd PI(+)', 'BL2 PI', 'BL2 PI(-)', 'BL2 PI(+)', 'BL2']\n",
        "\n",
        "    # Return the formatted DataFrame.\n",
        "    return df[col_order]\n",
        "\n",
        "def _format_table_6(raw_results: Dict[str, Any]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Constructs a DataFrame replicating Table 6 from the source paper.\n",
        "\n",
        "    This function formats the results of the Second-Digit (BL2) tests for the\n",
        "    Total Assets (TA) and PI/TA ratio variables.\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary containing the\n",
        "            detailed outputs from the full test suite.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame formatted to replicate the body of Table 6.\n",
        "    \"\"\"\n",
        "    # Define the specific variables required for this table.\n",
        "    table_vars = ['TA', 'PI/TA']\n",
        "    # Define the test type.\n",
        "    test_type = 'BL2'\n",
        "\n",
        "    # Generate theoretical BL2 probabilities.\n",
        "    bl_probs, _ = generate_bl2_distribution(sample_size=1)\n",
        "\n",
        "    # Initialize the DataFrame with the correct index.\n",
        "    df = pd.DataFrame(index=bl_probs.index)\n",
        "    df.index.name = 'd2'\n",
        "\n",
        "    # Iterate through variables to populate columns.\n",
        "    for var in table_vars:\n",
        "        results = raw_results[var][test_type]\n",
        "        digits = results['digits']\n",
        "        n_obs = digits.count()\n",
        "        obs_counts = digits.value_counts().reindex(df.index).fillna(0)\n",
        "        df[f'Sd {var}'] = obs_counts.astype(int)\n",
        "        df[f'BL2 {var}'] = obs_counts / n_obs\n",
        "\n",
        "    # Add the theoretical BL2 column.\n",
        "    df['BL2'] = bl_probs\n",
        "\n",
        "    # Enforce the exact column order from the paper.\n",
        "    col_order = ['Sd TA', 'Sd PI/TA', 'BL2 TA', 'BL2 PI/TA', 'BL2']\n",
        "\n",
        "    # Return the formatted DataFrame.\n",
        "    return df[col_order]\n",
        "\n",
        "def _format_table_7_and_8_body(\n",
        "    raw_results: Dict[str, Any],\n",
        "    table_vars: List[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a generic DataFrame body for Tables 7 and 8 (BL12 tests).\n",
        "\n",
        "    This helper function handles the common formatting logic for the First-Two-Digit\n",
        "    (BL12) test result tables. It populates observed counts (FSd) and proportions\n",
        "    (BL12) for a given list of variables.\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary of test results.\n",
        "        table_vars (List[str]): The list of variable names to include in the table.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A pandas DataFrame containing the formatted body for a\n",
        "                      BL12 results table.\n",
        "    \"\"\"\n",
        "    # Define the test type.\n",
        "    test_type = 'BL12'\n",
        "\n",
        "    # Generate theoretical BL12 probabilities.\n",
        "    bl_probs, _ = generate_bl12_distribution(sample_size=1)\n",
        "\n",
        "    # Initialize the DataFrame with the correct index (digits 10-99).\n",
        "    df = pd.DataFrame(index=bl_probs.index)\n",
        "    df.index.name = 'd12'\n",
        "\n",
        "    # Iterate through variables to populate columns.\n",
        "    for var in table_vars:\n",
        "        results = raw_results[var][test_type]\n",
        "        digits = results['digits']\n",
        "        n_obs = digits.count()\n",
        "        obs_counts = digits.value_counts().reindex(df.index).fillna(0)\n",
        "        df[f'FSd {var}'] = obs_counts.astype(int)\n",
        "        df[f'BL12 {var}'] = obs_counts / n_obs\n",
        "\n",
        "    # Add the theoretical BL12 column.\n",
        "    df['BL12'] = bl_probs\n",
        "\n",
        "    # Return the DataFrame (column order will be set by the calling function).\n",
        "    return df\n",
        "\n",
        "def _create_stats_footer(\n",
        "    raw_results: Dict[str, Any],\n",
        "    table_vars: List[str],\n",
        "    test_type: str\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Creates a standardized DataFrame to serve as a statistics footer for a table.\n",
        "\n",
        "    This function extracts the Chi-Squared and MAD test statistics for a given\n",
        "    set of variables and a specific test type, formatting them into a clean\n",
        "    DataFrame suitable for display beneath a main results table.\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary of test results.\n",
        "        table_vars (List[str]): The list of variables included in the main table.\n",
        "        test_type (str): The Benford's Law test type (e.g., 'BL1', 'BL2').\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: A DataFrame with test names ('χ²', 'MAD') as the index\n",
        "                      and variable names as columns, containing the relevant\n",
        "                      test statistics.\n",
        "    \"\"\"\n",
        "    # Initialize a list to hold the data for the footer.\n",
        "    footer_data = []\n",
        "\n",
        "    # Iterate through the variables to extract their test statistics.\n",
        "    for var in table_vars:\n",
        "        # Extract the Chi-Squared statistic.\n",
        "        chi2_stat = raw_results[var][test_type]['chi2_test']['chi_squared_stat']\n",
        "        # Extract the MAD statistic.\n",
        "        mad_stat = raw_results[var][test_type]['mad_test']['mad_statistic']\n",
        "        # Append the data to the list.\n",
        "        footer_data.append({'Variable': var, 'χ²': chi2_stat, 'MAD': mad_stat})\n",
        "\n",
        "    # Convert the list of dictionaries to a DataFrame, setting 'Variable' as the index.\n",
        "    # Transpose (.T) the DataFrame so that test names become the index.\n",
        "    footer_df = pd.DataFrame(footer_data).set_index('Variable').T\n",
        "\n",
        "    # Return the formatted footer DataFrame.\n",
        "    return footer_df\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase V, Task 15: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def compile_all_results(\n",
        "    analytical_datasets: Dict[str, pd.DataFrame],\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the entire testing pipeline and compiles all results into\n",
        "    publication-quality tables.\n",
        "\n",
        "    This master function performs a complete, end-to-end replication of the\n",
        "    study's core analysis. It systematically:\n",
        "    1. Defines the full suite of variables to be tested.\n",
        "    2. Executes the BL1, BL2, and BL12 tests for each variable, storing all\n",
        "       detailed outputs in a comprehensive raw results dictionary.\n",
        "    3. Uses a series of dedicated formatting functions to transform the raw\n",
        "       results into high-fidelity replications of Tables 3 through 8 from\n",
        "       the source paper.\n",
        "\n",
        "    Args:\n",
        "        analytical_datasets (Dict[str, pd.DataFrame]): The dictionary of\n",
        "            prepared analytical datasets from Task 5.\n",
        "        study_config (Dict[str, Any]): The global study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing two keys:\n",
        "            'raw_results': A nested dictionary with all detailed test outputs.\n",
        "            'publication_tables': A dictionary where keys are table names\n",
        "                                  (e.g., 'Table 3') and values are dictionaries\n",
        "                                  containing the formatted pandas DataFrame 'body'\n",
        "                                  and the corresponding 'stats' footer DataFrame.\n",
        "    \"\"\"\n",
        "    # Step 1: Define the full set of variables to be tested.\n",
        "    # This map defines the variable name, the source DataFrame, and the column name.\n",
        "    variables_to_test = {\n",
        "        'PI': ('full_data', 'PreTaxIncome_GBP'),\n",
        "        'PI(+)': ('pi_positive', 'PreTaxIncome_GBP'),\n",
        "        'PI(-)': ('pi_negative', 'PreTaxIncome_GBP'),\n",
        "        'TA': ('full_data', 'TotalAssets_GBP'),\n",
        "        'PI/TA': ('full_data', 'PI_TA_Ratio'),\n",
        "    }\n",
        "\n",
        "    # Step 2: Run the full test suite for each variable and store raw results.\n",
        "    raw_results = {}\n",
        "    for var_name, (df_key, col_name) in variables_to_test.items():\n",
        "        # Input validation for the analytical datasets dictionary.\n",
        "        if df_key not in analytical_datasets or col_name not in analytical_datasets[df_key].columns:\n",
        "            raise KeyError(f\"Data for '{var_name}' ('{df_key}', '{col_name}') not found in analytical_datasets.\")\n",
        "        # Select the data series for the current test run.\n",
        "        data_series = analytical_datasets[df_key][col_name]\n",
        "        # Execute the full suite of tests (BL1, BL2, BL12) for the series.\n",
        "        raw_results[var_name] = _run_full_test_suite_for_variable(data_series, study_config)\n",
        "\n",
        "    # Step 3: Generate all publication tables using the dedicated formatting helpers.\n",
        "    publication_tables = {}\n",
        "\n",
        "    try:\n",
        "        # --- Table 3: BL1 for PI variables ---\n",
        "        vars_t3 = ['PI', 'PI(-)', 'PI(+)']\n",
        "        publication_tables['Table 3'] = {\n",
        "            'body': _format_table_3(raw_results),\n",
        "            'stats': _create_stats_footer(raw_results, vars_t3, 'BL1')\n",
        "        }\n",
        "\n",
        "        # --- Table 4: BL1 for TA and PI/TA ---\n",
        "        vars_t4 = ['TA', 'PI/TA']\n",
        "        publication_tables['Table 4'] = {\n",
        "            'body': _format_table_4(raw_results),\n",
        "            'stats': _create_stats_footer(raw_results, vars_t4, 'BL1')\n",
        "        }\n",
        "\n",
        "        # --- Table 5: BL2 for PI variables ---\n",
        "        vars_t5 = ['PI', 'PI(-)', 'PI(+)']\n",
        "        publication_tables['Table 5'] = {\n",
        "            'body': _format_table_5(raw_results),\n",
        "            'stats': _create_stats_footer(raw_results, vars_t5, 'BL2')\n",
        "        }\n",
        "\n",
        "        # --- Table 6: BL2 for TA and PI/TA ---\n",
        "        vars_t6 = ['TA', 'PI/TA']\n",
        "        publication_tables['Table 6'] = {\n",
        "            'body': _format_table_6(raw_results),\n",
        "            'stats': _create_stats_footer(raw_results, vars_t6, 'BL2')\n",
        "        }\n",
        "\n",
        "        # --- Table 7: BL12 for PI variables ---\n",
        "        vars_t7 = ['PI', 'PI(-)', 'PI(+)']\n",
        "        body_t7 = _format_table_7_and_8_body(raw_results, vars_t7)\n",
        "        col_order_t7 = ['FSd PI', 'FSd PI(-)', 'FSd PI(+)', 'BL12 PI', 'BL12 PI(-)', 'BL12 PI(+)', 'BL12']\n",
        "        publication_tables['Table 7'] = {\n",
        "            'body': body_t7[col_order_t7],\n",
        "            'stats': _create_stats_footer(raw_results, vars_t7, 'BL12')\n",
        "        }\n",
        "\n",
        "        # --- Table 8: BL12 for TA and PI/TA ---\n",
        "        vars_t8 = ['TA', 'PI/TA']\n",
        "        body_t8 = _format_table_7_and_8_body(raw_results, vars_t8)\n",
        "        col_order_t8 = ['FSd TA', 'FSd PI/TA', 'BL12 TA', 'BL12 PI/TA', 'BL12']\n",
        "        publication_tables['Table 8'] = {\n",
        "            'body': body_t8[col_order_t8],\n",
        "            'stats': _create_stats_footer(raw_results, vars_t8, 'BL12')\n",
        "        }\n",
        "\n",
        "    except KeyError as e:\n",
        "        # Handle cases where expected results are missing (e.g., a test failed to run).\n",
        "        raise RuntimeError(f\"Failed to format tables because a required result key is missing: {e}\") from e\n",
        "\n",
        "    # Return the final, structured output containing both raw and formatted results.\n",
        "    return {\n",
        "        'raw_results': raw_results,\n",
        "        'publication_tables': publication_tables\n",
        "    }\n"
      ],
      "metadata": {
        "id": "IY-BW9-DTScE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 16: End-to-End Research Pipeline Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 16: End-to-End Research Pipeline Orchestrator\n",
        "# ==============================================================================\n",
        "def run_benford_replication_pipeline(\n",
        "    raw_financial_data: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the complete end-to-end Benford's Law replication pipeline.\n",
        "\n",
        "    This master orchestrator function serves as the single entry point to\n",
        "    replicate the entire study \"Note on pre-taxation reported data by UK\n",
        "    FTSE-listed companies.\" It sequentially executes all major phases of the\n",
        "    research workflow, from initial data and configuration validation to the\n",
        "    final compilation of publication-ready results tables.\n",
        "\n",
        "    The pipeline follows these distinct phases:\n",
        "    1.  **Validation:** Rigorously validates the configuration object and the\n",
        "        structure and quality of the raw input data.\n",
        "    2.  **Preprocessing:** Prepares the data by standardizing precision,\n",
        "        flagging outliers, creating analytical variables (PI/TA ratio), and\n",
        "        segmenting the data into required subsets (PI+, PI-).\n",
        "    3.  **Summary Statistics:** Generates and validates the descriptive\n",
        "        statistics to ensure the dataset aligns with the study's sample.\n",
        "    4.  **Core Analysis & Reporting:** Executes the full suite of Benford's Law\n",
        "        tests (BL1, BL2, BL12) for all variables and compiles the results\n",
        "        into high-fidelity replications of the paper's analytical tables.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_data (pd.DataFrame): The raw panel data of FTSE-listed\n",
        "            companies, conforming to the specified input structure.\n",
        "        study_configuration (Dict[str, Any]): The high-fidelity configuration\n",
        "            object containing all methodological parameters for the study.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all major outputs\n",
        "            of the pipeline, including the data quality report, the replicated\n",
        "            summary statistics table (Table 1), the raw results of all\n",
        "            statistical tests, and the final, formatted publication tables\n",
        "            (Tables 3-8).\n",
        "\n",
        "    Raises:\n",
        "        Exception: Propagates any exception raised during any stage of the\n",
        "                   pipeline, halting execution upon failure to ensure integrity.\n",
        "    \"\"\"\n",
        "    # Announce the start of the pipeline execution.\n",
        "    print(\"=\"*80)\n",
        "    print(\"Executing End-to-End Benford's Law Replication Pipeline...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Initialize a dictionary to store all pipeline outputs.\n",
        "    pipeline_outputs = {}\n",
        "\n",
        "    try:\n",
        "        # --- PHASE I: INPUT VALIDATION ---\n",
        "        print(\"\\n[PHASE I] Running Input Validation...\")\n",
        "\n",
        "        # Step 1: Validate the study configuration object.\n",
        "        is_config_valid = validate_study_configuration(study_config=study_configuration)\n",
        "        assert is_config_valid, \"Configuration validation failed.\"\n",
        "        print(\"  - Study configuration object is valid.\")\n",
        "\n",
        "        # Step 2: Validate the raw DataFrame structure.\n",
        "        is_df_structure_valid = validate_dataframe_structure(\n",
        "            df=raw_financial_data,\n",
        "            study_config=study_configuration\n",
        "        )\n",
        "        assert is_df_structure_valid, \"DataFrame structure validation failed.\"\n",
        "        print(\"  - Raw DataFrame structure is valid.\")\n",
        "\n",
        "        # Step 3: Perform and store the data quality validation report.\n",
        "        data_quality_report = validate_financial_data_quality(df=raw_financial_data)\n",
        "        pipeline_outputs['data_quality_report'] = data_quality_report\n",
        "        print(\"  - Financial data quality checks completed.\")\n",
        "        print(\"[PHASE I] Input Validation Successful.\")\n",
        "\n",
        "        # --- PHASE II: DATA PREPROCESSING AND VARIABLE CONSTRUCTION ---\n",
        "        print(\"\\n[PHASE II] Running Data Preprocessing and Variable Construction...\")\n",
        "\n",
        "        # Step 4: Prepare and standardize the data (creates flags, ensures precision).\n",
        "        prepared_df = prepare_and_standardize_data(raw_df=raw_financial_data)\n",
        "        print(\"  - Data standardization and flagging completed.\")\n",
        "\n",
        "        # Step 5: Create analytical variables and subsets (PI/TA, PI+, PI-).\n",
        "        analytical_datasets = create_analytical_variables(prepared_df=prepared_df)\n",
        "        pipeline_outputs['analytical_datasets'] = analytical_datasets\n",
        "        print(\"  - Analytical variables and subsets created.\")\n",
        "\n",
        "        # Step 6: Generate and store the summary statistics table (replication of Table 1).\n",
        "        summary_stats_table = generate_summary_statistics(analytical_datasets=analytical_datasets)\n",
        "        pipeline_outputs['summary_statistics_table_1'] = summary_stats_table\n",
        "        print(\"  - Summary statistics (Table 1) generated.\")\n",
        "        print(\"[PHASE II] Data Preprocessing Successful.\")\n",
        "\n",
        "        # --- PHASE V: STATISTICAL CONFORMITY TESTING AND REPORTING ---\n",
        "        # Note: Phase III (Digit Extraction) and IV (Distribution Generation) are\n",
        "        # called internally by the functions in Phase V.\n",
        "        print(\"\\n[PHASE V] Running Core Analysis and Compiling Results...\")\n",
        "\n",
        "        # Step 15: Run all tests and compile all results into final tables.\n",
        "        final_results = compile_all_results(\n",
        "            analytical_datasets=analytical_datasets,\n",
        "            study_config=study_configuration\n",
        "        )\n",
        "        pipeline_outputs.update(final_results) # Add raw_results and publication_tables\n",
        "        print(\"  - All Benford's Law tests executed.\")\n",
        "        print(\"  - Publication tables (3-8) generated.\")\n",
        "        print(\"[PHASE V] Core Analysis Successful.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # If any step in the pipeline fails, print an error and re-raise.\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"PIPELINE EXECUTION FAILED. Error: {e}\")\n",
        "        print(\"=\"*80)\n",
        "        raise\n",
        "\n",
        "    # Announce the successful completion of the pipeline.\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Pipeline Execution Completed Successfully.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return the comprehensive dictionary of all outputs.\n",
        "    return pipeline_outputs\n"
      ],
      "metadata": {
        "id": "TN2s2rDTVnf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 17: Robustness Analysis Implementation\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 1: Helper for a Single Bootstrap Iteration\n",
        "# ==============================================================================\n",
        "def _perform_single_bootstrap_iteration(\n",
        "    data_series: pd.Series,\n",
        "    sample_size: int,\n",
        "    config: Dict[str, Any]\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Performs a single bootstrap iteration for a given data series.\n",
        "\n",
        "    This function executes one cycle of the bootstrap process:\n",
        "    1. Draws a random sample with replacement from the original data series.\n",
        "    2. Runs the complete Benford's Law test suite on this bootstrapped sample.\n",
        "    3. Extracts and returns the key test statistics (χ² and MAD) for each test.\n",
        "\n",
        "    Args:\n",
        "        data_series (pd.Series): The original financial data series from which to sample.\n",
        "        sample_size (int): The size of the bootstrap sample to draw.\n",
        "        config (Dict[str, Any]): The global study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, float]: A flat dictionary containing the calculated test\n",
        "                          statistics for this single iteration (e.g.,\n",
        "                          {'BL1_chi2': 15.7, 'BL1_mad': 0.008, ...}).\n",
        "    \"\"\"\n",
        "    # Step 1.1: Draw a sample of size `sample_size` with replacement.\n",
        "    # `dropna()` is crucial to ensure we only sample from valid observations.\n",
        "    bootstrap_sample = data_series.dropna().sample(n=sample_size, replace=True)\n",
        "\n",
        "    # Step 1.2: Run the full test suite on the bootstrapped sample.\n",
        "    # This reuses the powerful helper function from Task 15.\n",
        "    iteration_results = _run_full_test_suite_for_variable(\n",
        "        data_series=bootstrap_sample,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # Step 1.3: Extract the key statistics into a flattened dictionary.\n",
        "    stats = {}\n",
        "    for test_type, results in iteration_results.items():\n",
        "        stats[f'{test_type}_chi2'] = results['chi2_test']['chi_squared_stat']\n",
        "        stats[f'{test_type}_mad'] = results['mad_test']['mad_statistic']\n",
        "\n",
        "    return stats\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 1: Helper for Confidence Interval Calculation\n",
        "# ==============================================================================\n",
        "def _calculate_confidence_intervals(\n",
        "    stats_list: List[float],\n",
        "    confidence_level: float = 0.95\n",
        ") -> Tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Calculates the confidence interval from a list of bootstrap statistics.\n",
        "\n",
        "    This function uses the percentile method to determine the confidence\n",
        "    interval from the empirical distribution of bootstrap statistics.\n",
        "\n",
        "    Args:\n",
        "        stats_list (List[float]): A list of a single statistic (e.g., MAD)\n",
        "                                  from all bootstrap iterations.\n",
        "        confidence_level (float): The desired confidence level (e.g., 0.95 for 95%).\n",
        "\n",
        "    Returns:\n",
        "        Tuple[float, float]: A tuple containing the lower and upper bounds of\n",
        "                             the confidence interval.\n",
        "    \"\"\"\n",
        "    # Calculate the lower and upper percentile bounds.\n",
        "    lower_percentile = (1.0 - confidence_level) / 2.0 * 100\n",
        "    upper_percentile = (1.0 + confidence_level) / 2.0 * 100\n",
        "\n",
        "    # Use numpy.percentile to calculate the interval bounds.\n",
        "    lower_bound, upper_bound = np.percentile(stats_list, [lower_percentile, upper_percentile])\n",
        "\n",
        "    return lower_bound, upper_bound\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 1: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def run_bootstrap_analysis(\n",
        "    analytical_datasets: Dict[str, pd.DataFrame],\n",
        "    study_config: Dict[str, Any],\n",
        "    sample_sizes: List[int] = [1000, 3000, 5000],\n",
        "    iterations: int = 1000\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a comprehensive bootstrap resampling analysis.\n",
        "\n",
        "    This function assesses the stability of the Benford's Law test statistics\n",
        "    by repeatedly resampling from the original data. It iterates through a\n",
        "    defined set of variables, tests multiple sample sizes, and runs a large\n",
        "    number of bootstrap iterations for each combination. It then computes the\n",
        "    mean and 95% confidence interval for each test statistic.\n",
        "\n",
        "    Args:\n",
        "        analytical_datasets (Dict[str, pd.DataFrame]): The dictionary of\n",
        "            prepared analytical datasets.\n",
        "        study_config (Dict[str, Any]): The global study configuration object.\n",
        "        sample_sizes (List[int]): A list of sample sizes to test.\n",
        "        iterations (int): The number of bootstrap iterations to perform.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A deeply nested dictionary containing the bootstrap\n",
        "                        results, structured by variable, sample size, and\n",
        "                        test statistic.\n",
        "    \"\"\"\n",
        "    # Define the primary variables to subject to bootstrap analysis.\n",
        "    variables_to_test = {\n",
        "        'PI': ('full_data', 'PreTaxIncome_GBP'),\n",
        "        'TA': ('full_data', 'TotalAssets_GBP'),\n",
        "    }\n",
        "\n",
        "    # Initialize the master results dictionary.\n",
        "    bootstrap_results = {}\n",
        "\n",
        "    # Outer loop: Iterate through each variable to be tested.\n",
        "    for var_name, (df_key, col_name) in variables_to_test.items():\n",
        "        bootstrap_results[var_name] = {}\n",
        "        data_series = analytical_datasets[df_key][col_name]\n",
        "\n",
        "        # Middle loop: Iterate through each specified sample size.\n",
        "        for size in sample_sizes:\n",
        "            print(f\"\\nRunning bootstrap for '{var_name}' with sample size {size} ({iterations} iterations)...\")\n",
        "            bootstrap_results[var_name][size] = {}\n",
        "\n",
        "            # Storage for the statistics from all iterations.\n",
        "            iteration_stats_lists = {\n",
        "                'BL1_chi2': [], 'BL1_mad': [],\n",
        "                'BL2_chi2': [], 'BL2_mad': [],\n",
        "                'BL12_chi2': [], 'BL12_mad': []\n",
        "            }\n",
        "\n",
        "            # Inner loop: Perform the bootstrap iterations.\n",
        "            for _ in tqdm(range(iterations), desc=f\"  - Iterating (n={size})\"):\n",
        "                # Perform one bootstrap iteration and get the stats.\n",
        "                single_run_stats = _perform_single_bootstrap_iteration(data_series, size, study_config)\n",
        "                # Append the stats to their respective lists.\n",
        "                for key, value in single_run_stats.items():\n",
        "                    if key in iteration_stats_lists:\n",
        "                        iteration_stats_lists[key].append(value)\n",
        "\n",
        "            # After all iterations, calculate CIs for each statistic.\n",
        "            for key, stats_list in iteration_stats_lists.items():\n",
        "                if stats_list:\n",
        "                    mean_stat = np.mean(stats_list)\n",
        "                    ci_lower, ci_upper = _calculate_confidence_intervals(stats_list)\n",
        "                    bootstrap_results[var_name][size][key] = {\n",
        "                        'mean': mean_stat,\n",
        "                        '95_ci_lower': ci_lower,\n",
        "                        '95_ci_upper': ci_upper\n",
        "                    }\n",
        "\n",
        "    return bootstrap_results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 2: Helper for Chi-Squared Sensitivity\n",
        "# ==============================================================================\n",
        "def _test_chi_squared_sensitivity(\n",
        "    statistic: float,\n",
        "    dof: int,\n",
        "    alpha_levels: List[float]\n",
        ") -> Dict[float, bool]:\n",
        "    \"\"\"\n",
        "    Tests the sensitivity of a Chi-Squared test conclusion to different alpha levels.\n",
        "\n",
        "    For a given, fixed test statistic, this function calculates the critical\n",
        "    value for several different significance levels (alphas) and determines\n",
        "    whether the null hypothesis would be rejected at each level. This provides\n",
        "    insight into how robust the statistical conclusion is to the choice of alpha.\n",
        "\n",
        "    Args:\n",
        "        statistic (float): The pre-calculated Chi-Squared statistic.\n",
        "        dof (int): The degrees of freedom for the test.\n",
        "        alpha_levels (List[float]): A list of alpha levels to test (e.g., [0.01, 0.05, 0.10]).\n",
        "\n",
        "    Returns:\n",
        "        Dict[float, bool]: A dictionary where keys are the alpha levels and\n",
        "                           values are booleans indicating the rejection\n",
        "                           decision (True if rejected, False if not).\n",
        "    \"\"\"\n",
        "    # Input validation.\n",
        "    if not all(0 < alpha < 1 for alpha in alpha_levels):\n",
        "        raise ValueError(\"All alpha levels must be between 0 and 1.\")\n",
        "\n",
        "    # Initialize a dictionary to store the sensitivity results.\n",
        "    sensitivity_results: Dict[float, bool] = {}\n",
        "\n",
        "    # Iterate through each specified alpha level.\n",
        "    for alpha in alpha_levels:\n",
        "        # Calculate the critical value for the current alpha level using the\n",
        "        # percent-point function (ppf), which is the inverse of the cdf.\n",
        "        # We use (1 - alpha) because the Chi-Squared test is an upper-tailed test.\n",
        "        critical_value = chi2.ppf(1 - alpha, df=dof)\n",
        "\n",
        "        # Determine the rejection decision: the null is rejected if the\n",
        "        # observed statistic is greater than the critical value for this alpha.\n",
        "        sensitivity_results[alpha] = statistic > critical_value\n",
        "\n",
        "    return sensitivity_results\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 2: Helper for MAD Sensitivity\n",
        "# ==============================================================================\n",
        "def _test_mad_sensitivity(\n",
        "    statistic: float,\n",
        "    base_thresholds: Dict[str, float],\n",
        "    factors: List[float]\n",
        ") -> Dict[float, str]:\n",
        "    \"\"\"\n",
        "    Tests the sensitivity of a MAD conformity classification to varying thresholds.\n",
        "\n",
        "    For a given, fixed MAD statistic, this function scales the base conformity\n",
        "    thresholds by a list of factors (e.g., 0.8 for -20%, 1.2 for +20%) and\n",
        "    re-evaluates the conformity classification at each new threshold level. This\n",
        "    shows how the qualitative conclusion depends on the stringency of the thresholds.\n",
        "\n",
        "    Args:\n",
        "        statistic (float): The pre-calculated MAD statistic.\n",
        "        base_thresholds (Dict[str, float]): The original dictionary of conformity\n",
        "                                           thresholds from the study config.\n",
        "        factors (List[float]): A list of multiplicative factors to apply to the\n",
        "                               thresholds.\n",
        "\n",
        "    Returns:\n",
        "        Dict[float, str]: A dictionary where keys are the sensitivity factors\n",
        "                          and values are the resulting conformity classifications.\n",
        "    \"\"\"\n",
        "    # Initialize a dictionary to store the sensitivity results.\n",
        "    sensitivity_results: Dict[float, str] = {}\n",
        "\n",
        "    # Iterate through each specified sensitivity factor.\n",
        "    for factor in factors:\n",
        "        # Create a deep copy of the base thresholds to avoid modifying the\n",
        "        # original configuration object. This is crucial for isolation.\n",
        "        modified_thresholds = copy.deepcopy(base_thresholds)\n",
        "\n",
        "        # Scale each threshold value in the copied dictionary by the current factor.\n",
        "        for key in modified_thresholds:\n",
        "            modified_thresholds[key] *= factor\n",
        "\n",
        "        # Re-classify the conformity level using the new, modified thresholds.\n",
        "        # This reuses the robust classification logic defined in Task 14.\n",
        "        classification = _classify_mad_conformity(\n",
        "            mad_statistic=statistic,\n",
        "            thresholds=modified_thresholds\n",
        "        )\n",
        "        # Store the result, keyed by the factor used.\n",
        "        sensitivity_results[factor] = classification\n",
        "\n",
        "    return sensitivity_results\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 2: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def run_parameter_sensitivity_analysis(\n",
        "    raw_results: Dict[str, Any],\n",
        "    study_config: Dict[str, Any],\n",
        "    alpha_levels: List[float] = [0.10, 0.05, 0.01],\n",
        "    mad_factors: List[float] = [0.8, 1.0, 1.2]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a parameter sensitivity analysis for all test results.\n",
        "\n",
        "    This function takes the raw, pre-computed results from the main analysis\n",
        "    and systematically tests how the conclusions change when key statistical\n",
        "    parameters are varied. It operates on the static results, making it\n",
        "    computationally efficient. It provides a comprehensive view of the\n",
        "    robustness of the study's findings.\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary of raw test results\n",
        "                                      from the `compile_all_results` function.\n",
        "        study_config (Dict[str, Any]): The global study configuration object,\n",
        "                                       used to access base MAD thresholds.\n",
        "        alpha_levels (List[float]): The significance levels to test for the χ² test.\n",
        "        mad_factors (List[float]): The multiplicative factors to apply to the\n",
        "                                   MAD conformity thresholds (e.g., 1.2 is +20%).\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A deeply nested dictionary containing the sensitivity\n",
        "                        analysis results, structured by variable, test type,\n",
        "                        and parameter variation.\n",
        "    \"\"\"\n",
        "    # Initialize the master results dictionary for the sensitivity analysis.\n",
        "    sensitivity_analysis_results: Dict[str, Any] = {}\n",
        "\n",
        "    # Retrieve the base MAD thresholds from the configuration for easy access.\n",
        "    base_mad_thresholds = study_config.get('statistical_tests', {}).get('mad', {}).get('conformity_thresholds', {})\n",
        "    if not base_mad_thresholds:\n",
        "        raise ValueError(\"MAD conformity thresholds not found in study configuration.\")\n",
        "\n",
        "    # Iterate through each variable (e.g., 'PI', 'TA') in the raw results.\n",
        "    for var_name, var_tests in raw_results.items():\n",
        "        sensitivity_analysis_results[var_name] = {}\n",
        "\n",
        "        # Iterate through each test type (e.g., 'BL1', 'BL2') for the variable.\n",
        "        for test_type, test_results in var_tests.items():\n",
        "            sensitivity_analysis_results[var_name][test_type] = {}\n",
        "\n",
        "            # --- Chi-Squared Sensitivity Analysis ---\n",
        "            # Extract the pre-calculated results for the Chi-Squared test.\n",
        "            chi2_results = test_results['chi2_test']\n",
        "            # Run the sensitivity analysis for the Chi-Squared test.\n",
        "            chi2_sensitivity = _test_chi_squared_sensitivity(\n",
        "                statistic=chi2_results['chi_squared_stat'],\n",
        "                dof=chi2_results['dof'],\n",
        "                alpha_levels=alpha_levels\n",
        "            )\n",
        "            # Store the results.\n",
        "            sensitivity_analysis_results[var_name][test_type]['chi2_sensitivity'] = chi2_sensitivity\n",
        "\n",
        "            # --- MAD Sensitivity Analysis ---\n",
        "            # Extract the pre-calculated results for the MAD test.\n",
        "            mad_results = test_results['mad_test']\n",
        "            # Retrieve the correct set of base thresholds for the current test type (BL1, BL2, etc.).\n",
        "            current_base_thresholds = base_mad_thresholds.get(test_type)\n",
        "            if not current_base_thresholds:\n",
        "                raise ValueError(f\"MAD thresholds for '{test_type}' not found in configuration.\")\n",
        "            # Run the sensitivity analysis for the MAD test.\n",
        "            mad_sensitivity = _test_mad_sensitivity(\n",
        "                statistic=mad_results['mad_statistic'],\n",
        "                base_thresholds=current_base_thresholds,\n",
        "                factors=mad_factors\n",
        "            )\n",
        "            # Store the results.\n",
        "            sensitivity_analysis_results[var_name][test_type]['mad_sensitivity'] = mad_sensitivity\n",
        "\n",
        "    return sensitivity_analysis_results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 3: Helper for Temporal Partitioning\n",
        "# ==============================================================================\n",
        "def _partition_data_temporally(\n",
        "    df: pd.DataFrame,\n",
        "    split_year: int = 2015\n",
        ") -> Dict[str, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Partitions a DataFrame into two temporal subsets based on a split year.\n",
        "\n",
        "    This function leverages the 'Year' level of the DataFrame's MultiIndex to\n",
        "    divide the data into an \"early period\" (up to and including the split year)\n",
        "    and a \"late period\" (after the split year).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame with a MultiIndex including a 'Year' level.\n",
        "        split_year (int): The final year to be included in the \"early period\".\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, pd.DataFrame]: A dictionary containing two DataFrames keyed\n",
        "                                 by 'early_period' and 'late_period'.\n",
        "    \"\"\"\n",
        "    # Input validation.\n",
        "    if 'Year' not in df.index.names:\n",
        "        raise ValueError(\"DataFrame must have a 'Year' level in its MultiIndex.\")\n",
        "\n",
        "    # Step 1.1: Access the 'Year' level from the MultiIndex.\n",
        "    year_level = df.index.get_level_values('Year')\n",
        "\n",
        "    # Step 1.2: Create a boolean mask for the early period.\n",
        "    early_period_mask = year_level <= split_year\n",
        "\n",
        "    # Step 1.3: Create a boolean mask for the late period.\n",
        "    late_period_mask = year_level > split_year\n",
        "\n",
        "    # Step 1.4: Filter the DataFrame to create the two temporal subsets.\n",
        "    early_df = df[early_period_mask].copy()\n",
        "    late_df = df[late_period_mask].copy()\n",
        "\n",
        "    # Step 1.5: Return the subsets in a structured dictionary.\n",
        "    return {\n",
        "        'early_period': early_df,\n",
        "        'late_period': late_df\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17, Step 3: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def run_temporal_stability_analysis(\n",
        "    prepared_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates a temporal stability analysis of the Benford's Law test results.\n",
        "\n",
        "    This function investigates whether the study's conclusions are stable over\n",
        "    time by splitting the dataset into two distinct periods (e.g., 2009-2015\n",
        "    and 2016-2022) and re-running the entire core analysis pipeline on each\n",
        "    subset. It provides a direct comparison of the test results across time.\n",
        "\n",
        "    Args:\n",
        "        prepared_df (pd.DataFrame): The fully prepared and standardized DataFrame\n",
        "                                    from Task 4.\n",
        "        study_config (Dict[str, Any]): The global study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the comprehensive test results\n",
        "                        for the 'early_period' and 'late_period' separately,\n",
        "                        allowing for a direct comparison of their statistical\n",
        "                        properties and Benford's Law conformity.\n",
        "    \"\"\"\n",
        "    # Initialize the master results dictionary.\n",
        "    temporal_results: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # Step 1: Partition the main dataset into early and late periods.\n",
        "        print(\"Step 1: Partitioning data into temporal subsets...\")\n",
        "        temporal_subsets = _partition_data_temporally(df=prepared_df)\n",
        "        print(f\"  - Early period (<=2015) has {len(temporal_subsets['early_period'])} observations.\")\n",
        "        print(f\"  - Late period (>2015) has {len(temporal_subsets['late_period'])} observations.\")\n",
        "\n",
        "        # Step 2: Iterate through each temporal subset and re-run the analysis.\n",
        "        for period_name, period_df in temporal_subsets.items():\n",
        "            print(f\"\\nRunning full analysis for: {period_name.upper()}...\")\n",
        "\n",
        "            # Step 2.1: Create the analytical variables (PI/TA, PI+, PI-) for this specific period.\n",
        "            # This reuses the robust orchestrator from Task 5.\n",
        "            period_analytical_datasets = create_analytical_variables(prepared_df=period_df)\n",
        "\n",
        "            # Step 2.2: Run the entire suite of tests and compile the results for this period.\n",
        "            # This reuses the master analysis and reporting orchestrator from Task 15.\n",
        "            # We are interested in the raw results for comparison.\n",
        "            period_full_results = compile_all_results(\n",
        "                analytical_datasets=period_analytical_datasets,\n",
        "                study_config=study_config\n",
        "            )\n",
        "\n",
        "            # Step 2.3: Store the raw results for this period in the master dictionary.\n",
        "            temporal_results[period_name] = period_full_results['raw_results']\n",
        "            print(f\"  - Analysis for {period_name.upper()} complete.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle any errors during this complex, multi-stage process.\n",
        "        print(f\"An error occurred during the temporal stability analysis: {e}\")\n",
        "        raise\n",
        "\n",
        "    return temporal_results\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 17: Top-Level Orchestrator for All Robustness Analyses\n",
        "# ==============================================================================\n",
        "def run_full_robustness_analysis(\n",
        "    analytical_datasets: Dict[str, pd.DataFrame],\n",
        "    raw_results: Dict[str, Any],\n",
        "    prepared_df: pd.DataFrame,\n",
        "    study_config: Dict[str, Any]\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Orchestrates the execution of the complete suite of robustness analyses.\n",
        "\n",
        "    This top-level function serves as the single entry point for validating the\n",
        "    stability and reliability of the primary research findings. It sequentially\n",
        "    executes three distinct and comprehensive robustness checks:\n",
        "\n",
        "    1.  **Bootstrap Resampling Analysis (Task 17, Step 1):** Assesses the\n",
        "        stability of test statistics across different sample sizes by resampling\n",
        "        the original data with replacement.\n",
        "\n",
        "    2.  **Parameter Sensitivity Analysis (Task 17, Step 2):** Evaluates how\n",
        "        the final conclusions change when key statistical parameters (alpha\n",
        "        levels for Chi-Squared, conformity thresholds for MAD) are varied.\n",
        "\n",
        "    3.  **Temporal Stability Analysis (Task 17, Step 3):** Investigates whether\n",
        "        the observed patterns of non-conformity are consistent across different\n",
        "        time periods by splitting the data and re-running the entire analysis.\n",
        "\n",
        "    Args:\n",
        "        analytical_datasets (Dict[str, pd.DataFrame]): The dictionary of\n",
        "            prepared analytical datasets (from Task 5).\n",
        "        raw_results (Dict[str, Any]): The master dictionary of raw test results\n",
        "            from the primary analysis (from Task 15).\n",
        "        prepared_df (pd.DataFrame): The fully prepared and standardized DataFrame\n",
        "            (from Task 4), required for temporal splitting.\n",
        "        study_config (Dict[str, Any]): The global study configuration object.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary containing the comprehensive results from\n",
        "                        all three robustness analyses, keyed by analysis type.\n",
        "    \"\"\"\n",
        "    # Announce the start of the full robustness analysis phase.\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Executing Full Robustness Analysis Suite (Task 17)...\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Initialize a dictionary to store all robustness analysis outputs.\n",
        "    robustness_outputs = {}\n",
        "\n",
        "    # --- Step 1: Bootstrap Resampling Analysis ---\n",
        "    try:\n",
        "        # Announce the start of this specific analysis step.\n",
        "        print(\"\\n[Step 1/3] Running Bootstrap Resampling Analysis...\")\n",
        "        # Execute the bootstrap analysis orchestrator.\n",
        "        bootstrap_report = run_bootstrap_analysis(\n",
        "            analytical_datasets=analytical_datasets,\n",
        "            study_config=study_config\n",
        "            # Using default iterations and sample sizes for this example.\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_outputs['bootstrap_analysis'] = bootstrap_report\n",
        "        # Announce completion.\n",
        "        print(\"[Step 1/3] Bootstrap Resampling Analysis complete.\")\n",
        "    except Exception as e:\n",
        "        # Provide specific error context if this step fails.\n",
        "        print(f\"[Step 1/3] Bootstrap Resampling Analysis FAILED. Error: {e}\")\n",
        "        # Re-raise the exception to halt execution.\n",
        "        raise\n",
        "\n",
        "    # --- Step 2: Parameter Sensitivity Analysis ---\n",
        "    try:\n",
        "        # Announce the start of this specific analysis step.\n",
        "        print(\"\\n[Step 2/3] Running Parameter Sensitivity Analysis...\")\n",
        "        # Execute the parameter sensitivity analysis orchestrator.\n",
        "        sensitivity_report = run_parameter_sensitivity_analysis(\n",
        "            raw_results=raw_results,\n",
        "            study_config=study_config\n",
        "            # Using default alpha levels and MAD factors.\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_outputs['parameter_sensitivity_analysis'] = sensitivity_report\n",
        "        # Announce completion.\n",
        "        print(\"[Step 2/3] Parameter Sensitivity Analysis complete.\")\n",
        "    except Exception as e:\n",
        "        # Provide specific error context if this step fails.\n",
        "        print(f\"[Step 2/3] Parameter Sensitivity Analysis FAILED. Error: {e}\")\n",
        "        # Re-raise the exception.\n",
        "        raise\n",
        "\n",
        "    # --- Step 3: Temporal Stability Analysis ---\n",
        "    try:\n",
        "        # Announce the start of this specific analysis step.\n",
        "        print(\"\\n[Step 3/3] Running Temporal Stability Analysis...\")\n",
        "        # Execute the temporal stability analysis orchestrator.\n",
        "        temporal_report = run_temporal_stability_analysis(\n",
        "            prepared_df=prepared_df,\n",
        "            study_config=study_config\n",
        "        )\n",
        "        # Store the results.\n",
        "        robustness_outputs['temporal_stability_analysis'] = temporal_report\n",
        "        # Announce completion.\n",
        "        print(\"[Step 3/3] Temporal Stability Analysis complete.\")\n",
        "    except Exception as e:\n",
        "        # Provide specific error context if this step fails.\n",
        "        print(f\"[Step 3/3] Temporal Stability Analysis FAILED. Error: {e}\")\n",
        "        # Re-raise the exception.\n",
        "        raise\n",
        "\n",
        "    # Announce the successful completion of the entire robustness suite.\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Full Robustness Analysis Suite Completed Successfully.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return the comprehensive dictionary of all robustness outputs.\n",
        "    return robustness_outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "FIa5i7fIXRmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Task 18: Final Validation and Quality Assurance\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 18: Helper to Codify Reference Results from Paper\n",
        "# ==============================================================================\n",
        "def _create_reference_results() -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Codifies the key numerical results from the source paper's tables.\n",
        "\n",
        "    This function creates a machine-readable \"answer key\" by meticulously\n",
        "    transcribing the primary test statistics (χ² and MAD) reported in Tables\n",
        "    3 through 8 of the source paper. This dictionary serves as the immutable\n",
        "    ground truth against which the pipeline's calculated results are\n",
        "    programmatically validated to certify the replication's accuracy.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A nested dictionary containing the reference statistics,\n",
        "                        structured as {variable: {test_type: {metric: value}}}.\n",
        "    \"\"\"\n",
        "    # These values are transcribed directly from the specified tables in the paper.\n",
        "    # This structure is designed to mirror the structure of the 'raw_results'\n",
        "    # dictionary for straightforward comparison.\n",
        "    reference_statistics = {\n",
        "        'PI': {\n",
        "            'BL1': {'chi2': 16.5706, 'mad': 0.04103},\n",
        "            'BL2': {'chi2': 9.855, 'mad': 0.02741},\n",
        "            'BL12': {'chi2': 116.213, 'mad': 0.09418}\n",
        "        },\n",
        "        'PI(+)': {\n",
        "            'BL1': {'chi2': 15.7448, 'mad': 0.04664},\n",
        "            'BL2': {'chi2': 10.742, 'mad': 0.03560},\n",
        "            'BL12': {'chi2': 128.654, 'mad': 0.11131}\n",
        "        },\n",
        "        'PI(-)': {\n",
        "            'BL1': {'chi2': 10.3233, 'mad': 0.07764},\n",
        "            'BL2': {'chi2': 15.208, 'mad': 0.08787},\n",
        "            'BL12': {'chi2': 93.989, 'mad': 0.21754}\n",
        "        },\n",
        "        'TA': {\n",
        "            'BL1': {'chi2': 27.757, 'mad': 0.04258},\n",
        "            'BL2': {'chi2': 3.6678, 'mad': 0.02057},\n",
        "            'BL12': {'chi2': 26.5234, 'mad': 0.04658}\n",
        "        },\n",
        "        'PI/TA': {\n",
        "            'BL1': {'chi2': 117.287, 'mad': 0.11404},\n",
        "            'BL2': {'chi2': 18.063, 'mad': 0.04253},\n",
        "            'BL12': {'chi2': 196.574, 'mad': 0.13625}\n",
        "        }\n",
        "    }\n",
        "    # Return the completed dictionary of ground truth values.\n",
        "    return reference_statistics\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 18: Helper for Final Replication Validation\n",
        "# ==============================================================================\n",
        "def _validate_replication_accuracy(\n",
        "    raw_results: Dict[str, Any],\n",
        "    reference_results: Dict[str, Any],\n",
        "    tolerance: float = 0.01\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Validates the accuracy of the replication against reference results.\n",
        "\n",
        "    This function systematically compares the calculated test statistics from the\n",
        "    pipeline against the ground truth values transcribed from the source paper.\n",
        "    It uses a relative tolerance (`math.isclose`) to robustly handle minor\n",
        "    floating-point differences that can arise from different software\n",
        "    environments, producing a detailed, auditable validation report.\n",
        "\n",
        "    Args:\n",
        "        raw_results (Dict[str, Any]): The master dictionary of raw test results\n",
        "                                      generated by the pipeline.\n",
        "        reference_results (Dict[str, Any]): The dictionary of reference values\n",
        "                                            from `_create_reference_results`.\n",
        "        tolerance (float): The relative tolerance to use for floating-point\n",
        "                           comparisons. A value of 0.01 corresponds to 1%.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A dictionary summarizing the validation outcome,\n",
        "                        including a success rate and a detailed log of each\n",
        "                        comparison.\n",
        "    \"\"\"\n",
        "    # Initialize a list to store a log of each individual comparison.\n",
        "    validation_log: List[Dict[str, Any]] = []\n",
        "    # Initialize counters for calculating the success rate.\n",
        "    match_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # Iterate through the reference results to ensure every key metric is checked.\n",
        "    for var, tests in reference_results.items():\n",
        "        for test, metrics in tests.items():\n",
        "            for metric, ref_val in metrics.items():\n",
        "                # Increment the total number of metrics being checked.\n",
        "                total_count += 1\n",
        "                # Map the simple metric name ('chi2', 'mad') to the nested keys in the raw_results dict.\n",
        "                res_key = 'chi2_test' if metric == 'chi2' else 'mad_test'\n",
        "                stat_key = 'chi_squared_stat' if metric == 'chi2' else 'mad_statistic'\n",
        "\n",
        "                # Safely retrieve the calculated value from our results using .get() to avoid KeyErrors.\n",
        "                calc_val = raw_results.get(var, {}).get(test, {}).get(res_key, {}).get(stat_key)\n",
        "\n",
        "                # Perform the comparison using a relative tolerance.\n",
        "                is_match = calc_val is not None and math.isclose(calc_val, ref_val, rel_tol=tolerance)\n",
        "\n",
        "                # Update the status and match counter based on the comparison result.\n",
        "                if is_match:\n",
        "                    status = \"PASS\"\n",
        "                    match_count += 1\n",
        "                else:\n",
        "                    status = \"FAIL\"\n",
        "\n",
        "                # Append a detailed record of this comparison to the log.\n",
        "                validation_log.append({\n",
        "                    'Variable': var, 'Test': test, 'Metric': metric.upper(),\n",
        "                    'Reference': ref_val, 'Calculated': calc_val, 'Status': status\n",
        "                })\n",
        "\n",
        "    # Calculate the final success rate.\n",
        "    success_rate = (match_count / total_count) * 100 if total_count > 0 else 0\n",
        "\n",
        "    # Return the comprehensive report.\n",
        "    return {\n",
        "        'summary': {\n",
        "            'total_metrics_checked': total_count,\n",
        "            'metrics_matched': match_count,\n",
        "            'success_rate_pct': success_rate,\n",
        "            'tolerance': tolerance\n",
        "        },\n",
        "        'detailed_log': validation_log\n",
        "    }\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 18: Helper for Deep JSON Serialization\n",
        "# ==============================================================================\n",
        "def _deep_serializer(obj: Any) -> Any:\n",
        "    \"\"\"\n",
        "    Recursively converts a potentially complex object to be JSON serializable.\n",
        "\n",
        "    This utility function traverses nested data structures (dicts, lists) and\n",
        "    converts non-native JSON types, such as pandas DataFrames, Series, and\n",
        "    numpy numerical types, into their native Python equivalents (dicts, lists,\n",
        "    ints, floats). This is essential for reliably saving complex research\n",
        "    outputs to JSON files.\n",
        "\n",
        "    Args:\n",
        "        obj (Any): The object to be serialized.\n",
        "\n",
        "    Returns:\n",
        "        Any: A JSON-serializable representation of the object.\n",
        "    \"\"\"\n",
        "    # If the object is a pandas DataFrame or Series, convert it to a dictionary.\n",
        "    # 'split' orientation is a good choice as it preserves the index, columns, and data.\n",
        "    if isinstance(obj, (pd.DataFrame, pd.Series)):\n",
        "        return obj.to_dict(orient='split')\n",
        "    # If the object is a numpy integer, cast it to a native Python int.\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    # If the object is a numpy float, cast it to a native Python float.\n",
        "    if isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    # If the object is a numpy array, convert it to a native Python list.\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    # If the object is a dictionary, recursively apply the serializer to its values.\n",
        "    if isinstance(obj, dict):\n",
        "        return {key: _deep_serializer(value) for key, value in obj.items()}\n",
        "    # If the object is a list or tuple, recursively apply the serializer to its items.\n",
        "    if isinstance(obj, (list, tuple)):\n",
        "        return [_deep_serializer(item) for item in obj]\n",
        "    # If the object is already a native type, return it as is.\n",
        "    return obj\n",
        "\n",
        "# ==============================================================================\n",
        "# Phase VI, Task 18: Main Orchestrator Function\n",
        "# ==============================================================================\n",
        "def package_research_outputs(\n",
        "    pipeline_outputs: Dict[str, Any],\n",
        "    output_directory: str\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Performs final validation and packages all research outputs for dissemination.\n",
        "\n",
        "    This function concludes the research pipeline, providing a final seal of\n",
        "    quality and ensuring perfect reproducibility. It:\n",
        "    1.  Performs a final, quantitative validation of the results against the\n",
        "        source paper's published statistics.\n",
        "    2.  Generates a detailed certification report based on this validation.\n",
        "    3.  Saves all key outputs—data quality reports, summary statistics, final\n",
        "        publication tables, raw results, and robustness analyses—to a\n",
        "        specified directory in clear, accessible formats (CSV and JSON).\n",
        "\n",
        "    Args:\n",
        "        pipeline_outputs (Dict[str, Any]): The master dictionary containing all\n",
        "            outputs from the `run_benford_replication_pipeline` function.\n",
        "        output_directory (str): The path to the directory where all output\n",
        "                                files will be saved.\n",
        "    \"\"\"\n",
        "    # Input validation for the output directory path.\n",
        "    if not isinstance(output_directory, (str, Path)):\n",
        "        raise TypeError(\"output_directory must be a string or a Path object.\")\n",
        "\n",
        "    # Create the output directory using pathlib for robust path handling.\n",
        "    # `parents=True` creates any necessary parent directories.\n",
        "    # `exist_ok=True` prevents an error if the directory already exists.\n",
        "    output_path = Path(output_directory)\n",
        "    output_path.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"Packaging all outputs to directory: {output_path.resolve()}\")\n",
        "\n",
        "    try:\n",
        "        # --- Step 1: Perform Final Validation and Save Report ---\n",
        "        print(\"\\nStep 1/2: Performing final validation against reference results...\")\n",
        "        # Create the dictionary of ground truth values from the paper.\n",
        "        reference_results = _create_reference_results()\n",
        "        # Perform the comparison and generate the report.\n",
        "        validation_report = _validate_replication_accuracy(\n",
        "            raw_results=pipeline_outputs['raw_results'],\n",
        "            reference_results=reference_results\n",
        "        )\n",
        "        # Save the detailed validation report to a JSON file.\n",
        "        with open(output_path / \"replication_validation_report.json\", \"w\") as f:\n",
        "            json.dump(validation_report, f, indent=4)\n",
        "        print(f\"  - Validation complete. Success Rate: {validation_report['summary']['success_rate_pct']:.2f}%\")\n",
        "        print(f\"  - Detailed validation log saved to 'replication_validation_report.json'\")\n",
        "\n",
        "        # --- Step 2: Save All Pipeline Outputs to Files ---\n",
        "        print(\"\\nStep 2/2: Saving all pipeline outputs to files...\")\n",
        "\n",
        "        # Define a nested helper function for saving JSON files using the deep serializer.\n",
        "        def save_json(data: Dict, filename: str):\n",
        "            \"\"\"Serializes and saves a dictionary to a JSON file.\"\"\"\n",
        "            serializable_data = _deep_serializer(data)\n",
        "            with open(output_path / filename, \"w\") as f:\n",
        "                json.dump(serializable_data, f, indent=4)\n",
        "\n",
        "        # Save the data quality report generated in Task 3.\n",
        "        save_json(pipeline_outputs['data_quality_report'], \"data_quality_report.json\")\n",
        "\n",
        "        # Save the summary statistics table (Table 1 replica) to a CSV file.\n",
        "        pipeline_outputs['summary_statistics_table_1'].to_csv(output_path / \"table_1_summary_statistics.csv\")\n",
        "\n",
        "        # Save the publication-ready tables (Tables 3-8) to separate CSV files.\n",
        "        for name, content in pipeline_outputs['publication_tables'].items():\n",
        "            # Create a file-system-friendly name (e.g., \"Table 3\" -> \"table_3\").\n",
        "            slug = name.lower().replace(\" \", \"_\")\n",
        "            # Save the main body of the table.\n",
        "            content['body'].to_csv(output_path / f\"{slug}_body.csv\")\n",
        "            # Save the corresponding statistics footer.\n",
        "            content['stats'].to_csv(output_path / f\"{slug}_stats.csv\")\n",
        "\n",
        "        # Save the comprehensive raw results dictionary to a JSON file.\n",
        "        save_json(pipeline_outputs['raw_results'], \"raw_test_results.json\")\n",
        "\n",
        "        # Save the robustness analysis results, if they exist in the outputs.\n",
        "        if 'robustness_analysis' in pipeline_outputs:\n",
        "             save_json(pipeline_outputs['robustness_analysis'], \"robustness_analysis_report.json\")\n",
        "\n",
        "        print(\"  - All reports and tables have been saved successfully.\")\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"Research Output Packaging Complete.\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "    except (IOError, KeyError, TypeError) as e:\n",
        "        # Catch potential errors during file writing or data access.\n",
        "        print(f\"Failed to package research outputs. An error occurred: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "AkMFXFSydQPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Top-Level Orchestrator\n",
        "\n",
        "# ==============================================================================\n",
        "# Final Task: Top-Level Project Orchestrator\n",
        "# ==============================================================================\n",
        "def execute_full_project_workflow(\n",
        "    raw_financial_data: pd.DataFrame,\n",
        "    study_configuration: Dict[str, Any],\n",
        "    output_directory: str,\n",
        "    run_robustness_checks: bool = True\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Executes the entire, end-to-end research project workflow from raw data to\n",
        "    final packaged and validated outputs.\n",
        "\n",
        "    This top-level master orchestrator serves as the single entry point for\n",
        "    the complete study replication. It encapsulates all phases of the project,\n",
        "    ensuring a reproducible, auditable, and robust execution with a seamless\n",
        "    and logically sound flow of data between each stage.\n",
        "\n",
        "    The workflow proceeds in a sequence of major, dependent stages:\n",
        "    1.  **Validation:** Rigorously validates the configuration object and the\n",
        "        structure and quality of the raw input data.\n",
        "    2.  **Preprocessing & Summarization:** Prepares the data by standardizing\n",
        "        precision, flagging outliers, creating analytical variables (PI/TA ratio),\n",
        "        segmenting the data into required subsets (PI+, PI-), and generating\n",
        "        the summary statistics table (Table 1 replica).\n",
        "    3.  **Core Analysis & Reporting:** Executes the full suite of Benford's Law\n",
        "        tests (BL1, BL2, BL12) for all variables and compiles the results\n",
        "        into high-fidelity replications of the paper's analytical tables (Tables 3-8).\n",
        "    4.  **Robustness Analysis:** If enabled, performs a comprehensive suite of\n",
        "        robustness checks, including bootstrap resampling, parameter sensitivity,\n",
        "        and temporal stability analyses.\n",
        "    5.  **Final Packaging:** Programmatically validates the replication's accuracy\n",
        "        against the source paper and saves all generated artifacts to a\n",
        "        specified output directory.\n",
        "\n",
        "    Args:\n",
        "        raw_financial_data (pd.DataFrame): The raw panel data of FTSE-listed\n",
        "            companies, conforming to the specified input structure.\n",
        "        study_configuration (Dict[str, Any]): The high-fidelity configuration\n",
        "            object containing all methodological parameters for the study.\n",
        "        output_directory (str): The path to the directory where all output\n",
        "                                files will be saved.\n",
        "        run_robustness_checks (bool): A flag to enable or disable the\n",
        "            computationally intensive robustness analysis phase. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        Dict[str, Any]: A comprehensive dictionary containing all major outputs\n",
        "                        of the pipeline, providing a complete in-memory record\n",
        "                        of the entire project's results.\n",
        "    \"\"\"\n",
        "    # Announce the start of the entire project workflow.\n",
        "    print(\"=\"*80)\n",
        "    print(\"INITIALIZING TOP-LEVEL PROJECT WORKFLOW\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Initialize a dictionary to hold all pipeline outputs as they are generated.\n",
        "    pipeline_outputs: Dict[str, Any] = {}\n",
        "\n",
        "    try:\n",
        "        # --- STAGE 1: INPUT VALIDATION ---\n",
        "        print(\"\\n[STAGE 1/5] Running Input Validation...\")\n",
        "        # Validate the study configuration object.\n",
        "        validate_study_configuration(study_config=study_configuration)\n",
        "        print(\"  - Study configuration object is valid.\")\n",
        "        # Validate the raw DataFrame structure.\n",
        "        validate_dataframe_structure(df=raw_financial_data, study_config=study_configuration)\n",
        "        print(\"  - Raw DataFrame structure is valid.\")\n",
        "        # Perform and store the data quality validation report.\n",
        "        pipeline_outputs['data_quality_report'] = validate_financial_data_quality(df=raw_financial_data)\n",
        "        print(\"  - Financial data quality checks completed.\")\n",
        "        print(\"[STAGE 1] Input Validation Successful.\")\n",
        "\n",
        "        # --- STAGE 2: PREPROCESSING & SUMMARIZATION ---\n",
        "        print(\"\\n[STAGE 2/5] Running Data Preprocessing and Summarization...\")\n",
        "        # Prepare and standardize the data (creates flags, ensures precision).\n",
        "        prepared_df = prepare_and_standardize_data(raw_df=raw_financial_data)\n",
        "        print(\"  - Data standardization and flagging completed.\")\n",
        "        # Create analytical variables and subsets (PI/TA, PI+, PI-).\n",
        "        analytical_datasets = create_analytical_variables(prepared_df=prepared_df)\n",
        "        pipeline_outputs['analytical_datasets'] = analytical_datasets\n",
        "        print(\"  - Analytical variables and subsets created.\")\n",
        "        # Generate and store the summary statistics table (replication of Table 1).\n",
        "        summary_stats_table = generate_summary_statistics(analytical_datasets=analytical_datasets)\n",
        "        pipeline_outputs['summary_statistics_table_1'] = summary_stats_table\n",
        "        print(\"  - Summary statistics (Table 1) generated.\")\n",
        "        print(\"[STAGE 2] Preprocessing & Summarization Successful.\")\n",
        "\n",
        "        # --- STAGE 3: CORE ANALYSIS & REPORTING ---\n",
        "        print(\"\\n[STAGE 3/5] Running Core Analysis and Compiling Results...\")\n",
        "        # Run all tests and compile all results into final tables.\n",
        "        core_analysis_results = compile_all_results(\n",
        "            analytical_datasets=analytical_datasets,\n",
        "            study_config=study_configuration\n",
        "        )\n",
        "        # Update the main outputs dictionary with the results from this stage.\n",
        "        pipeline_outputs.update(core_analysis_results)\n",
        "        print(\"  - All Benford's Law tests executed.\")\n",
        "        print(\"  - Publication tables (3-8) generated.\")\n",
        "        print(\"[STAGE 3] Core Analysis Successful.\")\n",
        "\n",
        "        # --- STAGE 4: ROBUSTNESS ANALYSIS ---\n",
        "        print(\"\\n[STAGE 4/5] Running Robustness Analysis Suite...\")\n",
        "        if run_robustness_checks:\n",
        "            # Execute the full suite of robustness checks.\n",
        "            robustness_outputs = run_full_robustness_analysis(\n",
        "                analytical_datasets=analytical_datasets,\n",
        "                raw_results=pipeline_outputs['raw_results'],\n",
        "                prepared_df=prepared_df, # Pass the correctly prepared DF\n",
        "                study_config=study_configuration\n",
        "            )\n",
        "            # Store the results.\n",
        "            pipeline_outputs['robustness_analysis'] = robustness_outputs\n",
        "            print(\"[STAGE 4] Robustness Analysis Successful.\")\n",
        "        else:\n",
        "            # Skip if the flag is set to False.\n",
        "            print(\"  - Skipped computationally intensive robustness analysis as requested.\")\n",
        "            print(\"[STAGE 4] Robustness Analysis Skipped.\")\n",
        "\n",
        "        # --- STAGE 5: FINAL VALIDATION & PACKAGING ---\n",
        "        print(\"\\n[STAGE 5/5] Running Final Validation and Packaging...\")\n",
        "        # Package all primary outputs for dissemination.\n",
        "        package_research_outputs(\n",
        "            pipeline_outputs=pipeline_outputs,\n",
        "            output_directory=output_directory\n",
        "        )\n",
        "        print(\"[STAGE 5] Final Validation & Packaging Successful.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # A top-level exception handler to catch any failure in the entire workflow.\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"TOP-LEVEL WORKFLOW FAILED. A critical error occurred: {e}\")\n",
        "        print(\"=\"*80)\n",
        "        # Re-raise the exception to halt execution and provide a traceback.\n",
        "        raise\n",
        "\n",
        "    # Announce the successful completion of the entire project.\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"TOP-LEVEL PROJECT WORKFLOW COMPLETED SUCCESSFULLY.\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Return the final, comprehensive dictionary of all in-memory results.\n",
        "    return pipeline_outputs\n"
      ],
      "metadata": {
        "id": "sQI11vjofP4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}